"""
Common text processing utilities for Japanese text normalization and cleaning.

This module provides centralized text processing functions used across
the application for consistent text handling.
"""

import re
from typing import List, Dict, Any

from src.config.settings import get_settings
from src.constants.messages import CLEANING_PATTERNS
from src.constants.settings import QUALITY_CONTROLS


def normalize_japanese_text(text: str) -> str:
    """
    Normalize Japanese text by removing redundant expressions and ensuring consistent polite form.
    
    Args:
        text: Input Japanese text
        
    Returns:
        Normalized Japanese text
    """
    if not text:
        return ""
    
    # Remove redundant expressions
    redundant_patterns = [
        (r'ã“ã¨ãŒã§ãã¾ã™ã€‚.*?ã“ã¨ãŒã§ãã¾ã™ã€‚', 'ã“ã¨ãŒã§ãã¾ã™ã€‚'),
        (r'ã“ã¨ã«ãªã‚Šã¾ã™ã€‚.*?ã“ã¨ã«ãªã‚Šã¾ã™ã€‚', 'ã“ã¨ã«ãªã‚Šã¾ã™ã€‚'),
        (r'ã¨æ€ã‚ã‚Œã¾ã™ã€‚.*?ã¨æ€ã‚ã‚Œã¾ã™ã€‚', 'ã¨æ€ã‚ã‚Œã¾ã™ã€‚'),
        (r'ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚.*?ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚', 'ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚'),
        (r'ã«é–¢ã—ã¦ã¯ã€', 'ã«ã¤ã„ã¦ã¯ã€'),
        (r'ã«ã¤ã„ã¦è¿°ã¹ã‚‹ã¨ã€', 'ã«ã¤ã„ã¦ã¯ã€'),
        (r'ã¨ã„ã†å½¢ã§', ''),
        (r'ã¨ã„ã£ãŸæ„Ÿã˜ã§', ''),
        (r'ã®ã‚ˆã†ãªçŠ¶æ³ã§ã™', 'ã§ã™'),
        (r'ã¨ã„ã†ã“ã¨ãŒè¨€ãˆã¾ã™', 'ã¨è¨€ãˆã¾ã™')
    ]
    
    for pattern, replacement in redundant_patterns:
        text = re.sub(pattern, replacement, text)
    
    # Ensure consistent polite form (desu/masu)
    polite_patterns = [
        (r'ã§ã‚ã‚‹ã€‚', 'ã§ã™ã€‚'),
        (r'ã ã€‚', 'ã§ã™ã€‚'),
        (r'([^ã¾ã™])ã‚‹ã€‚', r'\1ã¾ã™ã€‚'),
        (r'ã—ãŸã€‚', 'ã—ã¾ã—ãŸã€‚'),
        (r'([^ã‚ã‚Š])ãªã„ã€‚', r'\1ã¾ã›ã‚“ã€‚'),
    ]
    
    for pattern, replacement in polite_patterns:
        text = re.sub(pattern, replacement, text)
    
    # Clean up multiple periods and whitespace
    text = re.sub(r'ã€‚+', 'ã€‚', text)
    text = re.sub(r'\s+', ' ', text.strip())
    
    return text


def remove_duplicate_patterns(title: str) -> str:
    """
    Remove duplicate patterns from titles like 'LLMã®æŠ€è¡“LLM' -> 'LLMã®æŠ€è¡“'.
    
    Args:
        title: Input title text
        
    Returns:
        Title with duplications removed
    """
    if not title:
        return ""
    
    # Import duplicate patterns
    duplicate_patterns = CLEANING_PATTERNS.get('duplicate_patterns', [])
    
    cleaned_title = title
    for pattern in duplicate_patterns:
        try:
            # Apply the pattern with replacement that preserves the good part
            if 'ã®' in pattern and '\\1' in pattern:
                # For patterns like "(LLM)ã®([^ã®LLM]+)\\1", replace with "$1ã®$2"
                cleaned_title = re.sub(pattern, r'\1ã®\2', cleaned_title)
            elif 'ã§\\1' in pattern or 'ãŒ\\1' in pattern or 'ã‚’\\1' in pattern:
                # For patterns like "(LLM|AI)ã§\\1", replace with just "$1"
                cleaned_title = re.sub(pattern, r'\1', cleaned_title)
            else:
                # For other patterns, replace with the first group
                cleaned_title = re.sub(pattern, r'\1\2', cleaned_title)
        except Exception as e:
            # If pattern fails, continue with next pattern
            continue
    
    # Remove any resulting double spaces or punctuation
    cleaned_title = re.sub(r'\s+', ' ', cleaned_title.strip())
    
    return cleaned_title


def clean_llm_response(text: str) -> str:
    """
    Clean LLM response to extract only the relevant content.
    
    Args:
        text: Raw LLM response text
        
    Returns:
        Cleaned text content
    """
    if not text:
        return ""
    
    text = text.strip()
    
    # Handle multi-line responses intelligently
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    if not lines:
        return ""
    
    # Apply meta-pattern removal
    best_line = ""
    for line in lines[:3]:  # Check first 3 lines only
        cleaned_line = line
        
        # Apply meta-patterns to this line
        for pattern in CLEANING_PATTERNS['meta_removal']:
            cleaned_line = re.sub(pattern, r'\1' if r'\1' in pattern else '', cleaned_line, flags=re.IGNORECASE)
        
        cleaned_line = cleaned_line.strip()
        
        # Skip if line is primarily English meta-text
        if re.search(r'^[A-Za-z\s\.\,\:\!\?]+$', cleaned_line):
            continue
        
        # Skip common Japanese meta-responses
        if re.search(r'^(ã¯ã„|æ‰¿çŸ¥|ä»¥ä¸‹|è¦ç´„|ç¿»è¨³|ä½œæˆ)', cleaned_line) and not re.search(r'(ç™ºè¡¨|æŠ€è¡“|æŠ•è³‡|ä¼æ¥­|ã‚µãƒ¼ãƒ“ã‚¹)', cleaned_line):
            continue
            
        # Check for Japanese content that seems like actual content
        if re.search(QUALITY_CONTROLS['japanese_char_patterns'], cleaned_line) and len(cleaned_line) > 10:
            best_line = cleaned_line
            break
    
    # Fallback to first line if no Japanese found
    if not best_line and lines:
        best_line = lines[0]
        # Apply meta-patterns to fallback
        for pattern in CLEANING_PATTERNS['meta_removal']:
            best_line = re.sub(pattern, r'\1' if r'\1' in pattern else '', best_line, flags=re.IGNORECASE)
    
    text = best_line.strip()
    
    # Remove only paired quote marks at start and end
    text = re.sub(r'^[ã€Œã€"](.*?)[ã€ã€"]$', r'\1', text)
    
    # Remove trailing template phrases
    template_phrases = [
        r'ã¨ã„ã†.*?ãŒã‚ã‚Šã¾ã™ã€‚?$',
        r'ã«ã¤ã„ã¦.*?ã§ã™ã€‚?$',
        r'ã«é–¢ã™ã‚‹.*?è¨˜äº‹$',
        r'ã®è©³ç´°.*?$',
    ]
    
    for phrase in template_phrases:
        text = re.sub(phrase, '', text)
    
    return text.strip()


def normalize_title_for_comparison(title: str) -> str:
    """
    Normalize title for better comparison by removing common prefixes/suffixes.
    
    Args:
        title: Original title
        
    Returns:
        Normalized title
    """
    # Convert to lowercase for comparison
    normalized = title.lower()
    
    # Remove common prefixes that might differ between sources
    for prefix_pattern in CLEANING_PATTERNS['title_prefixes']:
        normalized = re.sub(prefix_pattern, '', normalized, flags=re.IGNORECASE)
    
    # Remove common suffixes
    for suffix_pattern in CLEANING_PATTERNS['title_suffixes']:
        normalized = re.sub(suffix_pattern, '', normalized, flags=re.IGNORECASE)
    
    # Remove extra whitespace and normalize
    normalized = re.sub(r'\s+', ' ', normalized.strip())
    
    return normalized


def truncate_at_sentence_boundary(text: str, max_length: int, placeholder: str = 'â€¦') -> str:
    """
    Truncate text at sentence boundaries when possible.
    
    Args:
        text: Text to truncate
        max_length: Maximum length
        placeholder: Placeholder for truncated text
        
    Returns:
        Truncated text
    """
    if len(text) <= max_length:
        return text
    
    # Try to find sentence break points within limit
    sentence_breaks = list(re.finditer(r'[ã€‚ã€]', text[:max_length + 10]))
    if sentence_breaks:
        # Take the last sentence break within reasonable range
        last_break = sentence_breaks[-1]
        if last_break.end() <= max_length - len(placeholder):
            return text[:last_break.end()]
    
    # Fallback to simple truncation
    return text[:max_length - len(placeholder)] + placeholder


def extract_japanese_sentences(text: str, min_length: int = 30, max_length: int = 300) -> List[str]:
    """
    Extract meaningful Japanese sentences from text.
    
    Args:
        text: Input text
        min_length: Minimum sentence length
        max_length: Maximum sentence length
        
    Returns:
        List of extracted sentences
    """
    # Split by sentences
    sentences = re.split(r'[ã€‚.!?]', text)
    meaningful_sentences = []
    
    for sentence in sentences:
        sentence = sentence.strip()
        if (min_length <= len(sentence) <= max_length and 
            not any(word in sentence.lower() for word in [
                "ç”³ã—è¨³", "ã™ã¿ã¾ã›ã‚“", "sorry", "i apologize", "i cannot",
                "ä»¥ä¸‹ã«", "è¦ç´„ã—ã¾ã™", "ã¾ã¨ã‚ã‚‹ã¨", "ã«ã¤ã„ã¦èª¬æ˜"
            ])):
            meaningful_sentences.append(sentence)
    
    return meaningful_sentences


def ensure_sentence_completeness(text: str, target_length: int) -> str:
    """
    ğŸ”¥ ULTRA THINK: è‡ªç„¶è¨€èªå¢ƒç•Œä¿æŒåˆ‡æ–­ï¼ˆæ–‡æ„ã‚’å£Šã•ãªã„åˆ‡æ–­ï¼‰
    
    Args:
        text: Input text
        target_length: Target length
        
    Returns:
        Text with complete sentences preserving meaning
    """
    if len(text) <= target_length:
        return text
    
    # Phase 1: å®Œå…¨ãªæ–‡å˜ä½ã§ã®åˆ‡æ–­ã‚’è©¦è¡Œï¼ˆã€Œã€‚ã€ã§çµ‚ã‚ã‚‹æ–‡ï¼‰
    complete_sentences = re.split(r'(?<=ã€‚)', text)
    if len(complete_sentences) > 1:
        result = ""
        for sentence in complete_sentences:
            if len(result + sentence) <= target_length:
                result += sentence
            else:
                break
        if result and result.endswith('ã€‚'):
            return result
    
    # Phase 2: è‡ªç„¶ãªåˆ‡æ–­ç‚¹ã‚’æ¢ç´¢ï¼ˆæ–‡æ„ã‚’ä¿æŒï¼‰
    if len(text) > target_length:
        # è‡ªç„¶ãªåˆ‡æ–­ç‚¹ã®å„ªå…ˆé †ä½ (ã‚¿ã‚¤ãƒˆãƒ«ç‰¹åŒ–ç‰ˆ)
        natural_breaks = [
            (r'ã€‚(?=[^ã€ã€])', 1),      # æ–‡æœ«ã®å¥ç‚¹ï¼ˆå¼•ç”¨å¤–ï¼‰
            (r'ã€(?=\w{8,})', 1),      # èª­ç‚¹ï¼ˆå¾Œã«8æ–‡å­—ä»¥ä¸Šã‚ã‚‹å ´åˆï¼‰
            (r'(?<=ã§ã™)(?=ã€‚)', 1),    # ã€Œã§ã™ã€ã®å¾Œ
            (r'(?<=ã¾ã™)(?=ã€‚)', 1),    # ã€Œã¾ã™ã€ã®å¾Œ
            (r'(?<=ã¾ã—ãŸ)(?=ã€‚)', 1),  # ã€Œã¾ã—ãŸã€ã®å¾Œ
            (r'(?<=ã•ã‚Œã‚‹)(?=ã€‚)', 1),  # ã€Œã•ã‚Œã‚‹ã€ã®å¾Œ
            (r'(?<=\d)(?=ï¼…|%)', 2),   # æ•°å­—ã®å¾Œã®ï¼…è¨˜å·å‰ã§åˆ‡æ–­
            (r'(?<=ï¼…|%)(?=[^ã€ã€])', 2), # ï¼…è¨˜å·ã®å¾Œã§åˆ‡æ–­
            (r'(?<=å„„|ä¸‡|åƒ|ç™¾)(?=ãƒ‰ãƒ«|å††|äºº)', 2), # å˜ä½ã®å¾Œã§åˆ‡æ–­
            (r'(?<=ãƒ‰ãƒ«|å††|äºº)(?=[^ã€ã€])', 2), # é€šè²¨å˜ä½ã®å¾Œã§åˆ‡æ–­
            (r'(?<=ã«ã‚ˆã‚Š)(?=[^ã€ã€])', 1), # ã€Œã«ã‚ˆã‚Šã€ã®å¾Œ
            (r'(?<=ã¨ã—ã¦)(?=[^ã€ã€])', 1), # ã€Œã¨ã—ã¦ã€ã®å¾Œ
            (r'(?<=ã«ãŠã„ã¦)(?=[^ã€ã€])', 1), # ã€Œã«ãŠã„ã¦ã€ã®å¾Œ
            (r'(?<=[A-Z]{2,})(?=[^A-Z])', 3), # è‹±èªç•¥èªã®å¾Œã§åˆ‡æ–­
        ]
        
        best_cut_pos = -1
        best_priority = 999
        
        # å„ªå…ˆåº¦é †ã«åˆ‡æ–­ç‚¹ã‚’æ¢ç´¢ï¼ˆæ•°å­—ã®å°ã•ã„æ–¹ãŒé«˜å„ªå…ˆåº¦ï¼‰
        for pattern, priority in natural_breaks:
            matches = list(re.finditer(pattern, text[:target_length + 20]))
            if matches and priority <= best_priority:
                # æœ€ã‚‚å¾Œã‚ã®ï¼ˆæ–‡å­—æ•°åˆ¶é™ã«è¿‘ã„ï¼‰åˆ‡æ–­ç‚¹ã‚’é¸æŠ
                for match in reversed(matches):
                    if match.end() <= target_length - 3:  # ä½™è£•ã‚’æŒãŸã›ã‚‹
                        if priority < best_priority or match.end() > best_cut_pos:
                            best_cut_pos = match.end()
                            best_priority = priority
                        break
        
        if best_cut_pos > 0:
            cut_text = text[:best_cut_pos].strip()
            # é©åˆ‡ãªçµ‚ç«¯å‡¦ç†
            if not cut_text.endswith(('ã€‚', 'ã€', 'ã§ã™', 'ã¾ã™', 'ã—ãŸ')):
                cut_text += 'ã€‚'
            return cut_text
    
    # Phase 3: æœ€å¾Œã®æ‰‹æ®µ - å˜èªå¢ƒç•Œã§ã®åˆ‡æ–­
    if len(text) > target_length:
        # åŠ©è©ãƒ»å‹•è©èªå°¾ç­‰ã®ä¸é©åˆ‡ãªåˆ‡æ–­ã‚’é¿ã‘ã‚‹
        bad_endings = ['ã®', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã¯', 'ã‚‚', 'ã—', 'ã¦', 'ã‚Œ', 'ã‘']
        safe_cut = target_length - 10
        
        while safe_cut > target_length // 2:
            if text[safe_cut] in 'ã€ã€‚' or not any(text[safe_cut-2:safe_cut].endswith(bad) for bad in bad_endings):
                return text[:safe_cut] + 'ã€‚'
            safe_cut -= 1
    
    # Fallback: å…ƒã®å‹•ä½œ
    return text[:target_length - 3] + "â€¦"


def detect_language_ratio(text: str) -> Dict[str, float]:
    """
    Detect the ratio of Japanese vs other characters in text.
    
    Args:
        text: Input text
        
    Returns:
        Dictionary with language ratios
    """
    japanese_chars = re.findall(QUALITY_CONTROLS['japanese_char_patterns'], text)
    english_chars = re.findall(r'[a-zA-Z]', text)
    total_chars = len(re.findall(r'[a-zA-Z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
    
    if total_chars == 0:
        return {'japanese': 0.0, 'english': 0.0, 'other': 0.0}
    
    japanese_ratio = len(japanese_chars) / total_chars
    english_ratio = len(english_chars) / total_chars
    other_ratio = 1.0 - japanese_ratio - english_ratio
    
    return {
        'japanese': japanese_ratio,
        'english': english_ratio, 
        'other': max(0.0, other_ratio)
    }


def standardize_punctuation(text: str) -> str:
    """
    Standardize punctuation in Japanese text.
    
    Args:
        text: Input text
        
    Returns:
        Text with standardized punctuation
    """
    # Standardize punctuation marks
    punctuation_map = {
        'ï¼Œ': 'ã€',
        'ï¼': 'ã€‚',
        'ï¼': '!',
        'ï¼Ÿ': '?',
        'ï¼ˆ': '(',
        'ï¼‰': ')',
        'ã€Œ': 'ã€Œ',
        'ã€': 'ã€'
    }
    
    for old, new in punctuation_map.items():
        text = text.replace(old, new)
    
    # Clean up spacing around punctuation
    text = re.sub(r'\s+([ã€‚ã€!?])', r'\1', text)
    text = re.sub(r'([ã€‚ã€!?])\s+', r'\1 ', text)
    
    return text.strip()