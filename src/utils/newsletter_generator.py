"""
Newsletter generation utilities.

This module handles the generation of Markdown newsletters from processed articles.
"""

import os
import re
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import shutil
from urllib.parse import urlparse
from collections import defaultdict
import json

# Optional dependencies
try:
    import markdown  # type: ignore
    HAS_MARKDOWN = True
except ImportError:
    HAS_MARKDOWN = False

try:
    import jinja2
    HAS_JINJA2 = True
except ImportError:
    HAS_JINJA2 = False
    jinja2 = None

try:
    from src.models.schemas import NewsletterOutput, ProcessedArticle, Citation
    HAS_SCHEMAS = True
except ImportError:
    HAS_SCHEMAS = False

try:
    from src.utils.logger import setup_logging
    logger = setup_logging()
    HAS_LOGGER = True
except ImportError:
    # Fallback logger
    import logging
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    HAS_LOGGER = False

try:
    from src.llm.llm_router import LLMRouter
    from src.utils.citation_generator import CitationGenerator
    HAS_LLM_ROUTER = True
except ImportError:
    HAS_LLM_ROUTER = False
    LLMRouter = None

try:
    from numpy import ndarray
    from src.models.schemas import ProcessedArticle, NewsletterOutput
    from src.utils.logger import setup_logging
    from src.utils.supabase_client import SupabaseManager
    from src.utils.embedding_manager import EmbeddingManager
    from numpy.linalg import norm
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np
    HAS_ADVANCED_FEATURES = True
except ImportError:
    HAS_ADVANCED_FEATURES = False

# Remove text_processing import dependency that was causing HAS_LLM_ROUTER to fail
# ensure_sentence_completeness is now defined as module-level function below


class NewsletterGenerator:
    """Generates Markdown newsletters from processed articles."""
    
    def __init__(self, templates_dir: str = "src/templates"):
        """
        Initialize newsletter generator.
        
        Args:
            templates_dir: Directory containing Jinja2 templates
        """
        self.templates_dir = Path(templates_dir)
        
        if HAS_LLM_ROUTER:
            self.llm_router = LLMRouter()
        else:
            self.llm_router = None
        
        # Initialize citation generator for later use
        try:
            from src.utils.citation_generator import CitationGenerator  # local import to avoid circular refs
            self.citation_generator = CitationGenerator(self.llm_router)
        except Exception:
            # In case CitationGenerator import fails (e.g. missing deps) fallback to None
            self.citation_generator = None
        
        # Setup Jinja2 environment (if available)
        if HAS_JINJA2 and jinja2:
            self.jinja_env = jinja2.Environment(
                loader=jinja2.FileSystemLoader(str(self.templates_dir)),
                autoescape=False,  # We want raw markdown
                trim_blocks=False,
                lstrip_blocks=False,
            )
            # Add custom filters
            self.jinja_env.filters['regex_replace'] = self._regex_replace_filter
            self.jinja_env.filters['slugify'] = self._slugify_filter
            self.jinja_env.filters['short_title'] = self._short_title_filter
            self.jinja_env.filters['toc_format'] = self._toc_format_filter
        else:
            self.jinja_env = None
    
    async def generate_newsletter(
        self,
        articles: List[ProcessedArticle],
        edition: str = "daily",
        processing_summary: Dict = None,
        output_dir: str = "drafts",
        quality_threshold: float = 0.35
    ) -> NewsletterOutput:
        """
        Generate newsletter from processed articles.
        
        Args:
            articles: List of processed articles
            edition: Newsletter edition type
            processing_summary: Processing statistics
            output_dir: Output directory for generated files
            quality_threshold: Minimum quality score for inclusion (0.0-1.0)
        
        Returns:
            NewsletterOutput with generated content
        """
        
        if HAS_LOGGER:
            logger.info(
                "Starting newsletter generation",
                articles_count=len(articles),
                edition=edition,
                quality_threshold=quality_threshold
            )
        else:
            logger.info(
                f"Starting newsletter generation: {len(articles)} articles, edition={edition}, quality_threshold={quality_threshold}"
            )
        
        # Apply quality filtering with dynamic threshold adjustment for article count assurance
        min_articles_target = 7  # Ensureæœ€ä½7æœ¬ã®è¨˜äº‹ã‚’ç¢ºä¿ã™ã‚‹ï¼ˆLawrence ã‹ã‚‰ã®è¿½åŠ è¦æœ›ï¼‰
        max_articles_target = 10  # ä¸Šé™ã‚’10æœ¬ã«æ‹¡å¤§
        
        # [CRITICAL FIX] Apply context analysis FIRST, then consolidate multi-source articles
        # This ensures that workflow-provided clustering data (_multi_articles_tmp) is properly processed
        logger.info("Starting context analysis and multi-source consolidation...")
        
        # Step 1: Apply context analysis (sequel detection)
        articles = await self._apply_context_analysis(articles)
        
        # Step 2: Consolidate multi-source articles (handles workflow clustering results)
        articles = await self._consolidate_multi_source_articles(articles)

        filtered_articles = self._filter_articles_by_quality(articles, quality_threshold)
        
        # Dynamic threshold adjustment if we don't have enough articles
        original_threshold = quality_threshold
        adjustment_attempts = 0
        max_attempts = 3
        
        while len(filtered_articles) < min_articles_target and adjustment_attempts < max_attempts:
            # Gradually lower threshold to capture more articles
            quality_threshold *= 0.9  # Reduce by 10% each iteration
            adjustment_attempts += 1
            
            if HAS_LOGGER:
                logger.info(
                    "Insufficient articles, adjusting quality threshold",
                    current_count=len(filtered_articles),
                    target_minimum=min_articles_target,
                    old_threshold=original_threshold if adjustment_attempts == 1 else quality_threshold / 0.9,
                    new_threshold=quality_threshold,
                    attempt=adjustment_attempts
                )
            
            # Re-filter with new threshold
            filtered_articles = self._filter_articles_by_quality(articles, quality_threshold)
            
            # Break if we have enough articles or hit minimum viable threshold
            if len(filtered_articles) >= min_articles_target or quality_threshold < 0.15:
                break
        
        # Apply deduplication
        filtered_articles = self._deduplicate_articles(filtered_articles)
        
        # Final check - if still insufficient after deduplication, try one more relaxed filter
        if len(filtered_articles) < min_articles_target and quality_threshold > 0.1:
            if HAS_LOGGER:
                logger.warning(
                    "Still insufficient articles after deduplication, final threshold relaxation",
                    post_dedup_count=len(filtered_articles),
                    target_minimum=min_articles_target
                )
            
            # Emergency threshold for minimum viable newsletter
            emergency_threshold = max(0.1, quality_threshold * 0.7)
            emergency_filtered = self._filter_articles_by_quality(articles, emergency_threshold)
            emergency_deduped = self._deduplicate_articles(emergency_filtered)
            
            if len(emergency_deduped) > len(filtered_articles):
                filtered_articles = emergency_deduped
                quality_threshold = emergency_threshold
        
        if HAS_LOGGER:
            logger.info(
                "Quality filtering completed",
                original_count=len(articles),
                filtered_count=len(filtered_articles),
                removed_count=len(articles) - len(filtered_articles)
            )
        else:
            logger.info(f"Quality filtering completed: {len(articles)} -> {len(filtered_articles)} ({len(articles) - len(filtered_articles)} removed)")
        
        # Generate newsletter content
        newsletter_date = datetime.now()
        lead_text = await self._generate_lead_text(filtered_articles, edition)
        
        # Generate Japanese titles for each article in parallel
        async def process_single_article(article):
            # First try LLM-based title generation, fallback to template-based
            article.japanese_title = await self._generate_article_title(article)

            # Fallback for failed title generation to prevent article loss
            if not article.japanese_title:
                raw_title = article.summarized_article.filtered_article.raw_article.title
                logger.warning(
                    "Japanese title generation failed, using original title as fallback",
                    article_id=article.summarized_article.filtered_article.raw_article.id
                )
                article.japanese_title = raw_title

            # PRD F-16æº–æ‹ : UPDATEè¨˜äº‹ã«ğŸ†™çµµæ–‡å­—ã‚’è¿½åŠ 
            if hasattr(article, 'is_update') and article.is_update:
                if not article.japanese_title.endswith('ğŸ†™'):
                    article.japanese_title = f"{article.japanese_title}ğŸ†™"
                    logger.info(
                        "Added UPDATE emoji to title",
                        article_id=article.summarized_article.filtered_article.raw_article.id,
                        title=article.japanese_title
                    )

            # Normalize terminology in Japanese title
            if article.japanese_title:
                article.japanese_title = self._normalize_terminology(article.japanese_title)
            if (article.summarized_article and 
                article.summarized_article.summary and 
                article.summarized_article.summary.summary_points):
                article.summarized_article.summary.summary_points = self._cleanup_summary_points(
                    article.summarized_article.summary.summary_points
                )
                # Apply terminology normalization to each summary point
                normalized_points = []
                for point in article.summarized_article.summary.summary_points:
                    normalized_points.append(self._normalize_terminology(point))
                article.summarized_article.summary.summary_points = normalized_points
                # Note: Removed _shorten_summary_points to preserve bullet content integrity per Lawrence's feedback
            return article
        
        # Process all articles in parallel for title generation and cleanup
        tasks = [process_single_article(article) for article in filtered_articles]
        filtered_articles = await asyncio.gather(*tasks)
        
        # New Step: Generate Japanese summaries for all secondary citations in parallel
        citation_generator = CitationGenerator(self.llm_router)
        all_citations_to_summarize = []
        for article in filtered_articles:
            if article.citations:
                all_citations_to_summarize.extend(article.citations)
        
        if all_citations_to_summarize:
            logger.info(f"Generating Japanese summaries for {len(all_citations_to_summarize)} secondary citations...")
            await citation_generator.generate_summaries_for_citations(all_citations_to_summarize)
            logger.info("Finished generating summaries for secondary citations.")
        
        # Prepare template context
        context = {
            "date": newsletter_date,
            "edition": edition,
            "articles": filtered_articles,
            "lead_text": lead_text,
            "processing_summary": processing_summary or {},
            "generation_timestamp": newsletter_date.strftime("%Y-%m-%d %H:%M:%S JST"),
            "word_count": 0,  # Will be calculated
        }
        
        # Render newsletter content ------------------------------------------------
        if self.jinja_env:
            # Use Jinja2 template rendering
            template_name = f"{edition}_newsletter.jinja2"
            
            try:
                template = self.jinja_env.get_template(template_name)
            except Exception:  # jinja2.TemplateNotFound ç­‰
                if HAS_LOGGER:
                    logger.warning(
                        "Template not found, using default",
                        requested_template=template_name,
                        fallback="daily_newsletter.jinja2"
                    )
                try:
                    template = self.jinja_env.get_template("daily_newsletter.jinja2")
                except Exception:
                    # Fall back to basic markdown generation
                    if HAS_LOGGER:
                        logger.warning("Default template also not found, using basic markdown")
                    template = None
                    newsletter_content = self._generate_basic_markdown(context)
        else:
            template_name = "basic"
            template = None

        # Render template (if available)
        if template:
            try:
                newsletter_content = template.render(**context)
            except Exception as e:
                if HAS_LOGGER:
                    logger.error(
                        "Template rendering failed",
                        template=template_name,
                        error=str(e)
                    )
                else:
                    logger.error(
                        f"Template rendering failed: template={template_name}, error={str(e)}"
                    )
                raise
        else:
            # Fallback: Generate basic markdown without template
            newsletter_content = self._generate_basic_markdown(context)
        
        # Collapse excessive blank lines (3+ -> 2)
        newsletter_content = re.sub(r"\n{3,}", "\n\n", newsletter_content)
        
        # Perform final quality validation on generated newsletter
        newsletter_content = self._validate_and_fix_newsletter_content(newsletter_content, filtered_articles)
        
        # Comprehensive quality check
        try:
            from src.utils.newsletter_quality_checker import check_newsletter_quality
            
            logger.info("Performing comprehensive quality check...")
            quality_report = await check_newsletter_quality(
                content=newsletter_content,
                metadata={
                    'edition': edition,
                    'article_count': len(filtered_articles),
                    'generation_timestamp': newsletter_date.isoformat()
                }
            )
            
            logger.info(
                f"Quality check completed: score={quality_report.overall_score:.2f}, "
                f"issues={quality_report.metrics.get('total_issues', 0)}, "
                f"regeneration_required={quality_report.requires_regeneration}"
            )
            
            # Log quality issues for monitoring
            if quality_report.issues:
                for issue in quality_report.issues:
                    if issue.severity == 'critical':
                        logger.error(f"Critical quality issue in {issue.location}: {issue.description}")
                    elif issue.severity == 'major':
                        logger.warning(f"Major quality issue in {issue.location}: {issue.description}")
                    else:
                        logger.info(f"Minor quality issue in {issue.location}: {issue.description}")
            
            # For now, we log the quality report but don't regenerate
            # In future iterations, could implement automatic regeneration for critical issues
            if quality_report.requires_regeneration:
                logger.warning(
                    "Newsletter quality below threshold but continuing with current content. "
                    "Consider implementing automatic regeneration for critical quality issues."
                )
            
        except Exception as e:
            logger.warning(f"Quality check failed, continuing with newsletter generation: {e}")
        
        # Calculate word count
        word_count = len(newsletter_content.split())
        
        # Save Markdown file
        output_file = self._save_newsletter(
            newsletter_content, newsletter_date, edition, output_dir
        )
        
        # ------------------------------------------------------------------
        # Backup (HTML ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ drafts ã«ã¯ä¸è¦ãªã®ã§ç”Ÿæˆã—ãªã„)
        # ------------------------------------------------------------------

        preview_file = None  # HTML ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ãªã„

        try:
            from src.utils.backup_manager import backup_file  # local import to avoid cycle
            backup_path = backup_file(output_file)
        except Exception as backup_err:  # pylint: disable=broad-except
            logger.warning(f"Backup failed: {str(backup_err)}")
            backup_path = None

        # Create newsletter output
        newsletter_output = NewsletterOutput(
            title=f"{newsletter_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} AI NEWS TLDR",
            date=newsletter_date,
            lead_text=lead_text,
            articles=filtered_articles,
            metadata={
                "edition": edition,
                "template": template_name,
                "output_file": str(output_file),
                "preview_file": str(preview_file),
                "backup_file": str(backup_path) if backup_path else None,
                "generation_method": "automated"
            },
            word_count=word_count,
            processing_summary=processing_summary or {}
        )
        
        if HAS_LOGGER:
            logger.info(
                "Newsletter generation completed",
                output_file=str(output_file),
                word_count=word_count,
                articles_included=len(filtered_articles)
            )
        else:
            logger.info(
                f"Newsletter generation completed: {output_file}, word_count={word_count}, articles_included={len(filtered_articles)}"
            )
        
        return newsletter_output
    
    async def _apply_context_analysis(self, articles: List[ProcessedArticle]) -> List[ProcessedArticle]:
        """
        [NEW] Apply context analysis based on PRD F-16.
        Checks for sequels to past articles and applies metadata.
        """
        try:
            # Ensure required imports are available
            import numpy as np
            from sklearn.metrics.pairwise import cosine_similarity
            
            from src.utils.supabase_client import get_recent_contextual_articles
            from src.utils.embedding_manager import EmbeddingManager

            # Get recent articles from Supabase
            past_articles = await get_recent_contextual_articles(days_back=7, limit=1000)
            
            if not past_articles:
                logger.info("No past articles found for context analysis.")
                return articles

            # Initialize embedding manager
            embedding_manager = EmbeddingManager()
            
            # Prepare past articles for similarity comparison
            past_embeddings = []
            past_article_map = {}
            
            for article_data in past_articles:
                if article_data.get('embedding'):
                    article_id = article_data['article_id']
                    embedding = article_data['embedding']
                    
                    # Convert embedding to numpy array if it's a string
                    if isinstance(embedding, str):
                        try:
                            import json
                            embedding = json.loads(embedding)
                        except:
                            try:
                                embedding = eval(embedding)
                            except:
                                logger.warning(f"Failed to parse embedding for article {article_id}")
                                continue
                    
                    try:
                        embedding_array = np.array(embedding, dtype=np.float32)
                        # Normalize for cosine similarity
                        embedding_array = embedding_array / np.linalg.norm(embedding_array)
                        past_embeddings.append(embedding_array)
                        past_article_map[article_id] = article_data
                    except Exception as e:
                        logger.warning(f"Failed to process embedding for article {article_id}: {e}")
                        continue

            if not past_embeddings:
                logger.info("No valid embeddings found in past articles for context analysis.")
                return articles

            # Create a matrix of past embeddings for efficient search
            past_embedding_matrix = np.array(past_embeddings)
            past_article_ids = list(past_article_map.keys())

            # Process each current article
            updates_found = 0
            for article in articles:
                try:
                    current_text = f"{article.summarized_article.filtered_article.raw_article.title} {' '.join(article.summarized_article.summary.summary_points)}"
                    current_embedding = await embedding_manager.get_embedding(current_text)

                    if current_embedding is None:
                        continue

                    # Normalize current embedding
                    current_embedding = current_embedding / np.linalg.norm(current_embedding)

                    # Find similar past articles
                    similarities = cosine_similarity(current_embedding.reshape(1, -1), past_embedding_matrix)[0]
                    
                    # Get top 3 most similar articles
                    top_indices = np.argsort(similarities)[-3:][::-1]

                    for i in top_indices:
                        similarity_score = similarities[i]
                        if similarity_score > 0.80: # Lowered threshold for better sequel detection
                            past_article_id = past_article_ids[i]
                            past_article_data = past_article_map[past_article_id]
                            
                            logger.info(
                                f"Found potential sequel for '{article.summarized_article.filtered_article.raw_article.title[:50]}...' "
                                f"(score: {similarity_score:.2f}) with '{past_article_data['title'][:50]}...'"
                            )

                            # Use LLM to confirm if it's an update
                            decision = await self._confirm_update_with_llm(article, past_article_data)
                            
                            if decision == "UPDATE":
                                article.is_update = True
                                article.previous_article_url = past_article_data.get('source_url', '')
                                updates_found += 1
                                logger.info(f"Confirmed UPDATE for article {article.summarized_article.filtered_article.raw_article.id}")
                                # Once confirmed, no need to check other past articles
                                break
                except Exception as e:
                    logger.warning(f"Failed to analyze context for article: {e}")
                    continue
            
            if updates_found > 0:
                logger.info(f"Context analysis completed: found {updates_found} update articles")
            
            return articles

        except Exception as e:
            logger.error(f"Context analysis failed: {e}", exc_info=True)
            return articles # Return original articles on failure

    async def _confirm_update_with_llm(self, current_article: ProcessedArticle, past_article: Dict) -> str:
        """
        Use LLM to confirm if the current article is an update to the past one.
        """
        try:
            current_title = current_article.summarized_article.filtered_article.raw_article.title
            current_summary = " ".join(current_article.summarized_article.summary.summary_points)
            
            past_title = past_article.get('title', '')
            past_summary = past_article.get('content_summary', '')

            prompt = f"""
            You are an expert news analyst. Compare the "current news" with the "past news" and decide their relationship.

            # Current News
            Title: {current_title}
            Summary: {current_summary}

            # Past News
            Title: {past_title}
            Summary: {past_summary}

            # Decision
            Based on the content, is the "current news" a direct follow-up, update, or sequel to the "past news"?
            Respond with only one word: "UPDATE", "RELATED", or "UNRELATED".
            """

            if HAS_LLM_ROUTER and self.llm_router:
                response = await self.llm_router.generate_simple_text(
                    prompt=prompt,
                    max_tokens=5,
                    temperature=0.0
                )
                
                decision = response.strip().upper()
                if decision in ["UPDATE", "RELATED", "UNRELATED"]:
                    return decision
            
            return "RELATED" # Default to related if LLM fails

        except Exception as e:
            logger.warning(f"LLM update confirmation failed: {e}")
            return "RELATED"
    
    async def _generate_lead_text(
        self,
        articles: List[ProcessedArticle],
        edition: str
    ) -> Dict[str, Any]:
        """Generate introduction text (title + paragraphs) based on actual article content.

        Returns
        -------
        dict
            {"title": str, "paragraphs": List[str]}
        """
        
        if not articles:
            return {
                "title": "æœ¬æ—¥ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "paragraphs": [
                    "æœ¬æ—¥ã¯æ³¨ç›®ã™ã¹ãAIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã”ã–ã„ã¾ã›ã‚“ã§ã—ãŸã€‚"
                ],
            }

        # Try LLM-driven lead text generation first (if available)
        if HAS_LLM_ROUTER and self.llm_router:
            try:
                llm_lead_text = await self._generate_llm_lead_text_with_retry(articles, edition)
                if llm_lead_text and llm_lead_text.get('lead_paragraphs'):
                    # Convert LLM format to template-expected format
                    lead_paragraphs = llm_lead_text['lead_paragraphs']
                    
                    # Generate title from first paragraph or articles
                    title = self._generate_lead_title_from_articles(articles, edition)
                    
                    formatted_lead_text = {
                        "title": title,
                        "paragraphs": lead_paragraphs
                    }
                    
                    logger.info(f"LLM lead text generated successfully: {len(lead_paragraphs)} paragraphs")
                    return formatted_lead_text
                else:
                    logger.warning("LLM lead text generation returned empty or invalid result")
            except Exception as e:
                logger.warning(f"LLM lead text generation failed, falling back to template: {e}")
        else:
            logger.info("LLM router not available, using template-based lead text")

        # Fallback to template-based generation
        update_count = sum(1 for article in articles if getattr(article, 'is_update', False))
        
        # Extract key highlights from actual articles
        key_highlights = self._extract_article_highlights(articles)
        themes = self._extract_key_themes(articles)

        # --- paragraphs --------------------------------------------------
        paragraphs: List[str] = []
        
        if edition == "daily":
            # First paragraph: Main highlights with specific details
            if key_highlights:
                main_highlight = key_highlights[0]
                paragraphs.append(main_highlight)
            elif themes:
                joined = "ã€".join(themes)
                paragraphs.append(
                    f"æœ¬æ—¥ã¯{joined}åˆ†é‡ã‚’ä¸­å¿ƒã¨ã—ãŸé‡è¦ãªAIé–¢é€£ã®å‹•å‘ãŒç›¸æ¬¡ã„ã§ç™ºè¡¨ã•ã‚Œã¾ã—ãŸã€‚"
                )
            else:
                paragraphs.append(
                    "æœ¬æ—¥ã¯æ³¨ç›®ã™ã¹ãAIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸ã—ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚"
                )

            # Second paragraph: Additional context or updates
            if len(key_highlights) > 1:
                second_highlight = key_highlights[1]
                paragraphs.append(second_highlight)
            elif update_count > 0:
                paragraphs.append(
                    f"ä¸€æ–¹ã§ã€{update_count}ä»¶ã®ç¶šå ±ã‚‚å«ã‚ã€æ€¥é€Ÿã«å¤‰åŒ–ã™ã‚‹AIæ¥­ç•Œã®æœ€æ–°å‹•å‘ã‚’ãŠä¼ãˆã—ã¾ã™ã€‚"
                )
            elif len(articles) >= 5:
                paragraphs.append(
                    "ä¼æ¥­ã®æ–°æŠ€è¡“ç™ºè¡¨ã‹ã‚‰ç ”ç©¶æ©Ÿé–¢ã®æˆæœã¾ã§ã€å¤šæ–¹é¢ã«ã‚ãŸã‚‹é‡è¦ãªç™ºè¡¨ãŒç¶šã„ã¦ã„ã¾ã™ã€‚"
                )
            else:
                paragraphs.append(
                    "æŠ€è¡“é©æ–°ã¨ãƒ“ã‚¸ãƒã‚¹æ´»ç”¨ã®ä¸¡é¢ã§æ³¨ç›®ã™ã¹ãå‹•ããŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚"
                )
            
            # Third paragraph: Additional details or context
            if len(key_highlights) > 2:
                third_highlight = key_highlights[2]
                paragraphs.append(third_highlight)
            elif themes and len(themes) > 1:
                additional_themes = themes[1:]
                joined_additional = "ã€".join(additional_themes)
                paragraphs.append(
                    f"ã¾ãŸã€{joined_additional}åˆ†é‡ã§ã‚‚å…·ä½“çš„ãªæ´»ç”¨äº‹ä¾‹ã‚„æŠ€è¡“é€²å±•ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚"
                )
        else:
            paragraphs = [
                f"ä»Šé€±ã¯{len(articles)}ä»¶ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸ã—ã¾ã—ãŸã€‚",
                "æŠ€è¡“å‹•å‘ã‹ã‚‰ãƒ“ã‚¸ãƒã‚¹æ´»ç”¨ã¾ã§å¹…åºƒãã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚",
                "ãã‚Œã§ã¯é€±åˆŠã¾ã¨ã‚ã‚’ã”è¦§ãã ã•ã„ã€‚",
            ]
        
        # å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆæ±ç”¨è¡¨ç¾ã‚’é¿ã‘ã‚‹ï¼‰
        if themes:
            # ä¼æ¥­åãƒ»æŠ€è¡“åã‚’å„ªå…ˆã—ã¦ã‚ˆã‚Šå…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã«
            specific_titles = self._generate_specific_theme_title(themes, articles)
            title = specific_titles or f"{themes[0]}ã«ã‚ˆã‚‹æŠ€è¡“ç™ºè¡¨"
        else:
            # æœ€å¾Œã®æ‰‹æ®µã§ã‚‚å…·ä½“çš„ãªå†…å®¹ã‚’å«ã‚ã‚‹
            title = self._generate_fallback_specific_title(articles)

        # ä½“è¨€æ­¢ã‚ã‚’ä¿è¨¼ï¼ˆæœ«å°¾ãŒã€Œã€‚ã€ã€Œã§ã™ã€ç­‰ãªã‚‰å‰Šé™¤ï¼‰
        title = re.sub(r"[ã€‚.!?ã§ã™ã¾ã™]+$", "", title)

        return {
            "title": title,
            "paragraphs": paragraphs,
        }
    
    async def _generate_llm_lead_text(
        self, 
        articles: List[ProcessedArticle], 
        edition: str
    ) -> Dict[str, Any]:
        """Generate high-quality lead text using LLM based on article content."""
        
        # Prepare structured input for LLM
        article_summaries = []
        
        for i, article in enumerate(articles[:10]):  # Use up to 10 articles for context
            try:
                if (article.summarized_article and 
                    article.summarized_article.summary and 
                    article.summarized_article.summary.summary_points):
                    
                    summary_points = article.summarized_article.summary.summary_points
                    title = article.japanese_title or article.summarized_article.filtered_article.raw_article.title
                    
                    # Extract key info for structured prompt
                    companies = self._extract_companies(summary_points[0], title) if summary_points else []
                    
                    article_summaries.append({
                        "title": title,
                        "key_points": summary_points[:2],  # First 2 points
                        "companies": companies,
                        "is_update": getattr(article, 'is_update', False)
                    })
                    
            except Exception as e:
                logger.warning(f"Failed to process article {i} for LLM lead text: {e}")
                continue
        
        if not article_summaries:
            return None
            
        # Create structured prompt for LLM
        prompt = self._create_lead_text_prompt(article_summaries, edition)
        
        # Generate lead text using LLM
        try:
            lead_text_response = await self.llm_router.generate_simple_text(
                prompt=prompt,
                max_tokens=700,  # Increased from 500 to 700 to prevent truncation
                temperature=0.2,  # Low temperature for consistency
            )
        except Exception as e:
            logger.warning(f"LLM call failed in lead text generation: {e}")
            return None
        
        if not lead_text_response:
            return None
            
        # Parse LLM response into title and paragraphs
        return self._parse_llm_lead_text_response(lead_text_response)
    
    def _create_lead_text_prompt(
        self, 
        article_summaries: List[Dict], 
        edition: str
    ) -> str:
        """Create structured prompt for LLM lead text generation."""
        
        # Build simple article context without complex JSON
        context_lines = []
        for i, summary in enumerate(article_summaries[:5], 1):  # Limit to top 5
            companies_str = "ã€".join(summary['companies'][:2]) if summary['companies'] else "AIä¼æ¥­"
            key_point = summary['key_points'][0] if summary['key_points'] else summary['title']
            update_str = " (ç¶šå ±)" if summary.get('is_update', False) else ""
            
            context_lines.append(f"{i}. {summary['title']}{update_str}")
            if companies_str and companies_str != "AIä¼æ¥­":
                context_lines.append(f"   é–¢é€£ä¼æ¥­: {companies_str}")
            context_lines.append(f"   è¦ç‚¹: {key_point[:100]}...")
            context_lines.append("")
        
        context = "\n".join(context_lines)
        
        prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ã‚’åŸºã«ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ã®å°å…¥æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€æœ¬æ—¥ã®ä¸»è¦è¨˜äº‹ã€‘
{context}

ã€å°å…¥æ–‡ã®è¦ä»¶ã€‘
- 3ã¤ã®æ®µè½ã§æ§‹æˆ
- å„æ®µè½ã¯1æ–‡ã§ã€60-150æ–‡å­—
- ç¬¬1æ®µè½: æœ€ã‚‚é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å…·ä½“çš„ã«ç´¹ä»‹
- ç¬¬2æ®µè½: ä»–ã®é‡è¦ãªå‹•å‘ã‚„é–¢é€£ã™ã‚‹è©±é¡Œ
- ç¬¬3æ®µè½: å…¨ä½“çš„ãªæ¥­ç•Œã¸ã®å½±éŸ¿ã‚„ä»Šå¾Œã®å±•æœ›

ã€é‡è¦ãªæ³¨æ„äº‹é …ã€‘
- å„æ–‡ã¯å¿…ãšã€Œã§ã™ã€ã€Œã¾ã™ã€ã€Œã¾ã—ãŸã€ã€Œã¦ã„ã¾ã™ã€ã§çµ‚ã‚ã‚‹ã“ã¨
- ã€Œã€œãŒã€‚ã€ã®ã‚ˆã†ãªä¸å®Œå…¨ãªæ–‡ã¯ä½œã‚‰ãªã„
- å…·ä½“çš„ãªä¼æ¥­åã‚„æŠ€è¡“åã‚’å«ã‚ã‚‹

ã€å›ç­”å½¢å¼ã€‘
ä»¥ä¸‹ã®å½¢å¼ã§3ã¤ã®æ®µè½ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

æ®µè½1: [ç¬¬1æ–‡ã‚’ã“ã“ã«è¨˜è¼‰]
æ®µè½2: [ç¬¬2æ–‡ã‚’ã“ã“ã«è¨˜è¼‰]
æ®µè½3: [ç¬¬3æ–‡ã‚’ã“ã“ã«è¨˜è¼‰]

JSONã§ã¯ãªãã€ä¸Šè¨˜ã®å½¢å¼ã§æ—¥æœ¬èªã®æ–‡ç« ã‚’ç›´æ¥ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""
        
        return prompt
    
    def _validate_lead_paragraph(self, paragraph: str) -> Tuple[bool, str]:
        """ãƒªãƒ¼ãƒ‰æ–‡ã®æ–‡æ³•æ¤œè¨¼"""
        
        if not paragraph or len(paragraph.strip()) < 10:
            return False, "æ®µè½ãŒçŸ­ã™ãã‚‹"
        
        paragraph = paragraph.strip()
        
        # ä¸å®Œå…¨ãªæ–‡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        incomplete_patterns = [
            (r'^[^ã€‚ï¼ï¼Ÿ]*[ãŒã¯ã‚’ã«ã§]ã€‚', "åŠ©è©ã®å¾Œã«å¥ç‚¹"),
            (r'[A-Za-z\u30A0-\u30FF\u3040-\u309F\u4E00-\u9FAF]+ãŒã€‚', "ä¸»èªã®å¾Œã™ãã«å¥ç‚¹"),
            (r'[ã¯ãŒã‚’ã«]$', "åŠ©è©ã§çµ‚ã‚ã‚‹æ–‡"),
            (r'[ãŒã¯ã‚’ã«ã§]ã€‚\s*[^ã€‚ï¼ï¼Ÿ]*$', "æ–‡ä¸­ã«ä¸å®Œå…¨ãªå¥ç‚¹")
        ]
        
        for pattern, reason in incomplete_patterns:
            if re.search(pattern, paragraph):
                return False, f"ä¸å®Œå…¨ãªæ–‡: {reason}"
        
        # é‡è¤‡è¡¨ç¾ã®ãƒã‚§ãƒƒã‚¯
        if re.search(r'(ã•ã‚Œã¦ã„ã¾ã™|ã—ã¦ã„ã¾ã™).*?(ã¨ç™ºè¡¨|ã¨å ±å‘Š|ã¨èª¬æ˜)', paragraph):
            return False, "é‡è¤‡è¡¨ç¾ï¼ˆã€œã—ã¦ã„ã¾ã™ã¨ç™ºè¡¨ï¼‰"
        
        # æ–‡ã®é•·ã•ãƒã‚§ãƒƒã‚¯
        if len(paragraph) > 200:
            return False, "æ–‡ãŒé•·ã™ãã‚‹ï¼ˆ200æ–‡å­—è¶…ï¼‰"
        
        # é©åˆ‡ãªèªå°¾ãƒã‚§ãƒƒã‚¯
        proper_endings = ('ã§ã™ã€‚', 'ã¾ã™ã€‚', 'ã¾ã—ãŸã€‚', 'ã¦ã„ã¾ã™ã€‚', 'ã§ã—ãŸã€‚', 'ã¾ã›ã‚“ã€‚')
        if not paragraph.endswith(proper_endings):
            return False, "é©åˆ‡ãªæ•¬èªã§çµ‚ã‚ã£ã¦ã„ãªã„"
        
        return True, "OK"

    async def _generate_llm_lead_text_with_retry(self, articles, edition, max_retries=3):
        """æ–‡æ³•æ¤œè¨¼ä»˜ããƒªãƒ¼ãƒ‰æ–‡ç”Ÿæˆ"""
        
        for attempt in range(max_retries):
            try:
                result = await self._generate_llm_lead_text(articles, edition)
                
                if not result or 'lead_paragraphs' not in result:
                    logger.warning(f"Lead text generation attempt {attempt + 1}: No valid response")
                    continue
                
                # å„æ®µè½ã‚’æ¤œè¨¼
                valid_paragraphs = []
                for i, para in enumerate(result.get('lead_paragraphs', [])):
                    is_valid, reason = self._validate_lead_paragraph(para)
                    if is_valid:
                        valid_paragraphs.append(para)
                        logger.info(f"Lead paragraph {i+1} validated: {para[:50]}...")
                    else:
                        logger.warning(f"Invalid paragraph {i+1}: {reason} - {para[:50]}...")
                
                # æœ€ä½2ã¤ã®æœ‰åŠ¹ãªæ®µè½ãŒå¿…è¦
                if len(valid_paragraphs) >= 2:
                    # 3ã¤ã«æº€ãŸãªã„å ´åˆã¯ã€æœ€å¾Œã®æ®µè½ã‚’è¤‡è£½ãƒ»ä¿®æ­£
                    while len(valid_paragraphs) < 3:
                        if valid_paragraphs:
                            # æœ€å¾Œã®æ®µè½ã‚’ãƒ™ãƒ¼ã‚¹ã«å±•æœ›æ–‡ã‚’ä½œæˆ
                            base_para = valid_paragraphs[-1]
                            if 'ã§ã™ã€‚' in base_para:
                                expansion = "ä»Šå¾Œã‚‚AIæŠ€è¡“ã®é€²æ­©ã¨ä¼æ¥­æ´»ç”¨ãŒåŠ é€Ÿã—ã¦ã„ãã“ã¨ãŒäºˆæƒ³ã•ã‚Œã¾ã™ã€‚"
                            else:
                                expansion = "ã“ã®åˆ†é‡ã§ã®æ›´ãªã‚‹æŠ€è¡“é©æ–°ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚"
                            valid_paragraphs.append(expansion)
                        else:
                            break
            
                    result['lead_paragraphs'] = valid_paragraphs[:3]
                    logger.info(f"Lead text generation successful on attempt {attempt + 1}")
                    return result
                else:
                    logger.warning(f"Lead text generation attempt {attempt + 1}: Only {len(valid_paragraphs)} valid paragraphs")
            
            except Exception as e:
                logger.error(f"Lead text generation attempt {attempt + 1} failed: {e}")
        
        # å…¨ã¦å¤±æ•—ã—ãŸå ´åˆã¯å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.warning("All lead text generation attempts failed, using fallback")
        return self._generate_safe_fallback_lead_text(articles)
    
    def _generate_safe_fallback_lead_text(self, articles):
        """å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿãªãƒªãƒ¼ãƒ‰æ–‡"""
        
        # è¨˜äº‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        article_count = len(articles) if articles else 0
        
        # ä¸»è¦ä¼æ¥­ã‚’æŠ½å‡º
        companies = set()
        for article in articles[:5]:
            if hasattr(article, 'summarized_article') and article.summarized_article:
                title = article.summarized_article.filtered_article.raw_article.title
                # ä¼æ¥­åã‚’ç°¡å˜ã«æŠ½å‡º
                for company in ['OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic', 'Apple']:
                    if company in title:
                        companies.add(company)
        
        main_companies = list(companies)[:2]
        companies_str = "ã€".join(main_companies) if main_companies else "ä¸»è¦AIä¼æ¥­"
        
        # ç¢ºå®Ÿã«æ–‡æ³•çš„ã«æ­£ã—ã„æ–‡ã‚’ç”Ÿæˆ
        fallback_paragraphs = [
            f"AIåˆ†é‡ã«ãŠã„ã¦{companies_str}ã‚’å«ã‚€è¤‡æ•°ã®ä¼æ¥­ãŒæ–°ãŸãªå–ã‚Šçµ„ã¿ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚",
            f"ä»Šå›ã¯{article_count}ä»¶ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚",
            "ã“ã‚Œã‚‰ã®å‹•å‘ã¯ä»Šå¾Œã®AIæŠ€è¡“ã®ç™ºå±•ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãŒäºˆæƒ³ã•ã‚Œã¾ã™ã€‚"
        ]
        
        return {
            'lead_paragraphs': fallback_paragraphs,
            'source': 'safe_fallback',
            'confidence': 0.5
        }

    def _parse_llm_lead_text_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response for lead text generation."""
        
        if not response:
            raise ValueError("Empty response from LLM")
        
        try:
            # First, try to parse as plain text format (new format)
            lines = response.strip().split('\n')
            paragraphs = []
            
            for line in lines:
                line = line.strip()
                
                # Look for lines that start with "æ®µè½" or contain actual content
                if line.startswith('æ®µè½') and ':' in line:
                    # Extract the content after the colon
                    content = line.split(':', 1)[1].strip()
                    if content and len(content) > 20:
                        paragraphs.append(content)
                elif len(line) > 30 and any(line.endswith(ending) for ending in ['ã§ã™ã€‚', 'ã¾ã™ã€‚', 'ã¾ã—ãŸã€‚', 'ã¦ã„ã¾ã™ã€‚', 'ã§ã—ãŸã€‚']):
                    # This looks like a complete sentence
                    paragraphs.append(line)
            
            # If we found paragraphs in plain text format, use them
            if len(paragraphs) >= 2:
                # Clean up each paragraph
                cleaned_paragraphs = []
                for para in paragraphs[:3]:  # Take up to 3 paragraphs
                    # Remove any remaining formatting artifacts
                    para = para.strip('[]ã€Œã€')
                    para = re.sub(r'^\d+\.\s*', '', para)  # Remove numbering
                    
                    # Validate and fix grammar
                    para = self._fix_paragraph_grammar(para)
                    
                    if para and len(para) >= 30:
                        cleaned_paragraphs.append(para)
                
                if len(cleaned_paragraphs) >= 2:
                    # Ensure we have exactly 3 paragraphs
                    while len(cleaned_paragraphs) < 3:
                        cleaned_paragraphs.append(
                            "ä»Šå¾Œã‚‚AIæŠ€è¡“ã®é€²åŒ–ã¨ç”£æ¥­ã¸ã®å¿œç”¨ãŒåŠ é€Ÿã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚"
                        )
                    
                    return {
                        'lead_paragraphs': cleaned_paragraphs[:3],
                        'source': 'plain_text_format',
                        'confidence': 0.8
                    }
            
            # Fallback: Try JSON parsing (legacy format)
            if '```json' in response:
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
                if json_match:
                    response = json_match.group(1)
            
            # Try to parse as JSON
            if response.strip().startswith('{'):
                data = json.loads(response)
                
                if 'lead_paragraphs' in data:
                    lead_paragraphs = data['lead_paragraphs']
                    if isinstance(lead_paragraphs, list) and len(lead_paragraphs) >= 2:
                        cleaned_paragraphs = []
                        for para in lead_paragraphs[:3]:
                            if isinstance(para, str) and len(para.strip()) > 10:
                                para = self._fix_paragraph_grammar(para.strip())
                                if para:
                                    cleaned_paragraphs.append(para)
                        
                        if len(cleaned_paragraphs) >= 2:
                            while len(cleaned_paragraphs) < 3:
                                cleaned_paragraphs.append(
                                    "ã“ã®åˆ†é‡ã§ã®æ›´ãªã‚‹æŠ€è¡“é©æ–°ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚"
                                )
                            
                            return {
                                'lead_paragraphs': cleaned_paragraphs[:3],
                                'source': 'json_format',
                                'confidence': data.get('confidence', 0.8)
                            }
            
            # Final fallback: Extract any good sentences from the response
            all_sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', response)
            valid_sentences = []
            
            for sentence in all_sentences:
                sentence = sentence.strip()
                if len(sentence) > 30 and not any(skip in sentence.lower() for skip in ['json', '```', 'æ®µè½', 'å½¢å¼']):
                    sentence = self._fix_paragraph_grammar(sentence)
                    if sentence:
                        valid_sentences.append(sentence)
            
            if len(valid_sentences) >= 2:
                return {
                    'lead_paragraphs': valid_sentences[:3],
                    'source': 'extracted_sentences',
                    'confidence': 0.6
                }
            
            raise ValueError("Could not extract valid lead text from LLM response")
            
        except Exception as e:
            logger.warning(f"Failed to parse LLM lead text response: {e}")
            raise
    
    def _fix_paragraph_grammar(self, para: str) -> str:
        """Fix common grammar issues in a paragraph."""
        
        if not para:
            return ""
        
        # Remove incomplete sentence patterns
        para = re.sub(r'([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+)ãŒã€‚\s*', '', para)
        para = re.sub(r'([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+)ã¯ã€‚\s*', '', para)
        para = re.sub(r'([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+)ã‚’ã€‚\s*', '', para)
        
        # Fix broken sentence connections
        para = re.sub(r'([A-Za-z]+)ãŒã€‚([^ã€‚]+)', r'\1ãŒ\2', para)
        para = re.sub(r'([A-Za-z]+)ã¯ã€‚([^ã€‚]+)', r'\1ã¯\2', para)
        
        # Remove sentences ending with particles
        if para.endswith(('ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã‹ã‚‰')):
            para = para[:-1].rstrip()
        
        # Ensure proper ending
        if para and not para.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            if any(word in para for word in ['ç™ºè¡¨', 'é–‹å§‹', 'å…¬é–‹', 'å°å…¥', 'æä¾›']):
                para += 'ã—ã¾ã—ãŸã€‚'
            elif any(word in para for word in ['äºˆå®š', 'è¦‹è¾¼ã¿', 'è¨ˆç”»', 'æ–¹é‡']):
                para += 'ã§ã™ã€‚'
            elif any(word in para for word in ['æœŸå¾…', 'äºˆæƒ³', 'è¦‹é€šã—']):
                para += 'ã•ã‚Œã¦ã„ã¾ã™ã€‚'
            else:
                para += 'ã€‚'
        
        # Check length
        if len(para) > 200:
            # Truncate at sentence boundary
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', para)
            if len(sentences) >= 3:
                para = sentences[0] + sentences[1]
        
        return para.strip()
    
    def _truncate_paragraphs_to_limit(self, paragraphs: List[str], char_limit: int) -> List[str]:
        """Truncate paragraphs to fit within character limit while preserving sentence boundaries."""
        
        if not paragraphs:
            return paragraphs
            
        truncated = []
        total_chars = 0
        
        for paragraph in paragraphs:
            if total_chars + len(paragraph) <= char_limit:
                # Entire paragraph fits
                truncated.append(paragraph)
                total_chars += len(paragraph)
            else:
                # Need to truncate this paragraph
                remaining_chars = char_limit - total_chars
                if remaining_chars > 30:  # Only truncate if we have reasonable space
                    try:
                        # Use proper sentence boundary detection
                        truncated_para = truncate_at_sentence_boundary(paragraph, max_length=remaining_chars)
                        if truncated_para and len(truncated_para) > 20:
                            truncated.append(truncated_para)
                        break
                    except:
                        # Fallback to simple truncation
                        truncated_para = paragraph[:remaining_chars]
                        last_sentence_end = truncated_para.rfind('ã€‚')
                        if last_sentence_end > 20:
                            truncated_para = paragraph[:last_sentence_end + 1]
                        else:
                            truncated_para = paragraph[:remaining_chars - 1] + 'â€¦'
                        truncated.append(truncated_para)
                break  # Stop adding more paragraphs
        
        return truncated
    
    def _generate_basic_markdown(self, context: Dict[str, Any]) -> str:
        """Generate basic markdown newsletter without Jinja2 template."""
        
        date = context.get("date", datetime.now())
        lead_text = context.get("lead_text", {})
        articles = context.get("articles", [])
        
        lines = [
            f"# {date.strftime('%Yå¹´%mæœˆ%dæ—¥')} AI NEWS TLDR",
            "",
        ]
        
        # Add lead text
        if lead_text and "title" in lead_text:
            lines.extend([
                f"### {lead_text['title']}",
                "",
            ])
            
            paragraphs = lead_text.get("paragraphs", [])
            for paragraph in paragraphs:
                lines.extend([paragraph, ""])
            
            # Removed: "ãã‚Œã§ã¯å„ãƒˆãƒ”ãƒƒã‚¯ã®è©³ç´°ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚"
        else:
            lines.extend([
                "### æœ¬æ—¥ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "",
                "æœ¬æ—¥ã¯æ³¨ç›®ã™ã¹ãAIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã”ã–ã„ã¾ã›ã‚“ã§ã—ãŸã€‚",
                "",
            ])
        
        # Add table of contents
        lines.extend(["### ç›®æ¬¡", ""])
        
        visible_articles = [a for a in articles if getattr(a, 'japanese_title', None)]
        
        for i, article in enumerate(visible_articles, 1):
            title = article.japanese_title[:45] if len(article.japanese_title) > 45 else article.japanese_title
            update_indicator = " ğŸ†™" if getattr(article, 'is_update', False) else ""
            lines.append(f"{i}. {title}{update_indicator}")
            lines.append("")  # Add blank line after each TOC entry
        
        lines.extend(["", "---", ""])
        
        # Add articles
        for article in visible_articles:
            title = article.japanese_title
            update_indicator = " ğŸ†™" if getattr(article, 'is_update', False) else ""
            lines.extend([f"### {title}{update_indicator}", ""])
            
            # Add summary points
            if (article.summarized_article and 
                article.summarized_article.summary and 
                article.summarized_article.summary.summary_points):
                
                for point in article.summarized_article.summary.summary_points:
                    lines.append(f"- {point}")
                lines.append("")
            
            # å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯
            if hasattr(article, 'citations') and article.citations:
                for citation in article.citations:
                    if isinstance(citation, Citation):
                        lines.extend([self._citation_to_markdown(citation), ""])
                    else:
                        # æ—§ä»•æ§˜ (str) ã«ã‚‚å¯¾å¿œ
                        lines.extend([str(citation), ""])
        
        return "\n".join(lines)
    
    def _citation_to_markdown(self, citation: 'Citation') -> str:
        """Convert Citation object to Markdown format."""
        if hasattr(citation, 'japanese_summary') and citation.japanese_summary:
            return f"> **{citation.source_name}** ({citation.url}): {citation.japanese_summary}"
        else:
            return f"> **{citation.source_name}** ({citation.url}): {citation.title}"
    
    def _generate_html_preview(
        self,
        content: str,
        articles: List[ProcessedArticle]
    ) -> str:
        """Generate HTML preview for newsletter content."""
        # This method should be implemented to generate HTML preview
        # For example, you can use a library like BeautifulSoup to parse the Markdown
        # and convert it to HTML for preview purposes.
        pass
    
    def _extract_article_highlights(self, articles: List[ProcessedArticle]) -> List[str]:
        """Extract PRD-quality highlights from articles for compelling lead text generation."""
        
        highlights = []
        
        # Prioritize articles by impact and specificity
        prioritized_articles = self._prioritize_articles_for_highlights(articles)
        
        for article in prioritized_articles[:3]:  # Top 3 most impactful articles
            try:
                if (article.summarized_article and 
                    article.summarized_article.summary and 
                    article.summarized_article.summary.summary_points):
                    
                    summary_points = article.summarized_article.summary.summary_points
                    title = article.summarized_article.filtered_article.raw_article.title
                    japanese_title = article.japanese_title or "AIæŠ€è¡“æ–°å±•é–‹"
                    
                    # Create sophisticated highlight with multiple details
                    highlight = self._create_sophisticated_highlight(
                        summary_points, title, japanese_title, getattr(article, 'is_update', False)
                    )
                    
                    if highlight and len(highlight) > 50:  # Ensure substantial content
                        highlights.append(highlight)
                        
                        # Add complementary context for major announcements
                        context = self._extract_contextual_impact(summary_points, title)
                        if context and len(highlights) < 3:
                            highlights.append(context)
                            
            except Exception as e:
                logger.warning(f"Failed to extract highlight from article: {e}")
                continue
        
        # If we still need more highlights, create thematic summaries
        if len(highlights) < 2:
            thematic_highlights = self._create_thematic_highlights(articles)
            highlights.extend(thematic_highlights[:3-len(highlights)])
            
        return highlights[:3]  # Maximum 3 highlights for clean lead text
    
    def _prioritize_articles_for_highlights(self, articles: List[ProcessedArticle]) -> List[ProcessedArticle]:
        """Prioritize articles based on impact indicators for highlight generation."""
        
        def calculate_impact_score(article: ProcessedArticle) -> float:
            score = 0.0
            
            try:
                if not (article.summarized_article and article.summarized_article.summary):
                    return 0.0
                    
                summary_points = article.summarized_article.summary.summary_points
                title = article.summarized_article.filtered_article.raw_article.title
                combined_text = title + " " + " ".join(summary_points)
                
                # Major companies/products boost score
                major_entities = [
                    'OpenAI', 'Google', 'Meta', 'Microsoft', 'Apple', 'Amazon', 'NVIDIA', 
                    'Anthropic', 'DeepMind', 'GPT', 'Claude', 'Gemini', 'ChatGPT'
                ]
                for entity in major_entities:
                    if entity in combined_text:
                        score += 2.0
                
                # Impact indicators
                impact_terms = [
                    'ç™ºè¡¨', 'ãƒªãƒªãƒ¼ã‚¹', 'é–‹å§‹', 'å…¬é–‹', 'å°å…¥', 'å®Ÿè£…', 'æ”¹å–„', 'å‘ä¸Š', 
                    'å¼·åŒ–', 'æ‹¡å¤§', 'å±•é–‹', 'æ´»ç”¨', 'å¿œç”¨', 'çªç ´', 'breakthrough'
                ]
                for term in impact_terms:
                    if term in combined_text:
                        score += 1.0
                
                # Numerical data increases credibility
                if re.search(r'\d+%|\d+å€|\d+[å¹´æœˆæ—¥]|\$\d+|Â¥\d+', combined_text):
                    score += 1.5
                
                # Update articles get slight priority
                if getattr(article, 'is_update', False):
                    score += 0.5
                    
                # Confidence score from LLM
                if article.summarized_article.summary.confidence_score:
                    score += article.summarized_article.summary.confidence_score
                    
            except Exception:
                return 0.0
                
            return score
        
        return sorted(articles, key=calculate_impact_score, reverse=True)
    
    def _create_sophisticated_highlight(
        self, 
        summary_points: List[str], 
        title: str, 
        japanese_title: str,
        is_update: bool
    ) -> str:
        """Create a sophisticated, PRD-quality highlight sentence."""
        
        if not summary_points:
            return ""
            
        first_point = summary_points[0]
        
        # Extract key entities and actions with advanced patterns
        companies = self._extract_companies(first_point, title)
        products = self._extract_products(first_point, title)
        actions = self._extract_key_actions(first_point)
        metrics = self._extract_metrics(first_point)
        
        # Build sophisticated highlight
        highlight_parts = []
        
        # Company/product identification
        if companies or products:
            entity = companies[0] if companies else products[0]
            if is_update:
                highlight_parts.append(f"{entity}ãŒæ–°ãŸã«")
            else:
                highlight_parts.append(f"{entity}ãŒ")
        
        # Core action with context (remove company name if already added)
        if actions:
            action = actions[0]
            
            # Remove company name from action if we already added it
            if companies:
                company = companies[0]
                # Remove company name and particles from the action
                action = re.sub(rf'{re.escape(company)}(?:ãŒ|ã¯|ã®)?', '', action).strip()
            
            if metrics:
                metric = metrics[0]
                highlight_parts.append(f"{action}ã€{metric}ã®")
            else:
                highlight_parts.append(action)
        
        # Additional context from second summary point
        if len(summary_points) > 1:
            second_point = summary_points[1]
            context = self._extract_business_context(second_point)
            if context:
                highlight_parts.append(f"ã€‚{context}")
        
        if highlight_parts:
            base_highlight = "".join(highlight_parts)
            # Ensure proper sentence ending
            if not base_highlight.endswith(('ã€‚', 'ã€‚')):
                base_highlight += "ã¨ç™ºè¡¨ã—ã¾ã—ãŸã€‚"
            return base_highlight
        
        # Fallback to enhanced first point
        return first_point if len(first_point) > 30 else ""
    
    def _extract_companies(self, text: str, title: str) -> List[str]:
        """Extract company names with expanded recognition."""
        companies = []
        combined_text = text + " " + title
        
        company_patterns = [
            r'(OpenAI|ChatGPT|GPT-\d+)',
            r'(Google|Alphabet|DeepMind|Gemini)',
            r'(Meta|Facebook|Instagram)',
            r'(Microsoft|Azure|Copilot)',
            r'(Apple|iPhone|iPad|macOS)',
            r'(Amazon|AWS|Alexa)',
            r'(NVIDIA|Tesla|SpaceX)',
            r'(Anthropic|Claude)',
            r'(IBM|Watson)',
            r'(Samsung|LG|Sony)'
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, combined_text, re.IGNORECASE)
            companies.extend(matches)
            
        return list(dict.fromkeys(companies))  # Remove duplicates while preserving order
    
    def _extract_products(self, text: str, title: str) -> List[str]:
        """Extract product/technology names."""
        products = []
        combined_text = text + " " + title
        
        product_patterns = [
            r'(GPT-\d+(?:\.\d+)?|ChatGPT|GPT)',
            r'(Claude(?:-\d+)?)',
            r'(Gemini(?:\s+\d+\.\d+)?)',
            r'(LLaMA|Llama)',
            r'(DALL-E|DALLÂ·E)',
            r'(Midjourney|Stable Diffusion)',
            r'(TensorFlow|PyTorch)',
            r'(Transformer|BERT|T5)'
        ]
        
        for pattern in product_patterns:
            matches = re.findall(pattern, combined_text, re.IGNORECASE)
            products.extend(matches)
            
        return list(dict.fromkeys(products))
    
    def _extract_key_actions(self, text: str) -> List[str]:
        """Extract key business actions."""
        actions = []
        
        action_patterns = [
            # Extract action part after company name, avoiding full sentence capture
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹|é–‹å§‹|å°å…¥|å®Ÿè£…|å±•é–‹|æä¾›))',
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:å‘ä¸Š|æ”¹å–„|å¼·åŒ–|æ‹¡å¤§|å¢—å¤§|å€å¢—))',
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:å€¤ä¸‹ã’|ä¾¡æ ¼.*?å‰Šæ¸›|ã‚³ã‚¹ãƒˆ.*?å‰Šæ¸›))',
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:æ–°æ©Ÿèƒ½|æ–°ã‚µãƒ¼ãƒ“ã‚¹|æ–°è£½å“|æ–°æŠ€è¡“))',
            # Fallback: extract just the action verb and immediate context
            r'((?:\d+[%å€å„„ãƒ‰ãƒ«]*.*?)?(?:ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹|é–‹å§‹|å°å…¥))',
            r'((?:\d+[%å€]*.*?)?(?:å‘ä¸Š|æ”¹å–„|å¼·åŒ–))'
        ]
        
        for pattern in action_patterns:
            matches = re.findall(pattern, text)
            if matches:
                actions.extend([match.strip() for match in matches if len(match.strip()) > 10])
                
        return actions[:3]  # Top 3 most relevant actions
    
    def _extract_metrics(self, text: str) -> List[str]:
        """Extract quantitative metrics."""
        metrics = []
        
        metric_patterns = [
            r'(\d+%(?:.*?(?:å‘ä¸Š|æ”¹å–„|å‰Šæ¸›|å¢—åŠ |æ¸›å°‘))?)',
            r'(\d+å€(?:.*?(?:å‘ä¸Š|æ”¹å–„|å¢—åŠ ))?)',
            r'(\$\d+(?:\.\d+)?(?:[MKB]illion)?)',
            r'(v?\d+\.\d+)',
            r'(\d+(?:GB|TB|PB))',
        ]
        
        for pattern in metric_patterns:
            matches = re.findall(pattern, text)
            metrics.extend(matches)
            
        return metrics
    
    def _extract_business_context(self, text: str) -> str:
        """Extract business/impact context."""
        context_patterns = [
            r'([^ã€‚]*(?:ä¼æ¥­|ãƒ“ã‚¸ãƒã‚¹|æ¥­ç•Œ|å¸‚å ´|ç«¶äº‰)[^ã€‚]*)',
            r'([^ã€‚]*(?:æ´»ç”¨|å¿œç”¨|å°å…¥|å®Ÿç”¨)[^ã€‚]*)',
            r'([^ã€‚]*(?:é–‹ç™ºè€…|ãƒ¦ãƒ¼ã‚¶ãƒ¼|é¡§å®¢)[^ã€‚]*)',
            r'([^ã€‚]*(?:å½±éŸ¿|åŠ¹æœ|æˆæœ|çµæœ)[^ã€‚]*)'
        ]
        
        for pattern in context_patterns:
            match = re.search(pattern, text)
            if match:
                context = match.group(1).strip()
                if len(context) > 15 and len(context) < 100:
                    return context
                    
        return ""
    
    def _extract_contextual_impact(self, summary_points: List[str], title: str) -> str:
        """Create contextual impact statement for complementary information."""
        
        if len(summary_points) < 2:
            return ""
            
        # Look for complementary information in subsequent points
        for point in summary_points[1:]:
            if any(keyword in point for keyword in [
                'ä¸€æ–¹', 'ã¾ãŸ', 'ã•ã‚‰ã«', 'åŠ ãˆã¦', 'åŒæ™‚ã«', 'ä¸¦è¡Œã—ã¦'
            ]):
                # Extract the contrasting or additional information
                context_match = re.search(r'([^ã€‚]*(?:ç™ºè¡¨|é–‹å§‹|å°å…¥|å®Ÿè£…)[^ã€‚]*)', point)
                if context_match and len(context_match.group(1)) > 20:
                    return f"ä¸€æ–¹ã€{context_match.group(1).strip()}ã€‚"
                    
        return ""
    
    def _create_thematic_highlights(self, articles: List[ProcessedArticle]) -> List[str]:
        """Create thematic highlights when individual article highlights are insufficient."""
        
        highlights = []
        
        # Analyze themes across articles
        all_text = []
        for article in articles:
            if (article.summarized_article and 
                article.summarized_article.summary and 
                article.summarized_article.summary.summary_points):
                all_text.extend(article.summarized_article.summary.summary_points)
        
        combined_text = " ".join(all_text)
        
        # Technology themes
        if any(term in combined_text for term in ['LLM', 'GPT', 'è¨€èªãƒ¢ãƒ‡ãƒ«', 'AI']):
            highlights.append("å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰åˆ†é‡ã§ã¯è¤‡æ•°ã®é‡è¦ãªæŠ€è¡“é€²å±•ãŒå ±å‘Šã•ã‚Œã€å®Ÿç”¨åŒ–ã«å‘ã‘ãŸå‹•ããŒåŠ é€Ÿã—ã¦ã„ã¾ã™ã€‚")
        
        # Business application themes  
        if any(term in combined_text for term in ['ä¼æ¥­', 'ãƒ“ã‚¸ãƒã‚¹', 'å°å…¥', 'æ´»ç”¨']):
            highlights.append("ä¼æ¥­ã«ãŠã‘ã‚‹AIæ´»ç”¨ãŒæœ¬æ ¼åŒ–ã—ã€å…·ä½“çš„ãªæ¥­å‹™æ”¹å–„ã‚„æ–°ã‚µãƒ¼ãƒ“ã‚¹å‰µå‡ºã®äº‹ä¾‹ãŒç›¸æ¬¡ã„ã§ç™ºè¡¨ã•ã‚Œã¦ã„ã¾ã™ã€‚")
        
        # Research and development themes
        if any(term in combined_text for term in ['ç ”ç©¶', 'é–‹ç™º', 'æŠ€è¡“', 'æ€§èƒ½']):
            highlights.append("AIç ”ç©¶åˆ†é‡ã§ã¯æ€§èƒ½å‘ä¸Šã¨å®Ÿç”¨æ€§ã‚’ä¸¡ç«‹ã•ã›ã‚‹æŠ€è¡“é–‹ç™ºãŒé€²ã¿ã€ç”£æ¥­ç•Œã¸ã®å½±éŸ¿ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚")
            
        return highlights
    
    def _extract_key_themes(self, articles: List[ProcessedArticle]) -> List[str]:
        """Extract key themes from articles."""
        
        # Common AI themes and their keywords
        theme_keywords = {
            "OpenAIãƒ»GPTé–¢é€£": ["openai", "gpt", "chatgpt"],
            "Googleãƒ»Geminié–¢é€£": ["google", "gemini", "bard"],
            "Anthropicãƒ»Claudeé–¢é€£": ["anthropic", "claude"],
            "ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹": ["ä¼æ¥­", "ãƒ“ã‚¸ãƒã‚¹", "æŠ•è³‡", "è³‡é‡‘èª¿é”", "startup"],
            "ç ”ç©¶ãƒ»æŠ€è¡“": ["ç ”ç©¶", "è«–æ–‡", "æŠ€è¡“", "é–‹ç™º", "breakthrough"],
            "è¦åˆ¶ãƒ»æ”¿ç­–": ["è¦åˆ¶", "æ”¿ç­–", "æ³•å¾‹", "æ”¿åºœ", "policy"],
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿": ["é‡å­", "quantum"],
            "ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹": ["ãƒ­ãƒœãƒƒãƒˆ", "robot", "è‡ªå‹•åŒ–"],
            "ç”»åƒç”Ÿæˆ": ["ç”»åƒç”Ÿæˆ", "stable diffusion", "midjourney", "dall-e"]
        }
        
        # Count theme occurrences
        theme_counts = {}
        
        for article in articles:
            title = article.summarized_article.filtered_article.raw_article.title.lower()
            content = ' '.join(article.summarized_article.summary.summary_points).lower()
            text = f"{title} {content}"
            
            for theme, keywords in theme_keywords.items():
                for keyword in keywords:
                    if keyword in text:
                        theme_counts[theme] = theme_counts.get(theme, 0) + 1
                        break  # Count theme only once per article
        
        # Return top themes
        sorted_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)
        return [theme for theme, count in sorted_themes[:3] if count > 0]
    
    def _generate_specific_theme_title(self, themes: List[str], articles: List[ProcessedArticle]) -> Optional[str]:
        """å…·ä½“çš„ãªãƒ†ãƒ¼ãƒã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆï¼ˆæ±ç”¨è¡¨ç¾ã‚’å›é¿ï¼‰"""
        try:
            # è¨˜äº‹ã‹ã‚‰å…·ä½“çš„ãªä¼æ¥­åãƒ»æŠ€è¡“åã‚’æŠ½å‡º
            companies = []
            technologies = []
            
            for article in articles[:3]:  # ä¸Šä½3è¨˜äº‹ã‹ã‚‰æŠ½å‡º
                raw_article = article.summarized_article.filtered_article.raw_article
                text = f"{raw_article.title} {raw_article.content or ''}"
                
                # ä¼æ¥­åæŠ½å‡º
                company_patterns = [
                    'OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic', 'Apple', 
                    'Amazon', 'Tesla', 'NVIDIA', 'DeepMind', 'LinkedIn', 'GitHub'
                ]
                
                for company in company_patterns:
                    if company.lower() in text.lower() and company not in companies:
                        companies.append(company)
                
                # æŠ€è¡“ãƒ»è£½å“åæŠ½å‡º
                tech_patterns = [
                    'ChatGPT', 'GPT-4', 'Gemini', 'Claude', 'Llama', 'Copilot',
                    'API', 'CLI', 'LLM', 'Agent', 'RAG', 'Transformer'
                ]
                
                for tech in tech_patterns:
                    if tech.lower() in text.lower() and tech not in technologies:
                        technologies.append(tech)
            
            # å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
            if companies and technologies:
                return f"{companies[0]}ãƒ»{technologies[0]}ã‚’ä¸­å¿ƒã¨ã—ãŸæŠ€è¡“é€²å±•"
            elif len(companies) >= 2:
                return f"{companies[0]}ãƒ»{companies[1]}ã«ã‚ˆã‚‹é‡è¦ç™ºè¡¨"
            elif companies:
                return f"{companies[0]}ã®æŠ€è¡“é©æ–°ã¨æ¥­ç•Œå‹•å‘"
            elif technologies:
                return f"{technologies[0]}æŠ€è¡“ã®é€²æ­©ã¨æ´»ç”¨å±•é–‹"
            else:
                return None
                
        except Exception as e:
            logger.warning(f"Specific theme title generation failed: {e}")
            return None
    
    def _generate_fallback_specific_title(self, articles: List[ProcessedArticle]) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®å…·ä½“çš„ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæ±ç”¨è¡¨ç¾ã‚’é¿ã‘ã‚‹ï¼‰"""
        try:
            if articles:
                # æœ€åˆã®è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è¦ç´ ã‚’æŠ½å‡º
                first_article = articles[0]
                raw_title = first_article.summarized_article.filtered_article.raw_article.title
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å…·ä½“çš„è¦ç´ ã‚’æŠ½å‡º
                if any(company in raw_title for company in ['OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic']):
                    return "å¤§æ‰‹AIä¼æ¥­ã«ã‚ˆã‚‹æŠ€è¡“ç™ºè¡¨ãƒ»æˆ¦ç•¥è»¢æ›"
                elif any(tech in raw_title for tech in ['ChatGPT', 'GPT', 'Gemini', 'Claude', 'LLM']):
                    return "ä¸»è¦AIãƒ¢ãƒ‡ãƒ«ã®æ©Ÿèƒ½å¼·åŒ–ãƒ»æ–°å±•é–‹"
                elif any(keyword in raw_title for keyword in ['ç ”ç©¶', 'Research', 'è«–æ–‡']):
                    return "AIç ”ç©¶ã®æœ€æ–°æˆæœãƒ»æŠ€è¡“ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼"
                elif any(keyword in raw_title for keyword in ['æŠ•è³‡', 'è³‡é‡‘', 'funding']):
                    return "AIæ¥­ç•Œã®æŠ•è³‡å‹•å‘ãƒ»ä¼æ¥­å‹•å‘"
                else:
                    # æœ€å¾Œã®æ‰‹æ®µï¼šè¨˜äº‹æ•°ã‚’æ´»ç”¨
                    return f"æ³¨ç›®AIä¼æ¥­{len(articles)}ç¤¾ã®é‡è¦ç™ºè¡¨"
            else:
                return "AIæŠ€è¡“ã®é‡è¦ãªé€²å±•"
                
        except Exception as e:
            logger.warning(f"Fallback specific title generation failed: {e}")
            return "AIæŠ€è¡“ã®é‡è¦ãªé€²å±•"
    
    def _save_newsletter(
        self,
        content: str,
        date: datetime,
        edition: str,
        output_dir: str
    ) -> Path:
        """Save newsletter content to file."""
        
        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Generate filename â€“ include HHMM to avoid overwrite and aid manual comparison
        # "2025-06-22_0135_daily_newsletter.md" ã®ã‚ˆã†ãªå½¢å¼ã«ãªã‚‹
        date_str = date.strftime("%Y-%m-%d_%H%M")
        filename = f"{date_str}_{edition}_newsletter.md"
        
        output_file = output_path / filename
        
        # Write content
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)
                
            if HAS_LOGGER:
                logger.info("Newsletter saved", file=str(output_file))
            else:
                logger.info(f"Newsletter saved: {output_file}")
            
        except Exception as e:
            if HAS_LOGGER:
                logger.error(
                    "Failed to save newsletter",
                    file=str(output_file),
                    error=str(e)
                )
            else:
                logger.error(f"Failed to save newsletter: file={output_file}, error={str(e)}")
            raise
        
        return output_file
    
    def _regex_replace_filter(self, text: str, pattern: str, replacement: str) -> str:
        """Jinja2 filter for regex replacement."""
        return re.sub(pattern, replacement, text)
    
    def _slugify_filter(self, text: str) -> str:
        """Jinja2 filter to create URL-friendly slugs."""
        text = re.sub(r'<[^>]+>', '', text)
        text = text.lower()
        text = re.sub(r'[^\w\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', '-', text)
        text = text.strip('-')
        if len(text) > 50:
            text = text[:50].rstrip('-')
        return text

    def _generate_lead_title_from_articles(self, articles: List[ProcessedArticle], edition: str) -> str:
        """Generate a lead title based on the articles and edition."""
        
        if not articles:
            return "ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹ã¨AIæŠ€è¡“ã®æœ€æ–°å‹•å‘"
        
        # Extract key companies and technologies
        companies = set()
        technologies = set()
        
        for article in articles[:5]:  # Use top 5 articles
            try:
                title = article.summarized_article.filtered_article.raw_article.title
                
                # Extract companies
                for company in ['OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic', 'Apple', 'Amazon', 'LinkedIn']:
                    if company in title:
                        companies.add(company)
                
                # Extract technologies
                for tech in ['ChatGPT', 'GPT-4', 'Gemini', 'Claude', 'AI', 'LLM', 'ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ', 'API']:
                    if tech in title:
                        technologies.add(tech)
                        
            except Exception as e:
                logger.debug(f"Error extracting title info: {e}")
                continue
        
        # Generate title based on content
        companies_list = list(companies)[:2]
        technologies_list = list(technologies)[:2]
        
        if companies_list and technologies_list:
            companies_str = "ãƒ»".join(companies_list)
            tech_str = "ãƒ»".join(technologies_list)
            return f"{companies_str}ã«ã‚ˆã‚‹{tech_str}ã®æœ€æ–°å‹•å‘"
        elif companies_list:
            companies_str = "ãƒ»".join(companies_list)
            return f"{companies_str}ã®AIæŠ€è¡“é–‹ç™ºå‹•å‘"
        elif technologies_list:
            tech_str = "ãƒ»".join(technologies_list)
            return f"{tech_str}ã®ä¼æ¥­æ´»ç”¨ã¨æŠ€è¡“é€²å±•"
        else:
            # Default based on edition
            if edition == "daily":
                return "ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹ã¨AIæŠ€è¡“ã®æœ€æ–°å‹•å‘"
            else:
                return f"AIåˆ†é‡ã®{edition}ãƒ¬ãƒãƒ¼ãƒˆ"

    def _short_title_filter(self, text: str, max_chars: int = 70) -> str:
        """Jinja2 filter to shorten title text."""
        return self._intelligent_truncate(text, max_chars)
    
    def _toc_format_filter(self, text: str) -> str:
        """Jinja2 filter for formatting table of contents entries with improved Japanese truncation."""
        
        if not text:
            return text
        
        # For TOC, we want to show meaningful content but avoid extremely long entries
        max_toc_length = 80  # Optimal length for TOC entries
        
        # If text is already short enough, return as is
        if len(text) <= max_toc_length:
            return text
    
        # Special handling for titles with quoted content like ã€ŒDeep Researchã€
        # Try to include the quoted part if possible
        quote_pattern = r'ã€Œ([^ã€]+)ã€'
        quote_match = re.search(quote_pattern, text)
        
        if quote_match:
            quote_start = quote_match.start()
            quote_end = quote_match.end()
            
            # If the quote appears early enough, try to include it
            if quote_end <= max_toc_length + 10:  # Allow slight overflow for quotes
                # Find a good cut point after the quote
                cut_text = text[:quote_end]
                
                # Look for natural break points after the quote
                remaining = text[quote_end:]
                for break_char in ['ã‚’', 'ãŒ', 'ã«', 'ã§', 'ã¨']:
                    break_pos = remaining.find(break_char)
                    if break_pos != -1 and break_pos < 10:
                        cut_text = text[:quote_end + break_pos + 1]
                        if len(cut_text) <= max_toc_length + 5:
                            return cut_text.rstrip() + 'â€¦'
                        break
                
                # If we can fit the quote, use it
                if len(cut_text) <= max_toc_length + 10:
                    return cut_text.rstrip() + 'â€¦'
        
        # For very long text, try to find natural breaking points
        if len(text) > 120:
            # Look for early natural breaks
            early_break_patterns = [
                (r'ã«ãŠã„ã¦', 'ã€'),
                (r'ã«ã¤ã„ã¦', 'ã€'), 
                (r'ã«é–¢ã—ã¦', 'ã€'),
                (r'ã«ã‚ˆã‚‹', ''),
                (r'ã®ãŸã‚ã®', ''),
                (r'ã‚’é€šã˜ã¦', ''),
                (r'ã«ãŠã‘ã‚‹', ''),
            ]
            
            for keyword, suffix in early_break_patterns:
                pattern = keyword + suffix
                match_pos = text.find(pattern)
                if match_pos != -1 and 20 <= match_pos <= 60:
                    # Cut after the pattern
                    cut_pos = match_pos + len(pattern)
                    return text[:cut_pos].rstrip() + 'â€¦'
        
        # Look for good breaking points within the limit
        snippet = text[:max_toc_length]
        
        # Priority: complete phrases or natural boundaries
        # 1. Try to break at punctuation
        for punct in ['ã€', 'ã€‚', 'ãƒ»']:
            last_punct = snippet.rfind(punct)
            if last_punct > max_toc_length * 0.6:  # At least 60% of target length
                return snippet[:last_punct + 1].rstrip()
        
        # 2. Try to break after particles (but not certain ones that need completion)
        safe_particles = ['ã§', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã«ã¦']
        for particle in safe_particles:
            last_pos = snippet.rfind(particle)
            if last_pos > max_toc_length * 0.6:
                return snippet[:last_pos + len(particle)].rstrip() + 'â€¦'
        
        # 3. Avoid breaking in the middle of important terms
        # Find the last "safe" position (not in the middle of a word/term)
        safe_pos = max_toc_length
        while safe_pos > max_toc_length * 0.7:
            char = text[safe_pos - 1] if safe_pos > 0 else ''
            next_char = text[safe_pos] if safe_pos < len(text) else ''
            
            # Don't break in the middle of:
            # - ASCII words
            # - Katakana words  
            # - Numbers
            if (char.isascii() and char.isalnum() and next_char.isascii() and next_char.isalnum()) or \
               (ord('ã‚¡') <= ord(char) <= ord('ãƒ¶') and ord('ã‚¡') <= ord(next_char) <= ord('ãƒ¶')) or \
               (char.isdigit() and next_char.isdigit()):
                safe_pos -= 1
                continue
            else:
                break
        
        # Use the safe position
        result = text[:safe_pos].rstrip()
        
        # Clean up any trailing particles that make it incomplete
        if result.endswith(('ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã¨')):
            result = result[:-1].rstrip()
        
        # Add ellipsis if we actually truncated
        if len(result) < len(text):
            result += 'â€¦'
        
        return result
    
    async def _generate_article_title(self, article: ProcessedArticle) -> str:
        """
        Generate Japanese title using LLM first, fallback to template-based generation.
        
        Args:
            article: Processed article
            
        Returns:
            Generated Japanese title
        """
        # First try LLM-based generation if available
        if HAS_LLM_ROUTER and self.llm_router:
            try:
                # LLM generates a base summary sentence
                base_summary = await self.llm_router.generate_japanese_title(article)
                
                if base_summary:
                    logger.debug(f"LLM generated base summary for title: {base_summary}")
                    # Programmatically format the headline
                    return self._format_headline_from_summary(base_summary)
            except Exception as e:
                logger.warning(f"LLM title generation failed, falling back: {e}")
        
        # Fallback to using the first summary point
        return self._generate_improved_fallback_title(article)
    
    def _format_headline_from_summary(self, summary_sentence: str) -> str:
        """Formats a summary sentence into a structured headline."""
        
        # Extract a key entity to make the title more specific
        entity_match = re.search(r'ã€Œ([^ã€]+)ã€|\b([A-Z][a-zA-Z0-9]+)\b', summary_sentence)
        entity = ""
        if entity_match:
            entity = entity_match.group(1) or entity_match.group(2)
        
        # Truncate the summary sentence to keep it concise
        truncated_summary = ensure_sentence_completeness(summary_sentence, 60)
        
        if entity and entity not in truncated_summary:
             return f"{entity}ã€{truncated_summary}"
        else:
            return truncated_summary

    def _generate_improved_fallback_title(self, article: ProcessedArticle) -> str:
        """
        Generate a fallback title by creating a concise summary from the first summary point.
        This provides a safe and informative fallback.
        """
        try:
            summary_points = article.summarized_article.summary.summary_points
            if summary_points and summary_points[0]:
                # Use the first bullet point to create a concise title
                first_point = summary_points[0]
                
                # Extract key entities and actions
                # Pattern 1: Extract company/organization names
                company_match = re.search(
                    r'(OpenAI|Google|Meta|Microsoft|Anthropic|Apple|Amazon|NVIDIA|DeepMind|'
                    r'[A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*?)(?:ç¤¾|ãŒ|ã¯|ã®|ã€)',
                    first_point
                )
                company = company_match.group(1) if company_match else None
                
                # Pattern 2: Extract key actions/technologies
                action_patterns = [
                    (r'(\d+å?ã®?(?:AI)?ç ”ç©¶è€…)', 'ç ”ç©¶è€…'),
                    (r'(ãƒˆãƒƒãƒ—.*?ç ”ç©¶è€…)', 'äººæ'),
                    (r'(æ–°æ©Ÿèƒ½|æ–°ã‚µãƒ¼ãƒ“ã‚¹|æ–°æŠ€è¡“)', 'ãƒªãƒªãƒ¼ã‚¹'),
                    (r'(ç™ºè¡¨|å…¬é–‹|é–‹å§‹|å°å…¥|ç²å¾—|é›‡ç”¨)', None),
                    (r'(ChatGPT|Claude|Gemini|GPT-\d+)', 'AI'),
                    (r'(Deep Research|RAG|LLM)', 'æŠ€è¡“'),
                ]
                
                key_action = None
                action_type = None
                for pattern, type_hint in action_patterns:
                    match = re.search(pattern, first_point)
                    if match:
                        key_action = match.group(1)
                        action_type = type_hint
                        break
                
                # Build concise title
                if company and key_action:
                    if 'ç ”ç©¶è€…' in key_action:
                        # Special handling for researcher hiring
                        return f"{company}ãŒ{key_action}ã‚’ç²å¾—"
                    elif action_type:
                        return f"{company}ã®{action_type}{key_action}"
                    else:
                        return f"{company}ãŒ{key_action}"
                elif company:
                    return f"{company}ãŒAIæŠ€è¡“ã‚’å¼·åŒ–"
                elif key_action:
                    # PRDæº–æ‹ : å®Œå…¨ãªæ–‡ç« ã§ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
                    if 'ç™ºè¡¨' in key_action:
                        return f"AIæ¥­ç•Œã§æ–°æŠ€è¡“ç™ºè¡¨ã€{key_action}ã«æ³¨ç›®"
                    elif 'å°å…¥' in key_action:
                        return f"AIæ¥­ç•Œã§æ–°ã‚·ã‚¹ãƒ†ãƒ å°å…¥ã€{key_action}ãŒé€²è¡Œ"
                    else:
                        return f"AIæ¥­ç•Œã§{key_action}ã®å‹•ããŒæ´»ç™ºåŒ–"
                else:
                    # PRDæº–æ‹ : ä¸å®Œå…¨ã‚¿ã‚¤ãƒˆãƒ«ç¦æ­¢ã€è¦ç´„ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å®Œå…¨ãªæ–‡ç« ç”Ÿæˆ
                    clean_point = re.sub(r'^(ã¾ãŸã€|ã•ã‚‰ã«ã€|ä¸€æ–¹ã€)', '', first_point)
                    
                    # ä¸»èªã¨è¿°èªã‚’æŠ½å‡ºã—ã¦å®Œå…¨ãªæ–‡ç« ã«
                    subject_match = re.search(r'([^ã€ã€‚]{1,15})(ãŒ|ã¯|ã®)', clean_point)
                    verb_match = re.search(r'(ç™ºè¡¨|é–‹å§‹|å°å…¥|å¼·åŒ–|ç²å¾—|å®Ÿç¾|æä¾›|é–‹ç™º)', clean_point)
                    
                    if subject_match and verb_match:
                        subject = subject_match.group(1)
                        verb = verb_match.group(1)
                        # åŠ©è©ã®é‡è¤‡ã‚’é¿ã‘ã¦è‡ªç„¶ãªæ–‡ç« ã«
                        if subject.endswith('ãŒ'):
                            subject = subject[:-1]
                        elif subject.endswith('ã¯'):
                            subject = subject[:-1]
                        return f"{subject}ã€æ–°æŠ€è¡“ã‚’{verb}ã§AIæ¥­ç•ŒãŒé€²å±•"
                    else:
                        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šè¦ç´„ãƒã‚¤ãƒ³ãƒˆã®æœ€åˆã®æ–‡ã‚’ä½¿ç”¨
                        first_sentence = re.split(r'[ã€‚ã€]', clean_point)[0]
                        if len(first_sentence) > 10 and len(first_sentence) < 50:
                            return first_sentence + "ã‚’ç™ºè¡¨"
                        else:
                            return "AIæŠ€è¡“ã®é‡è¦ãªé€²å±•ãŒå ±å‘Š"

            # If no summary points, create a title from the content
            content_summary = article.summarized_article.filtered_article.raw_article.content
            if content_summary and len(content_summary) > 20:
                # Sanitize content to avoid template injection
                sanitized_content = content_summary.replace("{", "").replace("}", "")
                return f"ã€AIé–¢é€£ã€‘{sanitized_content[:30]}..."
            
            # If content is also unavailable, use the original title but shortened
            original_title = article.summarized_article.filtered_article.raw_article.title
            if original_title:
                return self._intelligent_truncate(original_title, max_chars=50)
            
            return "AIæŠ€è¡“ã®æœ€æ–°å‹•å‘"
            
        except Exception as e:
            logger.warning(f"Fallback title generation failed: {e}")
            # Ultimate fallback for unexpected errors
            return "AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"

    def _clean_llm_generated_title(self, title: str) -> str:
        """Clean up common LLM title generation artifacts."""
        if not title or not isinstance(title, str):
            return ""
        
        # Remove duplicate phrases first
        title = re.sub(r'(.+?)(ã¨ç™ºè¡¨|ã¨å ±å‘Š|ã¨è¿°ã¹|ã¨èªã£|ã¨è¡¨æ˜)(.*?)\2', r'\1\2\3', title)
        
        # Remove verb endings like "ã€œã¨å ±ã˜ã‚‰ã‚Œã¾ã—ãŸ" or "ã€œã¨ç™ºè¡¨ã—ã¾ã—ãŸ"
        # to ensure the title is a proper headline (ä½“è¨€æ­¢ã‚).
        verb_endings_pattern = r"(?:ã¨å ±ã˜ã‚‰ã‚Œã¾ã—ãŸ|ã¨ç™ºè¡¨ã—ã¾ã—ãŸ|ã¨è¿°ã¹ã¾ã—ãŸ|ã¨èªã‚Šã¾ã—ãŸ|ã¨è¡¨æ˜ã—ã¾ã—ãŸ|ã¨æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸ|æ°ã¯|æ°ã¯ã€|ã‚’ç™ºè¡¨|ã‚’å ±å‘Š)$"
        cleaned_title = re.sub(verb_endings_pattern, '', title).strip()
        
        # Remove incomplete sentence patterns
        incomplete_patterns = r"(?:ã«ã¤ã„ã¦|ã«é–¢ã—ã¦|ã«ãŠã„ã¦|ã«å¯¾ã—ã¦|ã‚’ã‚ãã£ã¦)$"
        cleaned_title = re.sub(incomplete_patterns, '', cleaned_title).strip()
        
        # Additional cleanup for cases where a comma is left at the end
        if cleaned_title.endswith(('ã€', ',', 'ã€‚', 'ï¼š', ':')):
            cleaned_title = cleaned_title[:-1].strip()
        
        # Remove trailing particles that make titles incomplete
        if cleaned_title.endswith(('ãŒ', 'ã‚’', 'ã«', 'ã¯', 'ã§', 'ã¨')):
            cleaned_title = cleaned_title[:-1].strip()
            
        return cleaned_title

    def _is_highly_generic_title(self, title: str) -> bool:
        """Check if the title is too generic or low-quality."""
        # Only reject extremely generic titles
        highly_generic_patterns = [
            r'^AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹$',
            r'^æœ€æ–°AIå‹•å‘$',
            r'^æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹$',
            r'^æ¥­ç•Œå‹•å‘$',
            r'^æœ€æ–°æƒ…å ±$',
            r'^.*ã«ã¤ã„ã¦$',
            r'^.*ã«é–¢ã—ã¦$'
        ]
         
        return any(re.search(pattern, title, re.IGNORECASE) for pattern in highly_generic_patterns)

    def _shorten_summary_points(self, summary_points: List[str]) -> List[str]:
        """Shorten summary points to fit within character limits."""
        
        if not summary_points:
            return []
            
        cleaned_points = []
        
        for point in summary_points:
            if not isinstance(point, str):
                continue
                
            # Strip whitespace
            point = point.strip()
            
            # Skip empty or very short points
            if len(point) < 10:
                continue
            
            # Remove bullet point markers if present
            point = re.sub(r'^[\-\*\â€¢ãƒ»\u2022]\s*', '', point)
            
            # Remove redundant prefixes
            point = re.sub(r'^(ã¾ãŸã€|ã•ã‚‰ã«ã€|ãªãŠã€|ä¸€æ–¹ã€)', '', point)
            
            # Ensure point ends with proper punctuation
            if not point.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                point += 'ã€‚'
            
            cleaned_points.append(point)
        
        return cleaned_points

    def _generate_japanese_title(self, article: ProcessedArticle) -> str:
        """
        Generate a Japanese title for an article using template-based rules.
        
        This is used as a fallback when LLM title generation fails.
        
        Args:
            article: ProcessedArticle containing the article data
            
        Returns:
            Generated Japanese title string
        """
        # Ensure regular expressions are available throughout this method
        import re
         
        try:
            raw_article = article.summarized_article.filtered_article.raw_article
            summary_points = article.summarized_article.summary.summary_points
            
            # First, try to extract from first summary point
            if summary_points and len(summary_points) > 0:
                first_point = summary_points[0]
                
                # Improved extraction with better logic
                # Extract company name from summary
                company_patterns = [
                    r'([A-Za-z][A-Za-z0-9]*(?:\\s+[A-Za-z][A-Za-z0-9]*)*?)(?:ç¤¾|ãŒ|ã¯|ã®).{0,20}?(ç™ºè¡¨|é–‹å§‹|æä¾›|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹)',
                    r'(OpenAI|Meta|Google|Microsoft|Anthropic|Apple|Amazon|Tesla|NVIDIA|AMD)(?:ç¤¾|ãŒ|ã¯|ã®)?',
                    r'([A-Za-z]+)(?:ç¤¾|ãŒ|ã¯|ã®).*?(AI|äººå·¥çŸ¥èƒ½|æ©Ÿæ¢°å­¦ç¿’|ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°)'
                ]
                
                company = None
                for pattern in company_patterns:
                    match = re.search(pattern, first_point)
                    if match:
                        company = match.group(1)
                        break
                
                # Extract key action/technology
                tech_action_patterns = [
                    r'(AGI|ChatGPT|Claude|Gemini|GPT-\\d+|Llama|LLaMA)(?:ã®|ã‚’|ãŒ)?(.{0,15}?)(?:ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹|é–‹ç™º)',
                    r'(?:AI|äººå·¥çŸ¥èƒ½|æ©Ÿæ¢°å­¦ç¿’)(?:ã®|ã‚’|ãŒ)?(.{0,15}?)(?:ç™ºè¡¨|é–‹ç™º|å°å…¥|æ´»ç”¨)',
                    r'(é‡å­|æ³•çš„|åŒ»ç™‚|è‡ªå‹•åŒ–)(?:ã®|ã‚’|ãŒ)?(.{0,10}?)(?:æŠ€è¡“|ã‚·ã‚¹ãƒ†ãƒ |ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ |ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³)',
                    r'(è‘—ä½œæ¨©|è¨´è¨Ÿ|å¥‘ç´„|æŠ•è³‡|è²·å|ææº)(?:ã®|ã‚’|ãŒ)?(.{0,10}?)(?:å•é¡Œ|åˆæ„|ç™ºè¡¨)'
                ]
                
                tech_action = None
                for pattern in tech_action_patterns:
                    match = re.search(pattern, first_point)
                    if match:
                        tech_action = match.group(1) + (match.group(2) if match.group(2) else "")
                        break
                
                # Build natural title
                if company and tech_action:
                    title_candidate = f"{company}ã®{tech_action}é–¢é€£å‹•å‘"
                elif company:
                    title_candidate = f"{company}ã®æœ€æ–°å‹•å‘"
                elif tech_action:
                    title_candidate = f"{tech_action}é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                else:
                    # Extract first meaningful phrase (up to 20 chars)
                    clean_text = re.sub(r'^[ã€‚ã€]*', '', first_point)
                    sentences = re.split(r'[ã€‚ã€]', clean_text)
                    if sentences and len(sentences[0]) > 5:
                        first_sentence = sentences[0][:20]
                        title_candidate = first_sentence + ("..." if len(sentences[0]) > 20 else "")
                    else:
                        title_candidate = "AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                
                # Clean up redundant phrases
                title_candidate = re.sub(r'ã—ã¾ã—ãŸã‚’(ç™ºè¡¨|é–‹å§‹|æä¾›)', r'ã‚’\\1', title_candidate)
                title_candidate = re.sub(r'ãŒã—ã¾ã—ãŸ', 'ãŒ', title_candidate)
                title_candidate = re.sub(r'\\s+', ' ', title_candidate).strip()
                
                # Ensure title doesn't end with incomplete sentences
                title_candidate = re.sub(r'[ã€‚ã€]$', '', title_candidate)
                if not re.search(r'[é–¢ãƒ‹ç™ºè¡¨é–‹å§‹å°å…¥æŠ€è¡“å‹•å‘]', title_candidate):
                    title_candidate += "é–¢é€£"
                
                return title_candidate
            
            # Extract from original title as last resort
            title = raw_article.title if raw_article.title else ""
            
            # Clean up English title and create Japanese equivalent
            if title:
                # Remove common prefixes and suffixes
                cleaned_title = re.sub(r'^[A-Za-z0-9\s\-\.]+:', '', title)  # Remove "Company:" prefixes
                cleaned_title = re.sub(r'https?://\S+', '', cleaned_title)  # Remove URLs
                cleaned_title = cleaned_title.strip()
                
                # Map common English terms to Japanese
                term_mappings = {
                    "announces": "ãŒç™ºè¡¨",
                    "releases": "ãŒãƒªãƒªãƒ¼ã‚¹", 
                    "launches": "ãŒé–‹å§‹",
                    "introduces": "ãŒå°å…¥",
                    "AI": "AI",
                    "OpenAI": "OpenAI",
                    "Google": "Google",
                    "Meta": "Meta",
                    "Microsoft": "Microsoft"
                }
                
                # Simple mapping for common patterns
                for eng, jp in term_mappings.items():
                    if eng.lower() in cleaned_title.lower():
                        # Extract company name before the action
                        words = cleaned_title.split()
                        if len(words) > 0:
                            company = words[0]
                            if company in ["OpenAI", "Google", "Meta", "Microsoft", "Apple", "Amazon", "Tesla", "IBM"]:
                                if "AI" in cleaned_title or "model" in cleaned_title.lower():
                                    return f"{company}ã®æ–°AIæŠ€è¡“ç™ºè¡¨"
                                else:
                                    return f"{company}ã®æœ€æ–°å‹•å‘"
                
                # If no mapping found, use intelligent truncation
                if len(cleaned_title) > 25:
                    return self._intelligent_truncate(cleaned_title, max_chars=25) + "é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                else:
                    return cleaned_title + "é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                
            return "AIæŠ€è¡“ã®æœ€æ–°ç™ºè¡¨"
                
        except Exception as e:
            logger.warning(f"Template title generation failed: {e}")
            return "AIæŠ€è¡“ã®æœ€æ–°ç™ºè¡¨"
    
    def _extract_key_topic_from_summary(self, summary_point: str) -> Optional[str]:
        """
        Extract key topic/action from a summary point for title generation.
        
        Args:
            summary_point: A single summary point text
            
        Returns:
            Key topic string or None if not found
        """
        
        if not summary_point or not isinstance(summary_point, str):
            return None
        
        # Pattern to extract key actions/topics from Japanese summary points
        action_patterns = [
            r'(.{5,20}?ã‚’ç™ºè¡¨)',  # "XXXã‚’ç™ºè¡¨"
            r'(.{5,20}?ã‚’ãƒªãƒªãƒ¼ã‚¹)',  # "XXXã‚’ãƒªãƒªãƒ¼ã‚¹"
            r'(.{5,20}?ã‚’é–‹å§‹)',  # "XXXã‚’é–‹å§‹"
            r'(.{5,20}?ã‚’å°å…¥)',  # "XXXã‚’å°å…¥"
            r'(.{5,20}?ãŒå‘ä¸Š)',  # "XXXãŒå‘ä¸Š"
            r'(.{5,20}?ã‚’æ”¹å–„)',  # "XXXã‚’æ”¹å–„"
            r'(.{5,20}?ã«æˆåŠŸ)',  # "XXXã«æˆåŠŸ"
            r'(.{5,20}?ã‚’å®Ÿç¾)',  # "XXXã‚’å®Ÿç¾"
            r'(.{5,20}?ãŒå¯èƒ½)',  # "XXXãŒå¯èƒ½"
        ]
        
        for pattern in action_patterns:
            match = re.search(pattern, summary_point)
            if match:
                topic = match.group(1).strip()
                # Clean up common prefixes
                topic = re.sub(r'^(æ–°ã—ã„|æœ€æ–°ã®|æ¬¡ä¸–ä»£ã®)', '', topic)
                if len(topic) >= 5:  # Ensure meaningful length
                    return topic
        
        # Fallback to noun phrase extraction
        noun_patterns = [
            r'([A-Za-z0-9]{3,}[ã®]?(?:API|SDK|ã‚µãƒ¼ãƒ“ã‚¹|æ©Ÿèƒ½|æŠ€è¡“|ã‚·ã‚¹ãƒ†ãƒ |ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ))',
            r'([ã-ã‚“]{2,}[ã®]?(?:æ©Ÿèƒ½|æŠ€è¡“|ã‚µãƒ¼ãƒ“ã‚¹|ã‚·ã‚¹ãƒ†ãƒ |æ€§èƒ½|ç²¾åº¦))',
        ]
        
        for pattern in noun_patterns:
            match = re.search(pattern, summary_point)
            if match:
                return match.group(1).strip()
        
        return None

    def _sanitize_heading(self, heading: str) -> str:
        """
        Sanitize heading text for use in table of contents and section headers.
        
        Args:
            heading: Raw heading text
            
        Returns:
            Sanitized heading text
        """
        
        if not heading:
            return ""
        
        # Remove any markdown syntax
        heading = re.sub(r'[#*_`\[\]]', '', heading)
        
        # Remove URLs
        heading = re.sub(r'https?://\S+', '', heading)
        
        # Clean up extra whitespace
        heading = ' '.join(heading.split())
        
        # Remove trailing punctuation
        heading = re.sub(r'[.,:;!?]+$', '', heading)
        
        return heading.strip()
    
    def _normalize_terminology(self, text: str) -> str:
        """
        Normalize AI/tech terminology for consistency.
        
        Args:
            text: Text to normalize
            
        Returns:
            Normalized text with consistent terminology
        """
        
        if not text:
            return ""
        
        # Common terminology normalizations
        normalizations = {
            # AI model names
            r'\bGPT[\s-]?4[\s-]?o[\s-]?mini\b': 'GPT-4o-mini',
            r'\bGPT[\s-]?4[\s-]?o\b': 'GPT-4o',
            r'\bGPT[\s-]?3\.5\b': 'GPT-3.5',
            r'\bClaude[\s-]?3\.5\b': 'Claude 3.5',
            r'\bGemini[\s-]?Pro\b': 'Gemini Pro',
            
            # Company names
            r'\bOpenAI\b': 'OpenAI',
            r'\bAnthropic\b': 'Anthropic',
            r'\bGoogle\b': 'Google',
            r'\bMicrosoft\b': 'Microsoft',
            r'\bMeta\b': 'Meta',
            
            # Technology terms
            r'\bAI\b': 'AI',
            r'\bML\b': 'ML',
            r'\bLLM\b': 'LLM',
            r'\bAPI\b': 'API',
        }
        
        for pattern, replacement in normalizations.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text

    def _cleanup_summary_points(self, summary_points: List[str]) -> List[str]:
        """
        Clean up summary points by removing unwanted content.
        
        Args:
            summary_points: List of summary points to clean
            
        Returns:
            Cleaned list of summary points
        """
        
        if not summary_points:
            return []
        
        cleaned_points = []
        
        for point in summary_points:
            if not isinstance(point, str):
                continue
                
            # Strip whitespace
            point = point.strip()
            
            # Skip empty or very short points
            if len(point) < 10:
                continue
            
            # Remove bullet point markers if present
            point = re.sub(r'^[\-\*\â€¢ãƒ»\u2022]\s*', '', point)
            
            # Remove redundant prefixes
            point = re.sub(r'^(ã¾ãŸã€|ã•ã‚‰ã«ã€|ãªãŠã€|ä¸€æ–¹ã€)', '', point)
            
            # Ensure point ends with proper punctuation
            if not point.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                point += 'ã€‚'
            
            cleaned_points.append(point)
        
        return cleaned_points

    def _validate_and_fix_newsletter_content(
        self, 
        content: str, 
        articles: List[ProcessedArticle]
    ) -> str:
        """
        Validate and fix newsletter content for quality issues.
        
        Args:
            content: Generated newsletter content
            articles: Source articles for validation
            
        Returns:
            Fixed newsletter content
        """
        
        if not content:
            return ""
        
        # Fix common formatting issues
        content = re.sub(r'\n{3,}', '\n\n', content)  # Limit consecutive newlines
        content = re.sub(r'[ \t]+$', '', content, flags=re.MULTILINE)  # Remove trailing whitespace
        content = content.strip()
        
        # Ensure proper line endings
        if not content.endswith('\n'):
            content += '\n'
        
        # Validate that all articles are represented
        missing_articles = []
        for article in articles:
            if article.japanese_title:
                # Check if article title appears in content
                if article.japanese_title not in content:
                    missing_articles.append(article)
        
        if missing_articles:
            logger.warning(f"Found {len(missing_articles)} articles missing from newsletter content")
        
        return content

    async def _consolidate_multi_source_articles(self, articles: List[ProcessedArticle]) -> List[ProcessedArticle]:
        """Consolidate temporary multi/related article lists into proper citation data.

        The clustering stage may store helper lists (`_multi_articles_tmp`, `_related_articles_tmp`) on
        individual `ProcessedArticle` instances so that we can later attach richer citation
        information.  This helper walks through the provided articles, generates the corresponding
        `Citation` objects (using ``CitationGenerator`` when available), persists them to the
        article, and finally removes the temporary attributes to avoid unintended side-effects.

        Args:
            articles: List of `ProcessedArticle` objects coming from the workflow.

        Returns:
            The same list with citation information consolidated.
        """

        if not articles:
            return articles

        # If citation generator failed to initialise, just strip helper attributes and return.
        if not getattr(self, "citation_generator", None):
            for art in articles:
                for tmp_attr in ("_multi_articles_tmp", "_related_articles_tmp"):
                    if hasattr(art, tmp_attr):
                        delattr(art, tmp_attr)
            return articles

        # Helper to deduplicate citations by URL while preserving order
        def _dedup_citations(citations_list):
            seen = set()
            unique = []
            for cit in citations_list:
                url = getattr(cit, "url", None)
                if url and url not in seen:
                    seen.add(url)
                    unique.append(cit)
            return unique

        for article in articles:
            try:
                # Handle multi-source representative articles
                if hasattr(article, "_multi_articles_tmp") and not article.is_multi_source_enhanced:
                    cluster_articles = getattr(article, "_multi_articles_tmp", [])

                    # Generate up to 3 citations including the representative itself
                    citations = await self.citation_generator.generate_multi_source_citations(
                        representative_article=article,
                        cluster_articles=cluster_articles,
                        max_citations=3,
                    )

                    # Persist citations and metadata
                    article.citations = _dedup_citations(citations)
                    article.is_multi_source_enhanced = True

                    # Preserve list of source URLs for transparency
                    article.source_urls = [
                        art.summarized_article.filtered_article.raw_article.url for art in cluster_articles
                    ]

                    # Clean up temporary attribute
                    delattr(article, "_multi_articles_tmp")

                # Handle single articles that have a few "related" articles for extra citations
                if hasattr(article, "_related_articles_tmp"):
                    related_articles = getattr(article, "_related_articles_tmp", [])

                    # Merge with any existing citations while obeying the 3-citation limit
                    existing_citations = list(article.citations) if article.citations else []

                    additional_citations = await self.citation_generator.generate_citations(
                        article=article,
                        related_sources=[ra.summarized_article.filtered_article.raw_article for ra in related_articles],
                        max_citations=max(0, 3 - len(existing_citations)),
                    )

                    article.citations = _dedup_citations(existing_citations + additional_citations)

                    delattr(article, "_related_articles_tmp")

            except Exception as e:
                logger.warning(
                    "Failed consolidating multi-source citations",
                    error=str(e),
                )

        return articles

    # ------------------------------------------------------------------
    # Quality filtering & deduplication helpers (re-added)
    # ------------------------------------------------------------------

    def _filter_articles_by_quality(
        self,
        articles: List[ProcessedArticle],
        quality_threshold: float = 0.35,
    ) -> List[ProcessedArticle]:
        """Filter articles whose AI é–¢é€£åº¦ (relevance score) >= threshold.

        If an article is missing `ai_relevance_score`, it is kept to avoid
        accidental loss of content during early development phases.
        The resulting list is sorted by relevance score (desc).
        """

        if not articles:
            return []

        def _score(art: ProcessedArticle) -> float:
            try:
                return art.summarized_article.filtered_article.ai_relevance_score
            except Exception:
                return 0.5  # neutral default

        filtered = [a for a in articles if _score(a) >= quality_threshold]

        # Always ensure at least one article remains to avoid empty newsletter
        if not filtered:
            filtered = articles[:]

        return sorted(filtered, key=_score, reverse=True)

    def _deduplicate_articles(self, articles: List[ProcessedArticle]) -> List[ProcessedArticle]:
        """Remove articles flagged as duplicates or with identical raw IDs."""

        unique_articles = []
        seen_ids = set()

        for art in articles:
            try:
                raw_id = art.summarized_article.filtered_article.raw_article.id
            except Exception:
                raw_id = None

            # Skip if duplicate checker marked it or ID seen already
            is_dup_flag = False
            try:
                is_dup_flag = getattr(art.duplicate_check, "is_duplicate", False)
            except Exception:
                pass

            if is_dup_flag:
                continue

            if raw_id and raw_id in seen_ids:
                continue

            unique_articles.append(art)
            if raw_id:
                seen_ids.add(raw_id)

        return unique_articles

    # ------------------------------------------------------------------
    # Text truncation helper
    # ------------------------------------------------------------------

    def _intelligent_truncate(self, text: str, max_chars: int = 70) -> str:
        """Smartly truncate *text* within *max_chars* keeping sentence integrity."""
        return ensure_sentence_completeness(text, max_chars)

    def save_to_file(self, content: str, output_dir: str = "drafts") -> str:
        """Save newsletter content to markdown file with organized directory structure."""
        
        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")
        
        # Create organized directory structure: drafts/YYYY/MM/
        year = datetime.now().strftime("%Y")
        month = datetime.now().strftime("%m")
        organized_dir = os.path.join(output_dir, year, month)
        
        # Ensure directory exists
        os.makedirs(organized_dir, exist_ok=True)
        
        filename = f"{timestamp}_daily_newsletter.md"
        filepath = os.path.join(organized_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"Newsletter saved to: {filepath}")
            return filepath
            
        except Exception as e:
            self.logger.error(f"Failed to save newsletter: {str(e)}")
            # Fallback to flat structure if organized save fails
            fallback_path = os.path.join(output_dir, filename)
            with open(fallback_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return fallback_path


async def generate_markdown_newsletter(
    articles: List[ProcessedArticle],
    edition: str = "daily",
    processing_summary: Dict = None,
    output_dir: str = "drafts",
    templates_dir: str = "src/templates"
) -> NewsletterOutput:
    """
    Convenience function to generate newsletter.
    
    Args:
        articles: Processed articles
        edition: Newsletter edition
        processing_summary: Processing statistics
        output_dir: Output directory
        templates_dir: Templates directory
    
    Returns:
        NewsletterOutput
    """
    
    generator = NewsletterGenerator(templates_dir)
    return await generator.generate_newsletter(
        articles, edition, processing_summary, output_dir
    )

# ---------------------------------------------------------------------------
# Module-level helper functions
# ---------------------------------------------------------------------------

def ensure_sentence_completeness(text: str, max_chars: int = 60) -> str:
    """
    æ—¥æœ¬èªã«æœ€é©åŒ–ã•ã‚ŒãŸæ–‡å¢ƒç•Œæ¤œå‡ºã«ã‚ˆã‚‹è‡ªç„¶ãªåˆ‡æ–­å‡¦ç†ã€‚
    ç›®æ¬¡ã®å¯èª­æ€§ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€é©åˆ‡ãªåˆ‡æ–­ç‚¹ã‚’å„ªå…ˆåº¦é †ã«æ¢ç´¢ã€‚
    """

    if not text:
        return ""

    if len(text) <= max_chars:
        return text

    snippet = text[:max_chars]

    # å„ªå…ˆåº¦é †ã®åˆ‡æ–­ãƒã‚¤ãƒ³ãƒˆå®šç¾©
    cut_strategies = [
        # 1. å®Œå…¨ãªæ–‡å¢ƒç•Œï¼ˆæœ€å„ªå…ˆï¼‰
        {
            'patterns': [r'[ã€‚ï¼ï¼Ÿ]'],
            'include_match': True,
            'min_length_ratio': 0.3,
            'add_ellipsis': False,
            'description': 'æ–‡æœ«å¥èª­ç‚¹'
        },
        
        # 2. è‡ªç„¶ãªä¼‘æ­¢ç‚¹
        {
            'patterns': [r'[ã€ï¼Œ]'],
            'include_match': True, 
            'min_length_ratio': 0.4,
            'add_ellipsis': False,
            'description': 'èª­ç‚¹'
        },
        
        # 3. æ‹¬å¼§ã®å¤–å´ï¼ˆæƒ…å ±ã®å®Œçµæ€§ã‚’ä¿æŒï¼‰
        {
            'patterns': [r'[ï¼‰ã€ã€]'],
            'include_match': True,
            'min_length_ratio': 0.3,
            'add_ellipsis': False,
            'description': 'æ‹¬å¼§çµ‚äº†'
        },
        
        # 4. æ¥ç¶šè©ã®å‰ï¼ˆè«–ç†æ§‹é€ ã‚’ä¿æŒï¼‰
        {
            'patterns': [r'(?=ã¾ãŸ|ã•ã‚‰ã«|ä¸€æ–¹|ãªãŠ|ãŸã ã—|ã—ã‹ã—|ãã—ã¦)'],
            'include_match': False,
            'min_length_ratio': 0.4,
            'add_ellipsis': True,
            'description': 'æ¥ç¶šè©å‰'
        },
        
        # 5. åŠ©è©ã®å¾Œï¼ˆæœ€å¾Œã®æ‰‹æ®µã€ä½†ã—ä¸å®Œå…¨æ„Ÿã‚’é¿ã‘ã‚‹ï¼‰
        {
            'patterns': [r'(?<=[ã®ã§ã‚‚ã‚„])(?!\s*[ã¯ãŒã‚’ã«])'],  # ã€Œã¯ã€ã€ŒãŒã€ç­‰ã®ç›´å‰ã¯é¿ã‘ã‚‹
            'include_match': False,
            'min_length_ratio': 0.5,
            'add_ellipsis': True,
            'description': 'åŠ©è©å¾Œï¼ˆå®‰å…¨ãªä½ç½®ï¼‰'
        }
    ]

    min_acceptable_length = max(10, int(max_chars * 0.3))
    
    # å„æˆ¦ç•¥ã‚’è©¦è¡Œ
    for strategy in cut_strategies:
        for pattern in strategy['patterns']:
            matches = list(re.finditer(pattern, snippet))
            if matches:
                # æœ€å¾Œã®ãƒãƒƒãƒã‚’ä½¿ç”¨
                last_match = matches[-1]
                cut_pos = last_match.end() if strategy['include_match'] else last_match.start()
                
                # æœ€å°é•·è¦ä»¶ãƒã‚§ãƒƒã‚¯
                if cut_pos >= min_acceptable_length:
                    result = snippet[:cut_pos].rstrip()
                    
                    # ä¸å®Œå…¨ãªåŠ©è©ã§çµ‚ã‚ã‚‹å ´åˆã¯ä¿®æ­£
                    if result.endswith(('ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã‹ã‚‰')):
                        result = result[:-1].rstrip()
                    
                    # æ®‹ã‚Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç¢ºèª
                    remaining = text[cut_pos:].strip()
                    needs_ellipsis = (
                        strategy['add_ellipsis'] and 
                        remaining and 
                        len(remaining) > 8 and
                        not result.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ'))
                    )
                    
                    final_result = result + ('â€¦' if needs_ellipsis else '')
                    
                    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆç›®æ¬¡ã®å“è³ªç¢ºèªç”¨ï¼‰
                    logger.debug(
                        f"TOC truncation: '{strategy['description']}' at {cut_pos}/{max_chars} chars"
                    )
                    
                    return final_result

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå®‰å…¨ãªä½ç½®ã§ã®åˆ‡æ–­
    # å˜èªå¢ƒç•Œã‚’æ¢ã™ï¼ˆæ—¥æœ¬èªã§ã¯ç©ºç™½ã‚„å¥èª­ç‚¹ï¼‰
    safe_positions = []
    for i in range(len(snippet) - 1, min_acceptable_length, -1):
        char = snippet[i]
        if char in 'ã€€ ã€ï¼Œã€‚ï¼ï¼Ÿï¼‰ã€ã€':
            safe_positions.append(i + (1 if char in 'ã€ï¼Œã€‚ï¼ï¼Ÿï¼‰ã€ã€' else 0))
        elif i > 0 and snippet[i-1] in 'ã®ã§ã‚‚ã‚„' and char not in 'ã¯ãŒã‚’ã«':
            safe_positions.append(i)
    
    if safe_positions:
        cut_pos = safe_positions[0]  # æœ€ã‚‚å¾Œã‚ã®å®‰å…¨ãªä½ç½®
        result = snippet[:cut_pos].rstrip()
        
        # æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
        if not result.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', 'ï¼‰', 'ã€', 'ã€')):
            remaining = text[cut_pos:].strip()
            if remaining and len(remaining) > 8:
                result += 'â€¦'
        
        return result

    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå˜ç´”åˆ‡æ–­
    fallback_pos = max_chars - 3
    while fallback_pos > min_acceptable_length and snippet[fallback_pos] in 'ã¯ãŒã‚’ã«':
        fallback_pos -= 1
    
    return snippet[:fallback_pos].rstrip() + 'â€¦'