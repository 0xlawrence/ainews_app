"""
Newsletter generation utilities.

This module handles the generation of Markdown newsletters from processed articles.
"""

import asyncio
import json
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any

# Optional dependencies
try:
    import markdown  # type: ignore
    HAS_MARKDOWN = True
except ImportError:
    HAS_MARKDOWN = False

try:
    import jinja2
    HAS_JINJA2 = True
except ImportError:
    HAS_JINJA2 = False
    jinja2 = None

try:
    from src.models.schemas import Citation, NewsletterOutput, ProcessedArticle
    HAS_SCHEMAS = True
except ImportError:
    HAS_SCHEMAS = False

try:
    from src.utils.logger import setup_logging
    from src.utils.text_processing import remove_duplicate_patterns
    logger = setup_logging()
    HAS_LOGGER = True
except ImportError:
    # Fallback logger
    import logging
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    HAS_LOGGER = False

# Import text limits for title length constraints
try:
    from src.config.settings import get_settings
    HAS_TEXT_LIMITS = True
except ImportError:
    TEXT_LIMITS = {'title_short': 120}  # Fallback value for Japanese text
    HAS_TEXT_LIMITS = False

try:
    from src.llm.llm_router import LLMRouter
    from src.utils.citation_generator import CitationGenerator
    HAS_LLM_ROUTER = True
except ImportError:
    HAS_LLM_ROUTER = False
    LLMRouter = None

try:
    import numpy as np
    from numpy import ndarray
    from numpy.linalg import norm
    from sklearn.metrics.pairwise import cosine_similarity

    from src.models.schemas import NewsletterOutput, ProcessedArticle
    from src.utils.embedding_manager import EmbeddingManager
    from src.utils.logger import setup_logging
    from src.utils.supabase_client import SupabaseManager
    HAS_ADVANCED_FEATURES = True
except ImportError:
    HAS_ADVANCED_FEATURES = False

# Remove text_processing import dependency that was causing HAS_LLM_ROUTER to fail
# ensure_sentence_completeness is now defined as module-level function below


class NewsletterGenerator:
    """Generates Markdown newsletters from processed articles."""

    def __init__(self, templates_dir: str = "src/templates"):
        """
        Initialize newsletter generator.

        Args:
            templates_dir: Directory containing Jinja2 templates
        """
        self.templates_dir = Path(templates_dir)
        self.settings = get_settings()

        if HAS_LLM_ROUTER:
            self.llm_router = LLMRouter()
        else:
            self.llm_router = None

        # Initialize citation generator for later use
        try:
            from src.utils.citation_generator import (
                CitationGenerator,  # local import to avoid circular refs
            )
            self.citation_generator = CitationGenerator(self.llm_router)
        except Exception:
            # In case CitationGenerator import fails (e.g. missing deps) fallback to None
            self.citation_generator = None

        # Setup Jinja2 environment (if available)
        if HAS_JINJA2 and jinja2:
            self.jinja_env = jinja2.Environment(
                loader=jinja2.FileSystemLoader(str(self.templates_dir)),
                autoescape=False,  # We want raw markdown
                trim_blocks=False,
                lstrip_blocks=False,
            )
            # Add custom filters
            self.jinja_env.filters['regex_replace'] = self._regex_replace_filter
            self.jinja_env.filters['slugify'] = self._slugify_filter
            self.jinja_env.filters['short_title'] = self._short_title_filter
            self.jinja_env.filters['toc_format'] = self._toc_format_filter
        else:
            self.jinja_env = None

    async def generate_newsletter(
        self,
        articles: list[ProcessedArticle],
        edition: str = "daily",
        processing_summary: dict = None,
        output_dir: str = "drafts",
        quality_threshold: float = 0.25
    ) -> NewsletterOutput:
        """
        Generate newsletter from processed articles.

        Args:
            articles: List of processed articles
            edition: Newsletter edition type
            processing_summary: Processing statistics
            output_dir: Output directory for generated files
            quality_threshold: Minimum quality score for inclusion (0.0-1.0)

        Returns:
            NewsletterOutput with generated content
        """

        if HAS_LOGGER:
            logger.info(
                "Starting newsletter generation",
                articles_count=len(articles),
                edition=edition,
                quality_threshold=quality_threshold
            )
        else:
            logger.info(
                f"Starting newsletter generation: {len(articles)} articles, edition={edition}, quality_threshold={quality_threshold}"
            )

        # Apply quality filtering with dynamic threshold adjustment for article count assurance
        min_articles_target = 10  # F-5è¦ä»¶æº–æ‹ : ä¸Šä½10ä»¶ã¾ã§æœ¬æ–‡æ•´å½¢
        max_articles_target = 10  # ä¸Šé™ã‚’10æœ¬ã«æ‹¡å¤§

        # [CRITICAL FIX] Apply context analysis FIRST, then consolidate multi-source articles
        # This ensures that workflow-provided clustering data (_multi_articles_tmp) is properly processed
        logger.info("Starting context analysis and multi-source consolidation...")

        # Step 1: Apply context analysis (sequel detection)
        articles = await self._apply_context_analysis(articles)

        # Step 2: Consolidate multi-source articles (handles workflow clustering results)
        articles = await self._consolidate_multi_source_articles(articles)

        filtered_articles = self._filter_articles_by_quality(articles, quality_threshold)

        # Dynamic threshold adjustment if we don't have enough articles
        original_threshold = quality_threshold
        adjustment_attempts = 0
        max_attempts = 3

        while len(filtered_articles) < min_articles_target and adjustment_attempts < max_attempts:
            # Gradually lower threshold to capture more articles
            quality_threshold *= 0.9  # Reduce by 10% each iteration
            adjustment_attempts += 1

            if HAS_LOGGER:
                logger.info(
                    "Insufficient articles, adjusting quality threshold",
                    current_count=len(filtered_articles),
                    target_minimum=min_articles_target,
                    old_threshold=original_threshold if adjustment_attempts == 1 else quality_threshold / 0.9,
                    new_threshold=quality_threshold,
                    attempt=adjustment_attempts
                )

            # Re-filter with new threshold
            filtered_articles = self._filter_articles_by_quality(articles, quality_threshold)

            # Break if we have enough articles or hit minimum viable threshold
            if len(filtered_articles) >= min_articles_target or quality_threshold < 0.15:
                break

        # Apply deduplication with detailed logging
        pre_dedup_count = len(filtered_articles)
        filtered_articles = self._deduplicate_articles(filtered_articles)
        post_dedup_count = len(filtered_articles)

        if HAS_LOGGER:
            logger.info(
                "Article deduplication completed",
                pre_dedup_count=pre_dedup_count,
                post_dedup_count=post_dedup_count,
                removed_duplicates=pre_dedup_count - post_dedup_count
            )

        # Final check - if still insufficient after deduplication, try one more relaxed filter
        if len(filtered_articles) < min_articles_target and quality_threshold > 0.1:
            if HAS_LOGGER:
                logger.warning(
                    "Still insufficient articles after deduplication, final threshold relaxation",
                    post_dedup_count=len(filtered_articles),
                    target_minimum=min_articles_target
                )

            # Emergency threshold for minimum viable newsletter
            emergency_threshold = max(0.1, quality_threshold * 0.7)
            emergency_filtered = self._filter_articles_by_quality(articles, emergency_threshold)
            emergency_deduped = self._deduplicate_articles(emergency_filtered)

            if len(emergency_deduped) > len(filtered_articles):
                filtered_articles = emergency_deduped
                quality_threshold = emergency_threshold

        if HAS_LOGGER:
            logger.info(
                "Quality filtering completed",
                original_count=len(articles),
                filtered_count=len(filtered_articles),
                removed_count=len(articles) - len(filtered_articles)
            )
        else:
            logger.info(f"Quality filtering completed: {len(articles)} -> {len(filtered_articles)} ({len(articles) - len(filtered_articles)} removed)")

        # Generate newsletter content
        newsletter_date = datetime.now()

        # Generate Japanese titles for each article in parallel
        async def process_single_article(article):
            # PRD F-15 COMPLIANCE: Generate citation-based summary if citations exist
            if (hasattr(article, 'citations') and article.citations and
                len(article.citations) > 0 and self.llm_router):

                raw_article = article.summarized_article.filtered_article.raw_article

                # Prepare citations for LLM
                citation_dicts = []
                for citation in article.citations[:3]:  # Max 3 citations per PRD F-15
                    citation_dict = {
                        'title': citation.title,
                        'url': citation.url,
                        'source_name': citation.source_name,
                        'japanese_summary': citation.japanese_summary
                    }
                    citation_dicts.append(citation_dict)

                logger.info(
                    "Regenerating summary with citation context (PRD F-15)",
                    article_id=raw_article.id,
                    citation_count=len(citation_dicts)
                )

                try:
                    # Generate new citation-based summary
                    enhanced_summary = await self.llm_router.generate_citation_based_summary(
                        article_title=raw_article.title,
                        article_content=raw_article.content,
                        article_url=str(raw_article.url),
                        source_name=raw_article.source_id,
                        citations=citation_dicts
                    )

                    # Replace the existing summary with enhanced version
                    article.summarized_article.summary = enhanced_summary

                    logger.info(
                        "Successfully enhanced summary with citations",
                        article_id=raw_article.id,
                        confidence_score=enhanced_summary.confidence_score
                    )

                except Exception as e:
                    logger.warning(
                        "Failed to generate citation-based summary, keeping original",
                        article_id=raw_article.id,
                        error=str(e)
                    )
                    # Keep original summary if enhancement fails

            # ğŸ”¥ ULTRA THINK FIX: çµ±åˆã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã§ã€ç¶šå ±ã€‘+ğŸ†™é‡è¤‡ã‚’æ ¹æœ¬è§£æ±º
            # Check UPDATE status BEFORE title generation to pass context
            has_update_flag = hasattr(article, 'is_update') and article.is_update
            raw_title_has_emoji = 'ğŸ†™' in article.summarized_article.filtered_article.raw_article.title
            is_update_article = has_update_flag or raw_title_has_emoji

            # Pass update context to title generation for integrated processing
            article.japanese_title = await self._generate_article_title_integrated(
                article, is_update=is_update_article
            )

            # Fallback for failed title generation to prevent article loss
            if not article.japanese_title:
                raw_title = article.summarized_article.filtered_article.raw_article.title
                logger.warning(
                    "Integrated title generation failed, using fallback",
                    article_id=article.summarized_article.filtered_article.raw_article.id
                )
                # Apply integrated fallback logic
                if is_update_article:
                    # Remove redundant ã€ç¶šå ±ã€‘ and add ğŸ†™ if needed
                    clean_title = re.sub(r'ã€?ç¶šå ±ã€‘?[:ï¼š]?\s*', '', raw_title)
                    article.japanese_title = f"{clean_title}ğŸ†™" if 'ğŸ†™' not in clean_title else clean_title
                else:
                    article.japanese_title = raw_title

            # Log the integrated title generation result
            if is_update_article:
                logger.info(
                    "Integrated UPDATE title generation completed",
                    article_id=article.summarized_article.filtered_article.raw_article.id,
                    final_title=article.japanese_title,
                    has_update_flag=has_update_flag,
                    raw_title_has_emoji=raw_title_has_emoji
                )

            # Normalize terminology in Japanese title
            if article.japanese_title:
                # Clean any remaining hash marks (safety measure for double hash issue)
                article.japanese_title = re.sub(r'^[#\s]*["\'ã€Œ]|["\'ã€]+$', '', article.japanese_title).strip()
                article.japanese_title = re.sub(r'^#+\s*', '', article.japanese_title)
                article.japanese_title = self._normalize_terminology(article.japanese_title)
            if (article.summarized_article and
                article.summarized_article.summary and
                article.summarized_article.summary.summary_points):
                article.summarized_article.summary.summary_points = self._cleanup_summary_points(
                    article.summarized_article.summary.summary_points
                )
                # Apply terminology normalization and quality checks to each summary point
                normalized_points = []
                for point in article.summarized_article.summary.summary_points:
                    # First normalize terminology
                    normalized_point = self._normalize_terminology(point)
                    # Then apply quality checks for natural Japanese
                    quality_checked_point = self._improve_japanese_quality(normalized_point)
                    normalized_points.append(quality_checked_point)
                article.summarized_article.summary.summary_points = normalized_points
                # Note: Removed _shorten_summary_points to preserve bullet content integrity per Lawrence's feedback
            return article

        # Process all articles in parallel for title generation and cleanup
        tasks = [process_single_article(article) for article in filtered_articles]
        filtered_articles = await asyncio.gather(*tasks)

        # New Step: Generate Japanese summaries for all secondary citations in parallel
        citation_generator = CitationGenerator(self.llm_router)
        all_citations_to_summarize = []
        for article in filtered_articles:
            if article.citations:
                all_citations_to_summarize.extend(article.citations)

        if all_citations_to_summarize:
            logger.info(f"Generating Japanese summaries for {len(all_citations_to_summarize)} secondary citations...")
            await citation_generator.generate_summaries_for_citations(all_citations_to_summarize)
            logger.info("Finished generating summaries for secondary citations.")

        # Prepare template context
        context = {
            "date": newsletter_date,
            "edition": edition,
            "articles": filtered_articles,
            "lead_text": lead_text,
            "processing_summary": processing_summary or {},
            "generation_timestamp": newsletter_date.strftime("%Y-%m-%d %H:%M:%S JST"),
            "word_count": 0,  # Will be calculated
        }

        # Render newsletter content ------------------------------------------------
        if self.jinja_env:
            # Use Jinja2 template rendering
            template_name = f"{edition}_newsletter.jinja2"

            try:
                template = self.jinja_env.get_template(template_name)
            except Exception:  # jinja2.TemplateNotFound ç­‰
                if HAS_LOGGER:
                    logger.warning(
                        "Template not found, using default",
                        requested_template=template_name,
                        fallback="daily_newsletter.jinja2"
                    )
                try:
                    template = self.jinja_env.get_template("daily_newsletter.jinja2")
                except Exception:
                    # Fall back to basic markdown generation
                    if HAS_LOGGER:
                        logger.warning("Default template also not found, using basic markdown")
                    template = None
                    newsletter_content = self._generate_basic_markdown(context)
        else:
            template_name = "basic"
            template = None

        # Debug: Log UPDATE articles status before template rendering
        update_articles = [a for a in filtered_articles if getattr(a, 'is_update', False)]
        if update_articles:
            logger.info(
                f"Rendering template with {len(update_articles)} UPDATE articles",
                update_articles=[{
                    'id': a.summarized_article.filtered_article.raw_article.id,
                    'is_update': getattr(a, 'is_update', False),
                    'japanese_title': getattr(a, 'japanese_title', None),
                    'has_emoji': 'ğŸ†™' in getattr(a, 'japanese_title', '') if getattr(a, 'japanese_title', None) else False
                } for a in update_articles]
            )

        # Render template (if available)
        if template:
            try:
                newsletter_content = template.render(**context)
            except Exception as e:
                if HAS_LOGGER:
                    logger.error(
                        "Template rendering failed",
                        template=template_name,
                        error=str(e)
                    )
                else:
                    logger.error(
                        f"Template rendering failed: template={template_name}, error={str(e)}"
                    )
                raise
        else:
            # Fallback: Generate basic markdown without template
            newsletter_content = self._generate_basic_markdown(context)

        # Collapse excessive blank lines (3+ -> 2)
        newsletter_content = re.sub(r"\n{3,}", "\n\n", newsletter_content)

        # Perform final quality validation on generated newsletter
        newsletter_content = self._validate_and_fix_newsletter_content(newsletter_content, filtered_articles)

        # Comprehensive quality check
        try:
            from src.utils.newsletter_quality_checker import check_newsletter_quality

            logger.info("Performing comprehensive quality check...")
            quality_report = await check_newsletter_quality(
                content=newsletter_content,
                metadata={
                    'edition': edition,
                    'article_count': len(filtered_articles),
                    'generation_timestamp': newsletter_date.isoformat()
                }
            )

            logger.info(
                f"Quality check completed: score={quality_report.overall_score:.2f}, "
                f"issues={quality_report.metrics.get('total_issues', 0)}, "
                f"regeneration_required={quality_report.requires_regeneration}"
            )

            # Log quality issues for monitoring
            if quality_report.issues:
                for issue in quality_report.issues:
                    if issue.severity == 'critical':
                        logger.error(f"Critical quality issue in {issue.location}: {issue.description}")
                    elif issue.severity == 'major':
                        logger.warning(f"Major quality issue in {issue.location}: {issue.description}")
                    else:
                        logger.info(f"Minor quality issue in {issue.location}: {issue.description}")

            # For now, we log the quality report but don't regenerate
            # In future iterations, could implement automatic regeneration for critical issues
            if quality_report.requires_regeneration:
                logger.warning(
                    "Newsletter quality below threshold but continuing with current content. "
                    "Consider implementing automatic regeneration for critical quality issues."
                )

        except Exception as e:
            logger.warning(f"Quality check failed, continuing with newsletter generation: {e}")

        # Calculate word count
        word_count = len(newsletter_content.split())

        # Save Markdown file
        output_file = self._save_newsletter(
            newsletter_content, newsletter_date, edition, output_dir
        )

        # ------------------------------------------------------------------
        # Backup (HTML ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ drafts ã«ã¯ä¸è¦ãªã®ã§ç”Ÿæˆã—ãªã„)
        # ------------------------------------------------------------------

        preview_file = None  # HTML ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ãªã„

        try:
            from src.utils.backup_manager import backup_file  # local import to avoid cycle
            backup_path = backup_file(output_file)
        except Exception as backup_err:  # pylint: disable=broad-except
            logger.warning(f"Backup failed: {str(backup_err)}")
            backup_path = None

        # ğŸ”¥ ULTRA THINK: Final failsafeå‰Šé™¤ - çµ±åˆã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã§å‡¦ç†æ¸ˆã¿
        # é‡è¤‡ã«ã‚ˆã‚‹ã€ŒğŸ†™ğŸ†™ã€å•é¡Œã‚’æ ¹æœ¬è§£æ±º
        update_count_check = 0
        for article in filtered_articles:
            if hasattr(article, 'is_update') and article.is_update:
                update_count_check += 1
                # çµ±åˆã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã§ğŸ†™ãŒæ­£ã—ãä»˜ä¸ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã®ã¿
                japanese_title = getattr(article, 'japanese_title', None)
                has_emoji = japanese_title and 'ğŸ†™' in japanese_title
                logger.debug(
                    "UPDATE article title check",
                    article_id=article.summarized_article.filtered_article.raw_article.id,
                    has_emoji=has_emoji,
                    japanese_title=japanese_title
                )

        logger.info(f"F-16 UPDATE article verification: {update_count_check} articles processed with integrated title generation")

        # Generate lead text after all articles are processed
        lead_text = await self._generate_lead_text(filtered_articles, edition)

        # Create newsletter output
        newsletter_output = NewsletterOutput(
            title=f"{newsletter_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} AI NEWS TLDR",
            date=newsletter_date,
            lead_text=lead_text,
            articles=filtered_articles,
            metadata={
                "edition": edition,
                "template": template_name,
                "output_file": str(output_file),
                "preview_file": str(preview_file),
                "backup_file": str(backup_path) if backup_path else None,
                "generation_method": "automated"
            },
            word_count=word_count,
            processing_summary=processing_summary or {}
        )

        if HAS_LOGGER:
            logger.info(
                "Newsletter generation completed",
                output_file=str(output_file),
                word_count=word_count,
                articles_included=len(filtered_articles)
            )
        else:
            logger.info(
                f"Newsletter generation completed: {output_file}, word_count={word_count}, articles_included={len(filtered_articles)}"
            )

        return newsletter_output

    async def _apply_context_analysis(self, articles: list[ProcessedArticle]) -> list[ProcessedArticle]:
        """
        [FIXED] Apply context analysis based on PRD F-16.
        Now respects workflow's existing context analysis instead of overriding it.
        """
        try:
            # Check if articles already have workflow context analysis results
            workflow_analyzed_count = sum(
                1 for article in articles
                if hasattr(article, 'context_analysis') and article.context_analysis is not None
            )

            if workflow_analyzed_count > 0:
                logger.info(
                    f"Found {workflow_analyzed_count}/{len(articles)} articles with workflow context analysis, "
                    "respecting existing analysis and skipping secondary context analysis"
                )
                # Workflow has already done proper context analysis, don't override it
                return articles

            # Only run secondary analysis if workflow hasn't done it
            logger.info("No workflow context analysis found, running fallback context analysis")

            # Ensure required imports are available
            import numpy as np
            from sklearn.metrics.pairwise import cosine_similarity

            from src.utils.embedding_manager import EmbeddingManager
            from src.utils.supabase_client import get_recent_contextual_articles

            # Get recent articles from Supabase
            past_articles = await get_recent_contextual_articles(days_back=7, limit=1000)

            if not past_articles:
                logger.info("No past articles found for context analysis.")
                return articles

            # Initialize embedding manager
            embedding_manager = EmbeddingManager()

            # Prepare past articles for similarity comparison
            past_embeddings = []
            past_article_map = {}

            for article_data in past_articles:
                if article_data.get('embedding'):
                    article_id = article_data['article_id']
                    embedding = article_data['embedding']

                    # Convert embedding to numpy array if it's a string
                    if isinstance(embedding, str):
                        try:
                            import json
                            embedding = json.loads(embedding)
                        except:
                            try:
                                embedding = eval(embedding)
                            except:
                                logger.warning(f"Failed to parse embedding for article {article_id}")
                                continue

                    try:
                        embedding_array = np.array(embedding, dtype=np.float32)
                        # Normalize for cosine similarity
                        embedding_array = embedding_array / np.linalg.norm(embedding_array)
                        past_embeddings.append(embedding_array)
                        past_article_map[article_id] = article_data
                    except Exception as e:
                        logger.warning(f"Failed to process embedding for article {article_id}: {e}")
                        continue

            if not past_embeddings:
                logger.info("No valid embeddings found in past articles for context analysis.")
                return articles

            # Create a matrix of past embeddings for efficient search
            past_embedding_matrix = np.array(past_embeddings)
            past_article_ids = list(past_article_map.keys())

            # Process each current article (only those without workflow analysis)
            updates_found = 0
            for article in articles:
                # Skip if workflow already analyzed this article
                if hasattr(article, 'context_analysis') and article.context_analysis is not None:
                    continue

                try:
                    current_text = f"{article.summarized_article.filtered_article.raw_article.title} {' '.join(article.summarized_article.summary.summary_points)}"
                    current_embedding = await embedding_manager.get_embedding(current_text)

                    if current_embedding is None:
                        continue

                    # Normalize current embedding
                    current_embedding = current_embedding / np.linalg.norm(current_embedding)

                    # Find similar past articles
                    similarities = cosine_similarity(current_embedding.reshape(1, -1), past_embedding_matrix)[0]

                    # Get top 3 most similar articles
                    top_indices = np.argsort(similarities)[-3:][::-1]

                    for i in top_indices:
                        similarity_score = similarities[i]
                        # Very high threshold to reduce false positives
                        if similarity_score > 0.95: # Very high threshold - only near-identical content
                            past_article_id = past_article_ids[i]
                            past_article_data = past_article_map[past_article_id]

                            logger.info(
                                f"Found potential sequel for '{article.summarized_article.filtered_article.raw_article.title[:50]}...' "
                                f"(score: {similarity_score:.2f}) with '{past_article_data['title'][:50]}...'"
                            )

                            # Use LLM to confirm if it's an update with stricter criteria
                            decision = await self._confirm_update_with_llm(article, past_article_data)

                            if decision == "UPDATE":
                                article.is_update = True
                                article.previous_article_url = past_article_data.get('source_url', '')
                                updates_found += 1

                                # ğŸ”¥ ULTRA THINK: ğŸ†™çµµæ–‡å­—ã¯çµ±åˆã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã§å‡¦ç†æ¸ˆã¿
                                # raw_titleã¸ã®è¿½åŠ ã¯å‰Šé™¤ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
                                logger.info(
                                    "Confirmed fallback UPDATE - emoji handled in integrated title generation",
                                    article_id=article.summarized_article.filtered_article.raw_article.id
                                )

                                logger.info(f"Confirmed fallback UPDATE for article {article.summarized_article.filtered_article.raw_article.id}")
                                # Once confirmed, no need to check other past articles
                                break
                except Exception as e:
                    logger.warning(f"Failed to analyze context for article: {e}")
                    continue

            if updates_found > 0:
                logger.info(f"Fallback context analysis completed: found {updates_found} update articles")

            return articles

        except Exception as e:
            logger.error(f"Context analysis failed: {e}", exc_info=True)
            return articles # Return original articles on failure

    async def _confirm_update_with_llm(self, current_article: ProcessedArticle, past_article: dict) -> str:
        """
        Use LLM to confirm if the current article is an update to the past one.
        """
        try:
            current_title = current_article.summarized_article.filtered_article.raw_article.title
            current_summary = " ".join(current_article.summarized_article.summary.summary_points)

            past_title = past_article.get('title', '')
            past_summary = past_article.get('content_summary', '')

            prompt = f"""
            You are an expert news analyst. Compare the "current news" with the "past news" and decide if the current news is a TRUE UPDATE.

            # Current News
            Title: {current_title}
            Summary: {current_summary}

            # Past News
            Title: {past_title}
            Summary: {past_summary}

            # Decision Rules
            Return "UPDATE" ONLY if the current news is:
            1. A continuation of the EXACT SAME story/event
            2. New developments in the EXACT SAME project/product
            3. Follow-up results or consequences of the past news

            Return "UNRELATED" if they are:
            - Different stories about the same company
            - Different products/services
            - General industry news

            Respond with only one word: "UPDATE" or "UNRELATED".
            """

            if HAS_LLM_ROUTER and self.llm_router:
                response = await self.llm_router.generate_simple_text(
                    prompt=prompt,
                    max_tokens=5,
                    temperature=0.0
                )

                decision = response.strip().upper()
                if decision in ["UPDATE", "UNRELATED"]:
                    return decision
                # If LLM returns something else, be conservative
                return "UNRELATED"

            return "UNRELATED" # Default to unrelated if LLM fails - be conservative

        except Exception as e:
            logger.warning(f"LLM update confirmation failed: {e}")
            return "UNRELATED"

    async def _generate_lead_text(
        self,
        articles: list[ProcessedArticle],
        edition: str
    ) -> dict[str, Any]:
        """Generate introduction text (title + paragraphs) based on actual article content.

        Returns
        -------
        dict
            {"title": str, "paragraphs": List[str]}
        """

        if not articles:
            return {
                "title": "æœ¬æ—¥ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "paragraphs": [
                    "æœ¬æ—¥ã¯æ³¨ç›®ã™ã¹ãAIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã”ã–ã„ã¾ã›ã‚“ã§ã—ãŸã€‚"
                ],
            }

        # Try LLM-driven lead text generation first (if available)
        if HAS_LLM_ROUTER and self.llm_router:
            try:
                llm_lead_text = await self._generate_llm_lead_text_with_retry(articles, edition)
                if llm_lead_text and llm_lead_text.get('lead_paragraphs'):
                    # Convert LLM format to template-expected format
                    lead_paragraphs = llm_lead_text['lead_paragraphs']

                    # Generate title from first paragraph or articles
                    title = self._generate_lead_title_from_articles(articles, edition)

                    formatted_lead_text = {
                        "title": title,
                        "paragraphs": lead_paragraphs
                    }

                    logger.info(f"LLM lead text generated successfully: {len(lead_paragraphs)} paragraphs")
                    return formatted_lead_text
                else:
                    logger.warning("LLM lead text generation returned empty or invalid result")
            except Exception as e:
                logger.warning(f"LLM lead text generation failed, falling back to template: {e}")
        else:
            logger.info("LLM router not available, using template-based lead text")

        # Fallback to template-based generation
        update_count = sum(1 for article in articles if getattr(article, 'is_update', False))

        # Generate specific lead text directly from articles
        paragraphs = self._generate_specific_lead_paragraphs(articles, edition)
        if not paragraphs:
            # Final fallback
            paragraphs = [
                "æœ¬æ—¥ã¯æ³¨ç›®ã™ã¹ãAIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸ã—ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚",
                "ä¼æ¥­ã®æ–°æŠ€è¡“ç™ºè¡¨ã‹ã‚‰ç ”ç©¶é–‹ç™ºã®æˆæœã¾ã§ã€é‡è¦ãªå‹•å‘ã‚’ã¾ã¨ã‚ã¾ã—ãŸã€‚",
                "ãã‚Œã§ã¯å„ãƒˆãƒ”ãƒƒã‚¯ã®è©³ç´°ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚"
            ]
        else:
            # Weekly edition: Fixed 3-sentence structure
            paragraphs = [
                f"ä»Šé€±ã¯{len(articles)}ä»¶ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸ã—ã¾ã—ãŸã€‚",
                "æŠ€è¡“å‹•å‘ã‹ã‚‰ãƒ“ã‚¸ãƒã‚¹æ´»ç”¨ã¾ã§å¹…åºƒãã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚",
                "ãã‚Œã§ã¯é€±åˆŠã¾ã¨ã‚ã‚’ã”è¦§ãã ã•ã„ã€‚",
            ]

        # å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆï¼ˆæ±ç”¨è¡¨ç¾ã‚’é¿ã‘ã‚‹ï¼‰
        if themes:
            # ä¼æ¥­åãƒ»æŠ€è¡“åã‚’å„ªå…ˆã—ã¦ã‚ˆã‚Šå…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã«
            specific_titles = self._generate_specific_theme_title(themes, articles)
            title = specific_titles or f"{themes[0]}ã«ã‚ˆã‚‹æŠ€è¡“ç™ºè¡¨"
        else:
            # æœ€å¾Œã®æ‰‹æ®µã§ã‚‚å…·ä½“çš„ãªå†…å®¹ã‚’å«ã‚ã‚‹
            title = self._generate_fallback_specific_title(articles)

        # ä½“è¨€æ­¢ã‚ã‚’ä¿è¨¼ï¼ˆæœ«å°¾ãŒã€Œã€‚ã€ã€Œã§ã™ã€ç­‰ãªã‚‰å‰Šé™¤ï¼‰
        title = re.sub(r"[ã€‚.!?ã§ã™ã¾ã™]+$", "", title)

        return {
            "title": title,
            "paragraphs": paragraphs,
        }

    async def _generate_llm_lead_text(
        self,
        articles: list[ProcessedArticle],
        edition: str
    ) -> dict[str, Any]:
        """Generate high-quality lead text using LLM based on article content."""

        # Prepare structured input for LLM
        article_summaries = []

        for i, article in enumerate(articles[:10]):  # Use up to 10 articles for context
            try:
                if (article.summarized_article and
                    article.summarized_article.summary and
                    article.summarized_article.summary.summary_points):

                    summary_points = article.summarized_article.summary.summary_points
                    title = article.japanese_title or article.summarized_article.filtered_article.raw_article.title

                    # Extract key info for structured prompt
                    companies = self._extract_companies(summary_points[0], title) if summary_points else []

                    article_summaries.append({
                        "title": title,
                        "key_points": summary_points[:2],  # First 2 points
                        "companies": companies,
                        "is_update": getattr(article, 'is_update', False)
                    })

            except Exception as e:
                logger.warning(f"Failed to process article {i} for LLM lead text: {e}")
                continue

        if not article_summaries:
            return None

        # Create structured prompt for LLM
        prompt = self._create_lead_text_prompt(article_summaries, edition)

        # Generate lead text using LLM
        try:
            lead_text_response = await self.llm_router.generate_simple_text(
                prompt=prompt,
                max_tokens=700,  # Increased from 500 to 700 to prevent truncation
                temperature=0.2,  # Low temperature for consistency
            )
        except Exception as e:
            logger.warning(f"LLM call failed in lead text generation: {e}")
            return None

        if not lead_text_response:
            return None

        # Parse LLM response into title and paragraphs
        return self._parse_llm_lead_text_response(lead_text_response)

    def _create_lead_text_prompt(
        self,
        article_summaries: list[dict],
        edition: str
    ) -> str:
        """Create structured prompt for LLM lead text generation."""

        # Build simple article context without complex JSON
        context_lines = []
        for i, summary in enumerate(article_summaries[:5], 1):  # Limit to top 5
            companies_str = "ã€".join(summary['companies'][:2]) if summary['companies'] else "AIä¼æ¥­"
            key_point = summary['key_points'][0] if summary['key_points'] else summary['title']
            update_str = " (ç¶šå ±)" if summary.get('is_update', False) else ""

            context_lines.append(f"{i}. {summary['title']}{update_str}")
            if companies_str and companies_str != "AIä¼æ¥­":
                context_lines.append(f"   é–¢é€£ä¼æ¥­: {companies_str}")
            context_lines.append(f"   è¦ç‚¹: {key_point[:100]}...")
            context_lines.append("")

        context = "\n".join(context_lines)

        prompt = f"""ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ã‚’åŸºã«ã€ä¸€èˆ¬èª­è€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„é­…åŠ›çš„ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ã®å°å…¥æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€æœ¬æ—¥ã®ä¸»è¦è¨˜äº‹ã€‘
{context}

ã€å°å…¥æ–‡ã®è¦ä»¶ï¼ˆä¸€èˆ¬èª­è€…å‘ã‘ç°¡æ½”ç‰ˆï¼‰ã€‘
- 3ã¤ã®æ®µè½ã§æ§‹æˆ
- å„æ®µè½ã¯1æ–‡ã§ã€70-120æ–‡å­—ï¼ˆèª­ã¿ã‚„ã™ã•é‡è¦–ï¼‰
- ç¬¬1æ®µè½: æœ€ã‚‚æ³¨ç›®ã™ã¹ããƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åˆ†ã‹ã‚Šã‚„ã™ãç´¹ä»‹
- ç¬¬2æ®µè½: ä»–ã®é‡è¦ãªå‹•ãã‚’æ—¥å¸¸ç”Ÿæ´»ã¸ã®å½±éŸ¿ã¨ã¨ã‚‚ã«èª¬æ˜
- ç¬¬3æ®µè½: ã“ã‚Œã‚‰ã®å¤‰åŒ–ãŒç§ãŸã¡ã®æœªæ¥ã«ã‚‚ãŸã‚‰ã™å¤‰åŒ–ã‚’äºˆæ¸¬

ã€å³æ ¼ç¦æ­¢äº‹é …ã€‘
- å°‚é–€ç”¨èªã®å¤šç”¨: ã€ŒAPIã€ã€Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ã€Œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€
- æŠ½è±¡çš„è¡¨ç¾: ã€ŒAIæŠ€è¡“ã®é€²åŒ–ã€ã€Œæ¥­ç•Œã®å‹•å‘ã€ã€Œé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€
- æ›–æ˜§è¡¨ç¾: ã€Œæ§˜ã€…ãªã€ã€Œå¤šãã®ã€ã€Œã„ãã¤ã‹ã®ã€ã€Œè¤‡æ•°ã®ä¼æ¥­ã€
- å†—é•·è¡¨ç¾: ã€Œã€œã«ã¤ã„ã¦ç™ºè¡¨ã—ã¾ã—ãŸã€ã€Œã€œãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€
- ä¸å®Œå…¨æ–‡: ã€Œã€œãŒã€‚ã€ã€Œã€œã¯ã€‚ã€ç­‰ã®åŠ©è©çµ‚ã‚ã‚Š

ã€å¿…é ˆè¦ç´ ï¼ˆå„æ®µè½ã«å«ã‚ã‚‹ï¼‰ã€‘
- å…·ä½“çš„ä¼æ¥­åï¼ˆOpenAIã€Googleã€Metaç­‰ï¼‰
- èº«è¿‘ãªåˆ©ç”¨ä¾‹ï¼ˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€æ¤œç´¢ã€ç¿»è¨³ç­‰ï¼‰
- æ—¥å¸¸ç”Ÿæ´»ã¸ã®å½±éŸ¿ï¼ˆä»•äº‹åŠ¹ç‡åŒ–ã€å­¦ç¿’æ”¯æ´ã€å‰µä½œæ”¯æ´ç­‰ï¼‰

ã€å›ç­”å½¢å¼ã€‘
ä»¥ä¸‹ã®å½¢å¼ã§3ã¤ã®æ®µè½ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

æ®µè½1: [ç¬¬1æ–‡ã‚’ã“ã“ã«è¨˜è¼‰]
æ®µè½2: [ç¬¬2æ–‡ã‚’ã“ã“ã«è¨˜è¼‰]
æ®µè½3: [ç¬¬3æ–‡ã‚’ã“ã“ã«è¨˜è¼‰]

JSONã§ã¯ãªãã€ä¸Šè¨˜ã®å½¢å¼ã§æ—¥æœ¬èªã®æ–‡ç« ã‚’ç›´æ¥ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""

        return prompt

    def _validate_lead_paragraph(self, paragraph: str) -> tuple[bool, str]:
        """ãƒªãƒ¼ãƒ‰æ–‡ã®æ–‡æ³•æ¤œè¨¼"""

        if not paragraph or len(paragraph.strip()) < 10:
            return False, "æ®µè½ãŒçŸ­ã™ãã‚‹"

        paragraph = paragraph.strip()

        # ä¸å®Œå…¨ãªæ–‡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        incomplete_patterns = [
            (r'^[^ã€‚ï¼ï¼Ÿ]*[ãŒã¯ã‚’ã«ã§]ã€‚', "åŠ©è©ã®å¾Œã«å¥ç‚¹"),
            (r'[A-Za-z\u30A0-\u30FF\u3040-\u309F\u4E00-\u9FAF]+ãŒã€‚', "ä¸»èªã®å¾Œã™ãã«å¥ç‚¹"),
            (r'[ã¯ãŒã‚’ã«]$', "åŠ©è©ã§çµ‚ã‚ã‚‹æ–‡"),
            (r'[ãŒã¯ã‚’ã«ã§]ã€‚\s*[^ã€‚ï¼ï¼Ÿ]*$', "æ–‡ä¸­ã«ä¸å®Œå…¨ãªå¥ç‚¹")
        ]

        for pattern, reason in incomplete_patterns:
            if re.search(pattern, paragraph):
                return False, f"ä¸å®Œå…¨ãªæ–‡: {reason}"

        # é‡è¤‡è¡¨ç¾ã®ãƒã‚§ãƒƒã‚¯
        if re.search(r'(ã•ã‚Œã¦ã„ã¾ã™|ã—ã¦ã„ã¾ã™).*?(ã¨ç™ºè¡¨|ã¨å ±å‘Š|ã¨èª¬æ˜)', paragraph):
            return False, "é‡è¤‡è¡¨ç¾ï¼ˆã€œã—ã¦ã„ã¾ã™ã¨ç™ºè¡¨ï¼‰"

        # æ–‡ã®é•·ã•ãƒã‚§ãƒƒã‚¯
        if len(paragraph) > 200:
            return False, "æ–‡ãŒé•·ã™ãã‚‹ï¼ˆ200æ–‡å­—è¶…ï¼‰"

        # é©åˆ‡ãªèªå°¾ãƒã‚§ãƒƒã‚¯
        proper_endings = ('ã§ã™ã€‚', 'ã¾ã™ã€‚', 'ã¾ã—ãŸã€‚', 'ã¦ã„ã¾ã™ã€‚', 'ã§ã—ãŸã€‚', 'ã¾ã›ã‚“ã€‚')
        if not paragraph.endswith(proper_endings):
            return False, "é©åˆ‡ãªæ•¬èªã§çµ‚ã‚ã£ã¦ã„ãªã„"

        return True, "OK"

    async def _generate_llm_lead_text_with_retry(self, articles, edition, max_retries=3):
        """æ–‡æ³•æ¤œè¨¼ä»˜ããƒªãƒ¼ãƒ‰æ–‡ç”Ÿæˆ"""

        for attempt in range(max_retries):
            try:
                result = await self._generate_llm_lead_text(articles, edition)

                if not result or 'lead_paragraphs' not in result:
                    logger.warning(f"Lead text generation attempt {attempt + 1}: No valid response")
                    continue

                # å„æ®µè½ã‚’æ¤œè¨¼
                valid_paragraphs = []
                for i, para in enumerate(result.get('lead_paragraphs', [])):
                    is_valid, reason = self._validate_lead_paragraph(para)
                    if is_valid:
                        valid_paragraphs.append(para)
                        logger.info(f"Lead paragraph {i+1} validated: {para[:50]}...")
                    else:
                        logger.warning(f"Invalid paragraph {i+1}: {reason} - {para[:50]}...")

                # æœ€ä½2ã¤ã®æœ‰åŠ¹ãªæ®µè½ãŒå¿…è¦
                if len(valid_paragraphs) >= 2:
                    # 3ã¤ã«æº€ãŸãªã„å ´åˆã¯ã€æœ€å¾Œã®æ®µè½ã‚’è¤‡è£½ãƒ»ä¿®æ­£
                    while len(valid_paragraphs) < 3:
                        if valid_paragraphs:
                            # æœ€å¾Œã®æ®µè½ã‚’ãƒ™ãƒ¼ã‚¹ã«å±•æœ›æ–‡ã‚’ä½œæˆ
                            base_para = valid_paragraphs[-1]
                            if 'ã§ã™ã€‚' in base_para:
                                expansion = "ä»Šå¾Œã‚‚ã“ã‚Œã‚‰ã®å‹•å‘ãŒæ¥­ç•Œå…¨ä½“ã«ä¸ãˆã‚‹å½±éŸ¿ã«æ³¨ç›®ãŒé›†ã¾ã‚Šã¾ã™ã€‚"
                            else:
                                expansion = "ã“ã‚Œã‚‰ã®é€²å±•ã«ã‚ˆã‚Šã€AIåˆ†é‡ã«ãŠã‘ã‚‹ç«¶äº‰ã¨å”åŠ›ãŒã•ã‚‰ã«æ´»ç™ºåŒ–ã™ã‚‹ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚"
                            valid_paragraphs.append(expansion)
                        else:
                            break

                    result['lead_paragraphs'] = valid_paragraphs[:3]
                    logger.info(f"Lead text generation successful on attempt {attempt + 1}")
                    return result
                else:
                    logger.warning(f"Lead text generation attempt {attempt + 1}: Only {len(valid_paragraphs)} valid paragraphs")

            except Exception as e:
                logger.error(f"Lead text generation attempt {attempt + 1} failed: {e}")

        # å…¨ã¦å¤±æ•—ã—ãŸå ´åˆã¯å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        logger.warning("All lead text generation attempts failed, using fallback")
        return self._generate_safe_fallback_lead_text(articles)

    def _generate_safe_fallback_lead_text(self, articles):
        """å®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒ³ãƒ—ãƒ«ã§ç¢ºå®Ÿãªãƒªãƒ¼ãƒ‰æ–‡"""

        # è¨˜äº‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        article_count = len(articles) if articles else 0

        # ä¸»è¦ä¼æ¥­ã‚’æŠ½å‡º
        companies = set()
        for article in articles[:5]:
            if hasattr(article, 'summarized_article') and article.summarized_article:
                title = article.summarized_article.filtered_article.raw_article.title
                # ä¼æ¥­åã‚’ç°¡å˜ã«æŠ½å‡º
                for company in ['OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic', 'Apple']:
                    if company in title:
                        companies.add(company)

        main_companies = list(companies)[:2]
        companies_str = "ã€".join(main_companies) if main_companies else "ä¸»è¦AIä¼æ¥­"

        # ç¢ºå®Ÿã«æ–‡æ³•çš„ã«æ­£ã—ã„æ–‡ã‚’ç”Ÿæˆ
        fallback_paragraphs = [
            f"AIåˆ†é‡ã«ãŠã„ã¦{companies_str}ã‚’å«ã‚€è¤‡æ•°ã®ä¼æ¥­ãŒæ–°ãŸãªå–ã‚Šçµ„ã¿ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚",
            f"ä»Šå›ã¯{article_count}ä»¶ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãŠå±Šã‘ã—ã¾ã™ã€‚",
            "ã“ã‚Œã‚‰ã®å‹•å‘ã¯ä»Šå¾Œã®AIæŠ€è¡“ã®ç™ºå±•ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãŒäºˆæƒ³ã•ã‚Œã¾ã™ã€‚"
        ]

        return {
            'lead_paragraphs': fallback_paragraphs,
            'source': 'safe_fallback',
            'confidence': 0.5
        }

    def _parse_llm_lead_text_response(self, response: str) -> dict[str, Any]:
        """Parse LLM response for lead text generation."""

        if not response:
            raise ValueError("Empty response from LLM")

        try:
            # First, try to parse as plain text format (new format)
            lines = response.strip().split('\n')
            paragraphs = []

            for line in lines:
                line = line.strip()

                # Look for lines that start with "æ®µè½" or contain actual content
                if line.startswith('æ®µè½') and ':' in line:
                    # Extract the content after the colon
                    content = line.split(':', 1)[1].strip()
                    if content and len(content) > 20:
                        paragraphs.append(content)
                elif len(line) > 30 and any(line.endswith(ending) for ending in ['ã§ã™ã€‚', 'ã¾ã™ã€‚', 'ã¾ã—ãŸã€‚', 'ã¦ã„ã¾ã™ã€‚', 'ã§ã—ãŸã€‚']):
                    # This looks like a complete sentence
                    paragraphs.append(line)

            # If we found paragraphs in plain text format, use them
            if len(paragraphs) >= 2:
                # Clean up each paragraph
                cleaned_paragraphs = []
                for para in paragraphs[:3]:  # Take up to 3 paragraphs
                    # Remove any remaining formatting artifacts
                    para = para.strip('[]ã€Œã€')
                    para = re.sub(r'^\d+\.\s*', '', para)  # Remove numbering

                    # Validate and fix grammar
                    para = self._fix_paragraph_grammar(para)

                    if para and len(para) >= 30:
                        cleaned_paragraphs.append(para)

                if len(cleaned_paragraphs) >= 2:
                    # Ensure we have exactly 3 paragraphs
                    while len(cleaned_paragraphs) < 3:
                        cleaned_paragraphs.append(
                            "ä»Šå¾Œã‚‚AIæŠ€è¡“ã®é€²åŒ–ã¨ç”£æ¥­ã¸ã®å¿œç”¨ãŒåŠ é€Ÿã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚"
                        )

                    return {
                        'lead_paragraphs': cleaned_paragraphs[:3],
                        'source': 'plain_text_format',
                        'confidence': 0.8
                    }

            # Fallback: Try JSON parsing (legacy format)
            if '```json' in response:
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', response, re.DOTALL)
                if json_match:
                    response = json_match.group(1)

            # Try to parse as JSON
            if response.strip().startswith('{'):
                data = json.loads(response)

                if 'lead_paragraphs' in data:
                    lead_paragraphs = data['lead_paragraphs']
                    if isinstance(lead_paragraphs, list) and len(lead_paragraphs) >= 2:
                        cleaned_paragraphs = []
                        for para in lead_paragraphs[:3]:
                            if isinstance(para, str) and len(para.strip()) > 10:
                                para = self._fix_paragraph_grammar(para.strip())
                                if para:
                                    cleaned_paragraphs.append(para)

                        if len(cleaned_paragraphs) >= 2:
                            while len(cleaned_paragraphs) < 3:
                                cleaned_paragraphs.append(
                                    "ã“ã‚Œã‚‰ã®æŠ€è¡“å‹•å‘ãŒä»Šå¾Œã®å¸‚å ´å½¢æˆã«é‡è¦ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚"
                                )

                            return {
                                'lead_paragraphs': cleaned_paragraphs[:3],
                                'source': 'json_format',
                                'confidence': data.get('confidence', 0.8)
                            }

            # Final fallback: Extract any good sentences from the response
            all_sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', response)
            valid_sentences = []

            for sentence in all_sentences:
                sentence = sentence.strip()
                if len(sentence) > 30 and not any(skip in sentence.lower() for skip in ['json', '```', 'æ®µè½', 'å½¢å¼']):
                    sentence = self._fix_paragraph_grammar(sentence)
                    if sentence:
                        valid_sentences.append(sentence)

            if len(valid_sentences) >= 2:
                return {
                    'lead_paragraphs': valid_sentences[:3],
                    'source': 'extracted_sentences',
                    'confidence': 0.6
                }

            raise ValueError("Could not extract valid lead text from LLM response")

        except Exception as e:
            logger.warning(f"Failed to parse LLM lead text response: {e}")
            raise

    def _fix_paragraph_grammar(self, para: str) -> str:
        """Fix common grammar issues in a paragraph."""

        if not para:
            return ""

        # Fix common LLM generation issues
        # 1. Remove duplicate sentence endings
        para = re.sub(r'([ã€‚ï¼ï¼Ÿ])\1+', r'\1', para)

        # 2. Fix missing punctuation between sentences
        para = re.sub(r'([^\sã€‚ï¼ï¼Ÿ])([A-Z\u3042-\u3093\u30A2-\u30F3\u4E00-\u9FAF]{3,})', r'\1ã€‚\2', para)

        # 3. Fix overly long sentences (split at logical points)
        if len(para) > 120:
            # Split at conjunctions but keep them
            para = re.sub(r'([^\sã€‚ï¼ï¼Ÿ])(ã¾ãŸ|ã•ã‚‰ã«|ä¸€æ–¹|ãªãŠ|ãŸã ã—|ã—ã‹ã—)', r'\1ã€‚\2', para)

        # 4. Remove incomplete sentence patterns
        para = re.sub(r'([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+)ãŒã€‚\s*', '', para)
        para = re.sub(r'([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+)ã¯ã€‚\s*', '', para)

        # 5. Fix missing particles before nouns
        para = re.sub(r'([ã€ã€‚])([A-Z][a-z]+)([ãŒ|ã¯|ã‚’|ã«|ã§|ã¨])', r'\1\2\3', para)
        para = re.sub(r'([A-Za-z\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+)ã‚’ã€‚\s*', '', para)

        # Fix broken sentence connections
        para = re.sub(r'([A-Za-z]+)ãŒã€‚([^ã€‚]+)', r'\1ãŒ\2', para)
        para = re.sub(r'([A-Za-z]+)ã¯ã€‚([^ã€‚]+)', r'\1ã¯\2', para)

        # Remove sentences ending with particles
        if para.endswith(('ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã‹ã‚‰')):
            para = para[:-1].rstrip()

        # Ensure proper ending
        if para and not para.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            if any(word in para for word in ['ç™ºè¡¨', 'é–‹å§‹', 'å…¬é–‹', 'å°å…¥', 'æä¾›']):
                para += 'ã—ã¾ã—ãŸã€‚'
            elif any(word in para for word in ['äºˆå®š', 'è¦‹è¾¼ã¿', 'è¨ˆç”»', 'æ–¹é‡']):
                para += 'ã§ã™ã€‚'
            elif any(word in para for word in ['æœŸå¾…', 'äºˆæƒ³', 'è¦‹é€šã—']):
                para += 'ã•ã‚Œã¦ã„ã¾ã™ã€‚'
            else:
                para += 'ã€‚'

        # Check length
        if len(para) > 200:
            # Truncate at sentence boundary
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', para)
            if len(sentences) >= 3:
                para = sentences[0] + sentences[1]

        return para.strip()

    def _truncate_paragraphs_to_limit(self, paragraphs: list[str], char_limit: int) -> list[str]:
        """Truncate paragraphs to fit within character limit while preserving sentence boundaries."""

        if not paragraphs:
            return paragraphs

        truncated = []
        total_chars = 0

        for paragraph in paragraphs:
            if total_chars + len(paragraph) <= char_limit:
                # Entire paragraph fits
                truncated.append(paragraph)
                total_chars += len(paragraph)
            else:
                # Need to truncate this paragraph
                remaining_chars = char_limit - total_chars
                if remaining_chars > 30:  # Only truncate if we have reasonable space
                    try:
                        # Use proper sentence boundary detection
                        truncated_para = truncate_at_sentence_boundary(paragraph, max_length=remaining_chars)
                        if truncated_para and len(truncated_para) > 20:
                            truncated.append(truncated_para)
                        break
                    except:
                        # Fallback to simple truncation
                        truncated_para = paragraph[:remaining_chars]
                        last_sentence_end = truncated_para.rfind('ã€‚')
                        if last_sentence_end > 20:
                            truncated_para = paragraph[:last_sentence_end + 1]
                        else:
                            truncated_para = paragraph[:remaining_chars - 1] + 'â€¦'
                        truncated.append(truncated_para)
                break  # Stop adding more paragraphs

        return truncated

    def _generate_basic_markdown(self, context: dict[str, Any]) -> str:
        """Generate basic markdown newsletter without Jinja2 template."""

        date = context.get("date", datetime.now())
        lead_text = context.get("lead_text", {})
        articles = context.get("articles", [])

        lines = [
            f"# {date.strftime('%Yå¹´%mæœˆ%dæ—¥')} AI NEWS TLDR",
            "",
        ]

        # Add lead text
        if lead_text and "title" in lead_text:
            lines.extend([
                f"### {lead_text['title']}",
                "",
            ])

            paragraphs = lead_text.get("paragraphs", [])
            for paragraph in paragraphs:
                lines.extend([paragraph, ""])

            # Removed: "ãã‚Œã§ã¯å„ãƒˆãƒ”ãƒƒã‚¯ã®è©³ç´°ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚"
        else:
            lines.extend([
                "### æœ¬æ—¥ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                "",
                "æœ¬æ—¥ã¯æ³¨ç›®ã™ã¹ãAIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã”ã–ã„ã¾ã›ã‚“ã§ã—ãŸã€‚",
                "",
            ])

        # Add table of contents
        lines.extend(["### ç›®æ¬¡", ""])

        visible_articles = [a for a in articles if getattr(a, 'japanese_title', None)]

        for i, article in enumerate(visible_articles, 1):
            title = article.japanese_title[:45] if len(article.japanese_title) > 45 else article.japanese_title
            update_indicator = " ğŸ†™" if getattr(article, 'is_update', False) else ""
            lines.append(f"{i}. {title}{update_indicator}")
            lines.append("")  # Add blank line after each TOC entry

        lines.extend(["", "---", ""])

        # Add articles
        for article in visible_articles:
            title = article.japanese_title
            update_indicator = " ğŸ†™" if getattr(article, 'is_update', False) else ""
            lines.extend([f"### {title}{update_indicator}", ""])

            # Add summary points
            if (article.summarized_article and
                article.summarized_article.summary and
                article.summarized_article.summary.summary_points):

                for point in article.summarized_article.summary.summary_points:
                    lines.append(f"- {point}")
                lines.append("")

            # å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯
            if hasattr(article, 'citations') and article.citations:
                for citation in article.citations:
                    if isinstance(citation, Citation):
                        lines.extend([self._citation_to_markdown(citation), ""])
                    else:
                        # æ—§ä»•æ§˜ (str) ã«ã‚‚å¯¾å¿œ
                        lines.extend([str(citation), ""])

        return "\n".join(lines)

    def _citation_to_markdown(self, citation: 'Citation') -> str:
        """Convert Citation object to Markdown format."""
        if hasattr(citation, 'japanese_summary') and citation.japanese_summary:
            return f"> **{citation.source_name}** ({citation.url}): {citation.japanese_summary}"
        else:
            return f"> **{citation.source_name}** ({citation.url}): {citation.title}"

    def _generate_html_preview(
        self,
        content: str,
        articles: list[ProcessedArticle]
    ) -> str:
        """Generate HTML preview for newsletter content."""
        # This method should be implemented to generate HTML preview
        # For example, you can use a library like BeautifulSoup to parse the Markdown
        # and convert it to HTML for preview purposes.
        pass

    def _extract_article_highlights(self, articles: list[ProcessedArticle]) -> list[str]:
        """Extract PRD-quality highlights from articles for compelling lead text generation."""

        highlights = []

        # Prioritize articles by impact and specificity
        prioritized_articles = self._prioritize_articles_for_highlights(articles)

        for article in prioritized_articles[:3]:  # Top 3 most impactful articles
            try:
                if (article.summarized_article and
                    article.summarized_article.summary and
                    article.summarized_article.summary.summary_points):

                    summary_points = article.summarized_article.summary.summary_points
                    title = article.summarized_article.filtered_article.raw_article.title
                    japanese_title = article.japanese_title or "AIæŠ€è¡“æ–°å±•é–‹"

                    # Create sophisticated highlight with multiple details
                    highlight = self._create_sophisticated_highlight(
                        summary_points, title, japanese_title, getattr(article, 'is_update', False)
                    )

                    if highlight and len(highlight) > 50:  # Ensure substantial content
                        highlights.append(highlight)

                        # Add complementary context for major announcements
                        context = self._extract_contextual_impact(summary_points, title)
                        if context and len(highlights) < 3:
                            highlights.append(context)

            except Exception as e:
                logger.warning(f"Failed to extract highlight from article: {e}")
                continue

        # If we still need more highlights, create thematic summaries
        if len(highlights) < 2:
            thematic_highlights = self._create_thematic_highlights(articles)
            highlights.extend(thematic_highlights[:3-len(highlights)])

        return highlights[:3]  # Maximum 3 highlights for clean lead text

    def _prioritize_articles_for_highlights(self, articles: list[ProcessedArticle]) -> list[ProcessedArticle]:
        """Prioritize articles based on impact indicators for highlight generation."""

        def calculate_impact_score(article: ProcessedArticle) -> float:
            score = 0.0

            try:
                if not (article.summarized_article and article.summarized_article.summary):
                    return 0.0

                summary_points = article.summarized_article.summary.summary_points
                title = article.summarized_article.filtered_article.raw_article.title
                combined_text = title + " " + " ".join(summary_points)

                # Major companies/products boost score
                major_entities = [
                    'OpenAI', 'Google', 'Meta', 'Microsoft', 'Apple', 'Amazon', 'NVIDIA',
                    'Anthropic', 'DeepMind', 'GPT', 'Claude', 'Gemini', 'ChatGPT'
                ]
                for entity in major_entities:
                    if entity in combined_text:
                        score += 2.0

                # Impact indicators
                impact_terms = [
                    'ç™ºè¡¨', 'ãƒªãƒªãƒ¼ã‚¹', 'é–‹å§‹', 'å…¬é–‹', 'å°å…¥', 'å®Ÿè£…', 'æ”¹å–„', 'å‘ä¸Š',
                    'å¼·åŒ–', 'æ‹¡å¤§', 'å±•é–‹', 'æ´»ç”¨', 'å¿œç”¨', 'çªç ´', 'breakthrough'
                ]
                for term in impact_terms:
                    if term in combined_text:
                        score += 1.0

                # Numerical data increases credibility
                if re.search(r'\d+%|\d+å€|\d+[å¹´æœˆæ—¥]|\$\d+|Â¥\d+', combined_text):
                    score += 1.5

                # Update articles get slight priority
                if getattr(article, 'is_update', False):
                    score += 0.5

                # Confidence score from LLM
                if article.summarized_article.summary.confidence_score:
                    score += article.summarized_article.summary.confidence_score

            except Exception:
                return 0.0

            return score

        return sorted(articles, key=calculate_impact_score, reverse=True)

    def _create_sophisticated_highlight(
        self,
        summary_points: list[str],
        title: str,
        japanese_title: str,
        is_update: bool
    ) -> str:
        """Create a sophisticated, PRD-quality highlight sentence."""

        if not summary_points:
            return ""

        first_point = summary_points[0]

        # Extract key entities and actions with advanced patterns
        companies = self._extract_companies(first_point, title)
        products = self._extract_products(first_point, title)
        actions = self._extract_key_actions(first_point)
        metrics = self._extract_metrics(first_point)

        # Build sophisticated highlight
        highlight_parts = []

        # Company/product identification
        if companies or products:
            entity = companies[0] if companies else products[0]
            if is_update:
                highlight_parts.append(f"{entity}ãŒæ–°ãŸã«")
            else:
                highlight_parts.append(f"{entity}ãŒ")

        # Core action with context (remove company name if already added)
        if actions:
            action = actions[0]

            # Remove company name from action if we already added it
            if companies:
                company = companies[0]
                # Remove company name and particles from the action
                action = re.sub(rf'{re.escape(company)}(?:ãŒ|ã¯|ã®)?', '', action).strip()

            if metrics:
                metric = metrics[0]
                highlight_parts.append(f"{action}ã€{metric}ã®")
            else:
                highlight_parts.append(action)

        # Additional context from second summary point
        if len(summary_points) > 1:
            second_point = summary_points[1]
            context = self._extract_business_context(second_point)
            if context:
                highlight_parts.append(f"ã€‚{context}")

        if highlight_parts:
            base_highlight = "".join(highlight_parts)
            # Ensure proper sentence ending
            if not base_highlight.endswith(('ã€‚', 'ã€‚')):
                base_highlight += "ã¨ç™ºè¡¨ã—ã¾ã—ãŸã€‚"
            return base_highlight

        # Fallback to enhanced first point
        return first_point if len(first_point) > 30 else ""

    def _extract_companies(self, text: str, title: str) -> list[str]:
        """Extract company names with expanded recognition."""
        companies = []
        combined_text = text + " " + title

        company_patterns = [
            r'(OpenAI|ChatGPT|GPT-\d+)',
            r'(Google|Alphabet|DeepMind|Gemini)',
            r'(Meta|Facebook|Instagram)',
            r'(Microsoft|Azure|Copilot)',
            r'(Apple|iPhone|iPad|macOS)',
            r'(Amazon|AWS|Alexa)',
            r'(NVIDIA|Tesla|SpaceX)',
            r'(Anthropic|Claude)',
            r'(IBM|Watson)',
            r'(Samsung|LG|Sony)'
        ]

        for pattern in company_patterns:
            matches = re.findall(pattern, combined_text, re.IGNORECASE)
            companies.extend(matches)

        return list(dict.fromkeys(companies))  # Remove duplicates while preserving order

    def _extract_products(self, text: str, title: str) -> list[str]:
        """Extract product/technology names."""
        products = []
        combined_text = text + " " + title

        product_patterns = [
            r'(GPT-\d+(?:\.\d+)?|ChatGPT|GPT)',
            r'(Claude(?:-\d+)?)',
            r'(Gemini(?:\s+\d+\.\d+)?)',
            r'(LLaMA|Llama)',
            r'(DALL-E|DALLÂ·E)',
            r'(Midjourney|Stable Diffusion)',
            r'(TensorFlow|PyTorch)',
            r'(Transformer|BERT|T5)'
        ]

        for pattern in product_patterns:
            matches = re.findall(pattern, combined_text, re.IGNORECASE)
            products.extend(matches)

        return list(dict.fromkeys(products))

    def _extract_key_actions(self, text: str) -> list[str]:
        """Extract key business actions."""
        actions = []

        action_patterns = [
            # Extract action part after company name, avoiding full sentence capture
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹|é–‹å§‹|å°å…¥|å®Ÿè£…|å±•é–‹|æä¾›))',
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:å‘ä¸Š|æ”¹å–„|å¼·åŒ–|æ‹¡å¤§|å¢—å¤§|å€å¢—))',
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:å€¤ä¸‹ã’|ä¾¡æ ¼.*?å‰Šæ¸›|ã‚³ã‚¹ãƒˆ.*?å‰Šæ¸›))',
            r'(?:ãŒ|ã¯)([^ã€‚]*?(?:æ–°æ©Ÿèƒ½|æ–°ã‚µãƒ¼ãƒ“ã‚¹|æ–°è£½å“|æ–°æŠ€è¡“))',
            # Fallback: extract just the action verb and immediate context
            r'((?:\d+[%å€å„„ãƒ‰ãƒ«]*.*?)?(?:ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹|é–‹å§‹|å°å…¥))',
            r'((?:\d+[%å€]*.*?)?(?:å‘ä¸Š|æ”¹å–„|å¼·åŒ–))'
        ]

        for pattern in action_patterns:
            matches = re.findall(pattern, text)
            if matches:
                actions.extend([match.strip() for match in matches if len(match.strip()) > 10])

        return actions[:3]  # Top 3 most relevant actions

    def _extract_metrics(self, text: str) -> list[str]:
        """Extract quantitative metrics."""
        metrics = []

        metric_patterns = [
            r'(\d+%(?:.*?(?:å‘ä¸Š|æ”¹å–„|å‰Šæ¸›|å¢—åŠ |æ¸›å°‘))?)',
            r'(\d+å€(?:.*?(?:å‘ä¸Š|æ”¹å–„|å¢—åŠ ))?)',
            r'(\$\d+(?:\.\d+)?(?:[MKB]illion)?)',
            r'(v?\d+\.\d+)',
            r'(\d+(?:GB|TB|PB))',
        ]

        for pattern in metric_patterns:
            matches = re.findall(pattern, text)
            metrics.extend(matches)

        return metrics

    def _extract_business_context(self, text: str) -> str:
        """Extract business/impact context."""
        context_patterns = [
            r'([^ã€‚]*(?:ä¼æ¥­|ãƒ“ã‚¸ãƒã‚¹|æ¥­ç•Œ|å¸‚å ´|ç«¶äº‰)[^ã€‚]*)',
            r'([^ã€‚]*(?:æ´»ç”¨|å¿œç”¨|å°å…¥|å®Ÿç”¨)[^ã€‚]*)',
            r'([^ã€‚]*(?:é–‹ç™ºè€…|ãƒ¦ãƒ¼ã‚¶ãƒ¼|é¡§å®¢)[^ã€‚]*)',
            r'([^ã€‚]*(?:å½±éŸ¿|åŠ¹æœ|æˆæœ|çµæœ)[^ã€‚]*)'
        ]

        for pattern in context_patterns:
            match = re.search(pattern, text)
            if match:
                context = match.group(1).strip()
                if len(context) > 15 and len(context) < 100:
                    return context

        return ""

    def _extract_contextual_impact(self, summary_points: list[str], title: str) -> str:
        """Create contextual impact statement for complementary information."""

        if len(summary_points) < 2:
            return ""

        # Look for complementary information in subsequent points
        for point in summary_points[1:]:
            if any(keyword in point for keyword in [
                'ä¸€æ–¹', 'ã¾ãŸ', 'ã•ã‚‰ã«', 'åŠ ãˆã¦', 'åŒæ™‚ã«', 'ä¸¦è¡Œã—ã¦'
            ]):
                # Extract the contrasting or additional information
                context_match = re.search(r'([^ã€‚]*(?:ç™ºè¡¨|é–‹å§‹|å°å…¥|å®Ÿè£…)[^ã€‚]*)', point)
                if context_match and len(context_match.group(1)) > 20:
                    return f"ä¸€æ–¹ã€{context_match.group(1).strip()}ã€‚"

        return ""

    def _create_thematic_highlights(self, articles: list[ProcessedArticle]) -> list[str]:
        """Create thematic highlights when individual article highlights are insufficient."""

        highlights = []

        # Analyze themes across articles
        all_text = []
        for article in articles:
            if (article.summarized_article and
                article.summarized_article.summary and
                article.summarized_article.summary.summary_points):
                all_text.extend(article.summarized_article.summary.summary_points)

        combined_text = " ".join(all_text)

        # Technology themes
        if any(term in combined_text for term in ['LLM', 'GPT', 'è¨€èªãƒ¢ãƒ‡ãƒ«', 'AI']):
            highlights.append("å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰åˆ†é‡ã§ã¯è¤‡æ•°ã®é‡è¦ãªæŠ€è¡“é€²å±•ãŒå ±å‘Šã•ã‚Œã€å®Ÿç”¨åŒ–ã«å‘ã‘ãŸå‹•ããŒåŠ é€Ÿã—ã¦ã„ã¾ã™ã€‚")

        # Business application themes
        if any(term in combined_text for term in ['ä¼æ¥­', 'ãƒ“ã‚¸ãƒã‚¹', 'å°å…¥', 'æ´»ç”¨']):
            highlights.append("ä¼æ¥­ã«ãŠã‘ã‚‹AIæ´»ç”¨ãŒæœ¬æ ¼åŒ–ã—ã€å…·ä½“çš„ãªæ¥­å‹™æ”¹å–„ã‚„æ–°ã‚µãƒ¼ãƒ“ã‚¹å‰µå‡ºã®äº‹ä¾‹ãŒç›¸æ¬¡ã„ã§ç™ºè¡¨ã•ã‚Œã¦ã„ã¾ã™ã€‚")

        # Research and development themes
        if any(term in combined_text for term in ['ç ”ç©¶', 'é–‹ç™º', 'æŠ€è¡“', 'æ€§èƒ½']):
            highlights.append("AIç ”ç©¶åˆ†é‡ã§ã¯æ€§èƒ½å‘ä¸Šã¨å®Ÿç”¨æ€§ã‚’ä¸¡ç«‹ã•ã›ã‚‹æŠ€è¡“é–‹ç™ºãŒé€²ã¿ã€ç”£æ¥­ç•Œã¸ã®å½±éŸ¿ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚")

        return highlights

    def _extract_key_themes(self, articles: list[ProcessedArticle]) -> list[str]:
        """Extract key themes from articles."""

        # Common AI themes and their keywords
        theme_keywords = {
            "OpenAIãƒ»GPTé–¢é€£": ["openai", "gpt", "chatgpt"],
            "Googleãƒ»Geminié–¢é€£": ["google", "gemini", "bard"],
            "Anthropicãƒ»Claudeé–¢é€£": ["anthropic", "claude"],
            "ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹": ["ä¼æ¥­", "ãƒ“ã‚¸ãƒã‚¹", "æŠ•è³‡", "è³‡é‡‘èª¿é”", "startup"],
            "ç ”ç©¶ãƒ»æŠ€è¡“": ["ç ”ç©¶", "è«–æ–‡", "æŠ€è¡“", "é–‹ç™º", "breakthrough"],
            "è¦åˆ¶ãƒ»æ”¿ç­–": ["è¦åˆ¶", "æ”¿ç­–", "æ³•å¾‹", "æ”¿åºœ", "policy"],
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿": ["é‡å­", "quantum"],
            "ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹": ["ãƒ­ãƒœãƒƒãƒˆ", "robot", "è‡ªå‹•åŒ–"],
            "ç”»åƒç”Ÿæˆ": ["ç”»åƒç”Ÿæˆ", "stable diffusion", "midjourney", "dall-e"]
        }

        # Count theme occurrences
        theme_counts = {}

        for article in articles:
            title = article.summarized_article.filtered_article.raw_article.title.lower()
            content = ' '.join(article.summarized_article.summary.summary_points).lower()
            text = f"{title} {content}"

            for theme, keywords in theme_keywords.items():
                for keyword in keywords:
                    if keyword in text:
                        theme_counts[theme] = theme_counts.get(theme, 0) + 1
                        break  # Count theme only once per article

        # Return top themes
        sorted_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)
        return [theme for theme, count in sorted_themes[:3] if count > 0]

    def _generate_specific_theme_title(self, themes: list[str], articles: list[ProcessedArticle]) -> str | None:
        """å…·ä½“çš„ãªãƒ†ãƒ¼ãƒã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆï¼ˆæ±ç”¨è¡¨ç¾ã‚’å›é¿ï¼‰"""
        try:
            # è¨˜äº‹ã‹ã‚‰å…·ä½“çš„ãªä¼æ¥­åãƒ»æŠ€è¡“åã‚’æŠ½å‡º
            companies = []
            technologies = []

            for article in articles[:3]:  # ä¸Šä½3è¨˜äº‹ã‹ã‚‰æŠ½å‡º
                raw_article = article.summarized_article.filtered_article.raw_article
                text = f"{raw_article.title} {raw_article.content or ''}"

                # ä¼æ¥­åæŠ½å‡º
                company_patterns = [
                    'OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic', 'Apple',
                    'Amazon', 'Tesla', 'NVIDIA', 'DeepMind', 'LinkedIn', 'GitHub'
                ]

                for company in company_patterns:
                    if company.lower() in text.lower() and company not in companies:
                        companies.append(company)

                # æŠ€è¡“ãƒ»è£½å“åæŠ½å‡º
                tech_patterns = [
                    'ChatGPT', 'GPT-4', 'Gemini', 'Claude', 'Llama', 'Copilot',
                    'API', 'CLI', 'LLM', 'Agent', 'RAG', 'Transformer'
                ]

                for tech in tech_patterns:
                    if tech.lower() in text.lower() and tech not in technologies:
                        technologies.append(tech)

            # å…·ä½“çš„ãªã‚¿ã‚¤ãƒˆãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
            if companies and technologies:
                return f"{companies[0]}ãƒ»{technologies[0]}ã‚’ä¸­å¿ƒã¨ã—ãŸæŠ€è¡“é€²å±•"
            elif len(companies) >= 2:
                return f"{companies[0]}ãƒ»{companies[1]}ã«ã‚ˆã‚‹é‡è¦ç™ºè¡¨"
            elif companies:
                return f"{companies[0]}ã®æŠ€è¡“é©æ–°ã¨æ¥­ç•Œå‹•å‘"
            elif technologies:
                return f"{technologies[0]}æŠ€è¡“ã®é€²æ­©ã¨æ´»ç”¨å±•é–‹"
            else:
                return None

        except Exception as e:
            logger.warning(f"Specific theme title generation failed: {e}")
            return None

    def _generate_fallback_specific_title(self, articles: list[ProcessedArticle]) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®å…·ä½“çš„ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæ±ç”¨è¡¨ç¾ã‚’é¿ã‘ã‚‹ï¼‰"""
        try:
            if articles:
                # æœ€åˆã®è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰è¦ç´ ã‚’æŠ½å‡º
                first_article = articles[0]
                raw_title = first_article.summarized_article.filtered_article.raw_article.title

                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å…·ä½“çš„è¦ç´ ã‚’æŠ½å‡º
                if any(company in raw_title for company in ['OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic']):
                    return "å¤§æ‰‹AIä¼æ¥­ã«ã‚ˆã‚‹æŠ€è¡“ç™ºè¡¨ãƒ»æˆ¦ç•¥è»¢æ›"
                elif any(tech in raw_title for tech in ['ChatGPT', 'GPT', 'Gemini', 'Claude', 'LLM']):
                    return "ä¸»è¦AIãƒ¢ãƒ‡ãƒ«ã®æ©Ÿèƒ½å¼·åŒ–ãƒ»æ–°å±•é–‹"
                elif any(keyword in raw_title for keyword in ['ç ”ç©¶', 'Research', 'è«–æ–‡']):
                    return "AIç ”ç©¶ã®æœ€æ–°æˆæœãƒ»æŠ€è¡“ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼"
                elif any(keyword in raw_title for keyword in ['æŠ•è³‡', 'è³‡é‡‘', 'funding']):
                    return "AIæ¥­ç•Œã®æŠ•è³‡å‹•å‘ãƒ»ä¼æ¥­å‹•å‘"
                else:
                    # æœ€å¾Œã®æ‰‹æ®µï¼šè¨˜äº‹æ•°ã‚’æ´»ç”¨
                    return f"æ³¨ç›®AIä¼æ¥­{len(articles)}ç¤¾ã®é‡è¦ç™ºè¡¨"
            else:
                return "AIæŠ€è¡“ã®é‡è¦ãªé€²å±•"

        except Exception as e:
            logger.warning(f"Fallback specific title generation failed: {e}")
            return "AIæŠ€è¡“ã®é‡è¦ãªé€²å±•"

    def _save_newsletter(
        self,
        content: str,
        date: datetime,
        edition: str,
        output_dir: str
    ) -> Path:
        """Save newsletter content to file."""

        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # Generate filename â€“ include HHMM to avoid overwrite and aid manual comparison
        # "2025-06-22_0135_daily_newsletter.md" ã®ã‚ˆã†ãªå½¢å¼ã«ãªã‚‹
        date_str = date.strftime("%Y-%m-%d_%H%M")
        filename = f"{date_str}_{edition}_newsletter.md"

        output_file = output_path / filename

        # Write content
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(content)

            if HAS_LOGGER:
                logger.info("Newsletter saved", file=str(output_file))
            else:
                logger.info(f"Newsletter saved: {output_file}")

        except Exception as e:
            if HAS_LOGGER:
                logger.error(
                    "Failed to save newsletter",
                    file=str(output_file),
                    error=str(e)
                )
            else:
                logger.error(f"Failed to save newsletter: file={output_file}, error={str(e)}")
            raise

        return output_file

    def _regex_replace_filter(self, text: str, pattern: str, replacement: str) -> str:
        """Jinja2 filter for regex replacement."""
        return re.sub(pattern, replacement, text)

    def _slugify_filter(self, text: str) -> str:
        """Jinja2 filter to create URL-friendly slugs."""
        text = re.sub(r'<[^>]+>', '', text)
        text = text.lower()
        text = re.sub(r'[^\w\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+', '-', text)
        text = text.strip('-')
        if len(text) > 50:
            text = text[:50].rstrip('-')
        return text

    def _generate_lead_title_from_articles(self, articles: list[ProcessedArticle], edition: str) -> str:
        """Generate a lead title based on the articles and edition."""

        if not articles:
            return "ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹ã¨AIæŠ€è¡“ã®æœ€æ–°å‹•å‘"

        # Extract key companies and technologies
        companies = set()
        technologies = set()

        for article in articles[:5]:  # Use top 5 articles
            try:
                title = article.summarized_article.filtered_article.raw_article.title

                # Extract companies
                for company in ['OpenAI', 'Google', 'Meta', 'Microsoft', 'Anthropic', 'Apple', 'Amazon', 'LinkedIn']:
                    if company in title:
                        companies.add(company)

                # Extract technologies
                for tech in ['ChatGPT', 'GPT-4', 'Gemini', 'Claude', 'AI', 'LLM', 'ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ', 'API']:
                    if tech in title:
                        technologies.add(tech)

            except Exception as e:
                logger.debug(f"Error extracting title info: {e}")
                continue

        # Generate title based on content
        companies_list = list(companies)[:2]
        technologies_list = list(technologies)[:2]

        if companies_list and technologies_list:
            companies_str = "ãƒ»".join(companies_list)
            tech_str = "ãƒ»".join(technologies_list)
            return f"{companies_str}ã«ã‚ˆã‚‹{tech_str}ã®æœ€æ–°å‹•å‘"
        elif companies_list:
            companies_str = "ãƒ»".join(companies_list)
            return f"{companies_str}ã®AIæŠ€è¡“é–‹ç™ºå‹•å‘"
        elif technologies_list:
            tech_str = "ãƒ»".join(technologies_list)
            return f"{tech_str}ã®ä¼æ¥­æ´»ç”¨ã¨æŠ€è¡“é€²å±•"
        else:
            # Default based on edition
            if edition == "daily":
                return "ä¼æ¥­ãƒ»ãƒ“ã‚¸ãƒã‚¹ã¨AIæŠ€è¡“ã®æœ€æ–°å‹•å‘"
            else:
                return f"AIåˆ†é‡ã®{edition}ãƒ¬ãƒãƒ¼ãƒˆ"

    def _short_title_filter(self, text: str, max_chars: int = 70) -> str:
        """Jinja2 filter to shorten title text."""
        return self._intelligent_truncate(text, max_chars)

    def _toc_format_filter(self, text: str) -> str:
        """Jinja2 filter for formatting table of contents entries with improved Japanese truncation."""

        if not text:
            return text

        # Clean hash marks that might leak into title (Fix for PRD F-5)
        text = re.sub(r'^#+\s*', '', text.strip())

        # For TOC, we want to show meaningful content but avoid extremely long entries
        max_toc_length = 80  # Optimal length for TOC entries

        # If text is already short enough, return as is
        if len(text) <= max_toc_length:
            return text

        # Special handling for titles with quoted content like ã€ŒDeep Researchã€
        # Try to include the quoted part if possible
        quote_pattern = r'ã€Œ([^ã€]+)ã€'
        quote_match = re.search(quote_pattern, text)

        if quote_match:
            quote_start = quote_match.start()
            quote_end = quote_match.end()

            # If the quote appears early enough, try to include it
            if quote_end <= max_toc_length + 10:  # Allow slight overflow for quotes
                # Find a good cut point after the quote
                cut_text = text[:quote_end]

                # Look for natural break points after the quote
                remaining = text[quote_end:]
                for break_char in ['ã‚’', 'ãŒ', 'ã«', 'ã§', 'ã¨']:
                    break_pos = remaining.find(break_char)
                    if break_pos != -1 and break_pos < 10:
                        cut_text = text[:quote_end + break_pos + 1]
                        if len(cut_text) <= max_toc_length + 5:
                            return cut_text.rstrip() + 'â€¦'
                        break

                # If we can fit the quote, use it
                if len(cut_text) <= max_toc_length + 10:
                    return cut_text.rstrip() + 'â€¦'

        # For very long text, try to find natural breaking points
        if len(text) > 120:
            # Look for early natural breaks
            early_break_patterns = [
                (r'ã«ãŠã„ã¦', 'ã€'),
                (r'ã«ã¤ã„ã¦', 'ã€'),
                (r'ã«é–¢ã—ã¦', 'ã€'),
                (r'ã«ã‚ˆã‚‹', ''),
                (r'ã®ãŸã‚ã®', ''),
                (r'ã‚’é€šã˜ã¦', ''),
                (r'ã«ãŠã‘ã‚‹', ''),
            ]

            for keyword, suffix in early_break_patterns:
                pattern = keyword + suffix
                match_pos = text.find(pattern)
                if match_pos != -1 and 20 <= match_pos <= 60:
                    # Cut after the pattern
                    cut_pos = match_pos + len(pattern)
                    return text[:cut_pos].rstrip() + 'â€¦'

        # Look for good breaking points within the limit
        snippet = text[:max_toc_length]

        # Priority: complete phrases or natural boundaries
        # 1. Try to break at punctuation
        for punct in ['ã€', 'ã€‚', 'ãƒ»']:
            last_punct = snippet.rfind(punct)
            if last_punct > max_toc_length * 0.6:  # At least 60% of target length
                return snippet[:last_punct + 1].rstrip()

        # 2. Try to break after particles (but not certain ones that need completion)
        safe_particles = ['ã§', 'ã‹ã‚‰', 'ã¾ã§', 'ã‚ˆã‚Š', 'ã«ã¦']
        for particle in safe_particles:
            last_pos = snippet.rfind(particle)
            if last_pos > max_toc_length * 0.6:
                return snippet[:last_pos + len(particle)].rstrip() + 'â€¦'

        # 3. Avoid breaking in the middle of important terms
        # Find the last "safe" position (not in the middle of a word/term)
        safe_pos = max_toc_length
        while safe_pos > max_toc_length * 0.7:
            char = text[safe_pos - 1] if safe_pos > 0 else ''
            next_char = text[safe_pos] if safe_pos < len(text) else ''

            # Don't break in the middle of:
            # - ASCII words
            # - Katakana words
            # - Numbers
            if (char.isascii() and char.isalnum() and next_char.isascii() and next_char.isalnum()) or \
               (ord('ã‚¡') <= ord(char) <= ord('ãƒ¶') and ord('ã‚¡') <= ord(next_char) <= ord('ãƒ¶')) or \
               (char.isdigit() and next_char.isdigit()):
                safe_pos -= 1
                continue
            else:
                break

        # Use the safe position
        result = text[:safe_pos].rstrip()

        # Clean up any trailing particles that make it incomplete
        if result.endswith(('ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã¨')):
            result = result[:-1].rstrip()

        # Add ellipsis if we actually truncated
        if len(result) < len(text):
            result += 'â€¦'

        return result

    async def _generate_article_title_integrated(
        self,
        article: ProcessedArticle,
        is_update: bool = False
    ) -> str:
        """
        ğŸ”¥ ULTRA THINK: çµ±åˆã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã§ã€ç¶šå ±ã€‘+ğŸ†™é‡è¤‡ã‚’æ ¹æœ¬è§£æ±º

        Args:
            article: Processed article
            is_update: Whether this is an update article (determines emoji usage)

        Returns:
            Generated Japanese title with proper update handling
        """
        # First try LLM-based generation if available
        if HAS_LLM_ROUTER and self.llm_router:
            try:
                # LLM generates a base summary sentence
                base_summary = await self.llm_router.generate_japanese_title(article)

                if base_summary:
                    logger.debug(f"LLM generated base summary for title: {base_summary}")
                    # Apply integrated formatting with update context
                    return self._format_headline_integrated(base_summary, is_update)
            except Exception as e:
                logger.warning(f"LLM title generation failed, falling back: {e}")

        # Fallback to using the first summary point with integrated logic
        return self._generate_integrated_fallback_title(article, is_update)

    def _validate_title_quality(self, title: str) -> dict[str, Any]:
        """
        Validate title quality and detect common issues.

        Args:
            title: Generated title to validate

        Returns:
            Dictionary with validation results and suggested improvements
        """
        import re

        issues = []
        suggestions = []
        score = 100  # Start with perfect score

        # Check 1: Duplicate word patterns
        duplicate_patterns = [
            (r'(\w+)\s+\1\b', 'åŒã˜å˜èªãŒé€£ç¶šã—ã¦ã„ã¾ã™'),
            (r'(LLM|AI|GPT)ã®æŠ€è¡“.*?\1', 'LLM/AIç”¨èªã®é‡è¤‡ãŒã‚ã‚Šã¾ã™'),
            (r'(\w+)æŠ€è¡“\s*\1', 'æŠ€è¡“ç”¨èªã®é‡è¤‡ãŒã‚ã‚Šã¾ã™')
        ]

        for pattern, message in duplicate_patterns:
            if re.search(pattern, title):
                issues.append(f'é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³: {message}')
                suggestions.append('é‡è¤‡ã—ãŸå˜èªã‚’é™¤å»ã—ã¦ãã ã•ã„')
                score -= 30

        # Check 2: Generic/vague titles
        generic_patterns = [
            r'^LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰$',
            r'^æŠ€è¡“é€²å±•$',
            r'^AIæŠ€è¡“$',
            r'^æ–°æŠ€è¡“$'
        ]

        for pattern in generic_patterns:
            if re.search(pattern, title):
                issues.append('æ±ç”¨çš„ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã§ã™')
                suggestions.append('ã‚ˆã‚Šå…·ä½“çš„ãªæƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„')
                score -= 40
                break

        # Check 3: Incomplete titles (cut off)
        if title.endswith('...'):
            issues.append('ã‚¿ã‚¤ãƒˆãƒ«ãŒä¸­é€”åŠç«¯ã§åˆ‡ã‚Œã¦ã„ã¾ã™')
            suggestions.append('é©åˆ‡ãªé•·ã•ã§å®Œçµã•ã›ã¦ãã ã•ã„')
            score -= 25

        # Check 4: Missing key information
        if len(title) < 15:
            issues.append('ã‚¿ã‚¤ãƒˆãƒ«ãŒçŸ­ã™ãã¾ã™')
            suggestions.append('ã‚ˆã‚Šè©³ç´°ãªæƒ…å ±ã‚’è¿½åŠ ã—ã¦ãã ã•ã„')
            score -= 15

        # Check 5: Too many emoji
        emoji_count = len(re.findall(r'[ï¿½-ô°€-ï¿½â˜€-âŸ¿]', title))
        if emoji_count > 2:
            issues.append('çµµæ–‡å­—ãŒå¤šã™ãã¾ã™')
            suggestions.append('çµµæ–‡å­—ã¯1-2å€‹ã¾ã§ã«æŠ‘ãˆã¦ãã ã•ã„')
            score -= 10

        # Calculate final quality level
        if score >= 90:
            quality = 'excellent'
        elif score >= 70:
            quality = 'good'
        elif score >= 50:
            quality = 'fair'
        else:
            quality = 'poor'

        return {
            'score': max(0, score),
            'quality': quality,
            'issues': issues,
            'suggestions': suggestions,
            'is_acceptable': score >= 60
        }

    def _improve_title_based_on_validation(self, title: str, validation_result: dict[str, Any]) -> str:
        """
        Attempt to automatically improve title based on validation results.

        Args:
            title: Original title
            validation_result: Results from _validate_title_quality

        Returns:
            Improved title
        """
        import re

        improved_title = title

        # Fix duplicate patterns
        improved_title = re.sub(r'(\w+)\s+\1\b', r'\1', improved_title)
        improved_title = re.sub(r'(LLM|AI|GPT)(ã®æŠ€è¡“).*?\1', r'\1\2', improved_title)
        improved_title = re.sub(r'(\w+)æŠ€è¡“\s*\1', r'\1æŠ€è¡“', improved_title)

        # Remove trailing ellipsis and try to complete
        if improved_title.endswith('...'):
            improved_title = improved_title[:-3].strip()
            if not improved_title.endswith(('ã€‚', 'ã§ã™', 'ã¾ã™', 'ãŸ')):
                improved_title += 'ã‚’ç™ºè¡¨'

        # Clean up spacing
        improved_title = re.sub(r'\s+', ' ', improved_title.strip())

        return improved_title

    async def _generate_article_title(self, article: ProcessedArticle) -> str:
        """
        DEPRECATED: Use _generate_article_title_integrated instead.
        Kept for compatibility with existing code.
        """
        logger.warning("Using deprecated _generate_article_title. Use _generate_article_title_integrated instead.")
        return await self._generate_article_title_integrated(article, is_update=False)

    def _format_headline_integrated(self, summary_sentence: str, is_update: bool) -> str:
        """ğŸ”¥ ULTRA THINK: çµ±åˆãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ç”Ÿæˆã§ã€ç¶šå ±ã€‘+ğŸ†™é‡è¤‡ã‚’å®Œå…¨é˜²æ­¢"""

        # ğŸ”¥ ULTRA THINK: ã€ç¶šå ±ã€‘ãƒ†ã‚­ã‚¹ãƒˆå®Œå…¨é™¤å»å¼·åŒ–
        summary_sentence = re.sub(r'ã€?ç¶šå ±ã€‘?[:ï¼š]?\s*', '', summary_sentence)
        summary_sentence = re.sub(r'^ç¶šå ±[:ï¼š]\s*', '', summary_sentence)
        summary_sentence = re.sub(r'ç¶šå ±\s*[ï¼š:]\s*', '', summary_sentence)  # ä¸­é–“ä½ç½®ã®ç¶šå ±ã‚‚é™¤å»
        summary_sentence = re.sub(r'\s*ç¶šå ±$', '', summary_sentence)  # æœ«å°¾ã®ç¶šå ±ã‚‚é™¤å»

        # STEP 2: Extract key entity for specificity
        entity_match = re.search(r'ã€Œ([^ã€]+)ã€|\b([A-Z][a-zA-Z0-9]+)\b', summary_sentence)
        entity = ""
        if entity_match:
            entity = entity_match.group(1) or entity_match.group(2)

        # STEP 3: Truncate for proper length
        truncated_summary = ensure_sentence_completeness(summary_sentence, self.settings.processing.title_short_length)

        # STEP 4: Build base title
        if entity and entity not in truncated_summary:
            base_title = f"{entity}ã€{truncated_summary}"
        else:
            base_title = truncated_summary

        # STEP 5: Add UPDATE emoji ONLY if is_update=True (unified processing)
        if is_update:
            # Ensure no duplicate emoji
            if 'ğŸ†™' not in base_title:
                final_title = f"{base_title}ğŸ†™"
                logger.debug(f"Added UPDATE emoji: {base_title} â†’ {final_title}")
                return final_title
            else:
                logger.debug(f"UPDATE emoji already present: {base_title}")
                return base_title
        else:
            return base_title

    def _format_headline_from_summary(self, summary_sentence: str) -> str:
        """DEPRECATED: Use _format_headline_integrated instead."""
        logger.warning("Using deprecated _format_headline_from_summary. Use _format_headline_integrated instead.")
        return self._format_headline_integrated(summary_sentence, is_update=False)

    def _generate_integrated_fallback_title(self, article: ProcessedArticle, is_update: bool) -> str:
        """ğŸ”¥ ULTRA THINK: çµ±åˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ"""
        try:
            summary_points = article.summarized_article.summary.summary_points
            if summary_points and summary_points[0]:
                # Use the first bullet point with integrated processing
                first_point = summary_points[0]

                # ğŸ”¥ ULTRA THINK: ã€ç¶šå ±ã€‘ãƒ†ã‚­ã‚¹ãƒˆå®Œå…¨é™¤å»å¼·åŒ–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                first_point = re.sub(r'ã€?ç¶šå ±ã€‘?[:ï¼š]?\s*', '', first_point)
                first_point = re.sub(r'^ç¶šå ±[:ï¼š]\s*', '', first_point)
                first_point = re.sub(r'ç¶šå ±\s*[ï¼š:]\s*', '', first_point)  # ä¸­é–“ä½ç½®
                first_point = re.sub(r'\s*ç¶šå ±$', '', first_point)  # æœ«å°¾
                first_point = re.sub(r'ç¶šå ±.*?ã€', '', first_point)  # ç¶šå ±...ã€ãƒ‘ã‚¿ãƒ¼ãƒ³

                # Extract key entities and actions - æ‹¡å¼µç‰ˆ
                company_match = re.search(
                    r'(OpenAI|Google|Meta|Microsoft|Anthropic|Apple|Amazon|NVIDIA|DeepMind|Agentica|Together AI|'
                    r'Hugging Face|TechCrunch|VentureBeat|WIRED|IEEE|NextWord|SemiAnalysis|'
                    r'[A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*?)(?:ç¤¾|ãŒ|ã¯|ã®|ã€)',
                    first_point
                )
                company = company_match.group(1) if company_match else None

                # æ•°å€¤ãƒ»æˆæœã®æŠ½å‡ºå¼·åŒ–
                numbers_match = re.search(r'(\d+(?:\.\d+)?[%ä¸‡å„„å††ãƒ‰ãƒ«å€ä»¶å]|Pass@1\s+\d+\.\d+%|SOTA|å¹´é–“\d+ä¸‡ãƒ‰ãƒ«)', first_point)
                numbers = numbers_match.group(1) if numbers_match else None

                # Pattern matching for key actions/technologies - æ‹¡å¼µç‰ˆ
                action_patterns = [
                    (r'(DeepSWE-Preview|ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ)', 'AIé–‹ç™ºãƒ„ãƒ¼ãƒ«'),
                    (r'(å¼·åŒ–å­¦ç¿’|RL|GRPO\+\+|rLLM)', 'æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“'),
                    (r'(Pass@1\s+\d+\.\d+%|SOTA|ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)', 'æ€§èƒ½æŒ‡æ¨™'),
                    (r'(SWE-Bench|Qwen3-32B)', 'AIåŸºç›¤'),
                    (r'(å¹´é–“\d+ä¸‡ãƒ‰ãƒ«|åç›Š|å£²ä¸Š|æŠ•è³‡)', 'äº‹æ¥­å±•é–‹'),
                    (r'(ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°|ã‚µãƒ¼ãƒ“ã‚¹æ‹¡å¤§)', 'ãƒ“ã‚¸ãƒã‚¹'),
                    (r'(\d+å?ã®?(?:AI)?ç ”ç©¶è€…)', 'ç ”ç©¶è€…'),
                    (r'(ãƒˆãƒƒãƒ—.*?ç ”ç©¶è€…)', 'äººæ'),
                    (r'(æ–°æ©Ÿèƒ½|æ–°ã‚µãƒ¼ãƒ“ã‚¹|æ–°æŠ€è¡“)', 'ãƒªãƒªãƒ¼ã‚¹'),
                    (r'(ç™ºè¡¨|å…¬é–‹|é–‹å§‹|å°å…¥|ç²å¾—|é›‡ç”¨)', None),
                    (r'(ChatGPT|Claude|Gemini|GPT-\d+)', 'AI'),
                    (r'(Deep Research|RAG|LLM)', 'æŠ€è¡“'),
                ]

                key_action = None
                action_type = None
                for pattern, type_hint in action_patterns:
                    match = re.search(pattern, first_point)
                    if match:
                        key_action = match.group(1)
                        action_type = type_hint
                        break

                # Build base title - æ”¹è‰¯ç‰ˆ
                if company and key_action and numbers:
                    # æœ€é«˜å“è³ª: ä¼æ¥­å + ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ + æ•°å€¤
                    base_title = f"{company}ã€{key_action}ã§{numbers}é”æˆ"
                elif company and key_action:
                    # é«˜å“è³ª: ä¼æ¥­å + ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                    if 'ç ”ç©¶è€…' in key_action:
                        base_title = f"{company}ãŒ{key_action}ã‚’ç²å¾—"
                    elif action_type == 'AIé–‹ç™ºãƒ„ãƒ¼ãƒ«':
                        base_title = f"{company}ã®{key_action}ãŒæ–°è¨˜éŒ²é”æˆ"
                    elif action_type == 'äº‹æ¥­å±•é–‹':
                        base_title = f"{company}ãŒ{key_action}ã‚’æœ¬æ ¼å±•é–‹"
                    elif action_type:
                        base_title = f"{company}ã®{action_type}ã§{key_action}ã‚’å®Ÿç¾"
                    else:
                        base_title = f"{company}ãŒ{key_action}ã‚’ç™ºè¡¨"
                elif company and numbers:
                    # ä¸­å“è³ª: ä¼æ¥­å + æ•°å€¤
                    base_title = f"{company}ã€{numbers}ã®æˆæœã‚’é”æˆ"
                elif company:
                    # åŸºæœ¬å“è³ª: ä¼æ¥­åã®ã¿
                    if 'AI' in first_point or 'LLM' in first_point:
                        base_title = f"{company}ãŒAIæŠ€è¡“ã§æ–°å±•é–‹"
                    elif 'ç ”ç©¶' in first_point:
                        base_title = f"{company}ãŒç ”ç©¶é–‹ç™ºåˆ†é‡ã§æ–°æˆ¦ç•¥"
                    else:
                        base_title = f"{company}ã«ã‚ˆã‚‹æŠ€è¡“é©æ–°ç™ºè¡¨"
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¦ç´„ã‹ã‚‰æ™ºçš„ã«æŠ½å‡º
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–¢é€£ã®å‡¦ç†
                    if 'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°' in first_point:
                        base_title = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã«é€²åŒ–ã€LLMç²¾åº¦æœ€å¤§åŒ–"
                    elif 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°' in first_point:
                        base_title = "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ‰‹æ³•ã®æ–°å±•é–‹"
                    elif 'Agenticaã¨Together AI' in first_point:
                        base_title = "Agenticaã¨Together AIã€DeepSWE-Previewã§ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–°è¨˜éŒ²"
                    elif 'DeepSWE' in first_point or 'Pass@1' in first_point:
                        base_title = "AIé–‹ç™ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ–°è¨˜éŒ²é”æˆ"
                    elif 'å¼·åŒ–å­¦ç¿’' in first_point and 'ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°' in first_point:
                        base_title = "å¼·åŒ–å­¦ç¿’AIã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è‡ªå‹•åŒ–ã§ç”»æœŸçš„é€²å±•"
                    else:
                        # æœ€å¾Œã®æ‰‹æ®µ
                        base_title = ensure_sentence_completeness(first_point, 45)
                        # é‡è¤‡é™¤å»ã‚’æœ€å¾Œã«é©ç”¨
                        base_title = self._clean_llm_generated_title(base_title)

                        # Title quality validation
                        if self._is_highly_generic_title(base_title):
                            # Generate a more specific title based on content
                            base_title = self._generate_specific_title_from_content(article)

                # Apply integrated update processing
                if is_update:
                    if 'ğŸ†™' not in base_title:
                        return f"{base_title}ğŸ†™"
                    else:
                        return base_title
                else:
                    return base_title

        except Exception as e:
            logger.warning(f"Integrated fallback title generation failed: {e}")

        # Final fallback with update handling
        fallback = "AIæŠ€è¡“ã®æœ€æ–°å‹•å‘"
        if is_update:
            return f"{fallback}ğŸ†™"
        else:
            return fallback

    def _generate_improved_fallback_title(self, article: ProcessedArticle) -> str:
        """DEPRECATED: Use _generate_integrated_fallback_title instead."""
        logger.warning("Using deprecated _generate_improved_fallback_title. Use _generate_integrated_fallback_title instead.")
        return self._generate_integrated_fallback_title(article, is_update=False)

    def _clean_llm_generated_title(self, title: str) -> str:
        """Clean up common LLM title generation artifacts."""
        if not title or not isinstance(title, str):
            return ""

        # First apply the centralized duplicate pattern removal
        try:
            title = remove_duplicate_patterns(title)
        except Exception as e:
            logger.debug(f"Centralized duplicate removal failed: {e}")

        # Remove duplicate phrases and concatenation errors first
        title = re.sub(r'(.+?)(ã¨ç™ºè¡¨|ã¨å ±å‘Š|ã¨è¿°ã¹|ã¨èªã£|ã¨è¡¨æ˜)(.*?)\2', r'\1\2\3', title)
        # Fix duplicate company/product names (e.g., "Claude CodeAI Claude" -> "Claude Code") - more conservative
        title = re.sub(r'\b(\w+)\s+(AI|Code|Pro|Plus)\s+\1\b', r'\1 \2', title)
        title = re.sub(r'\b(\w+)(AI|Code|Pro|Plus)\1\b', r'\1\2', title)
        # Fix duplicate adjacent words
        title = re.sub(r'(\b\w+)\s+\1\b', r'\1', title)

        # Legacy patterns - kept for additional coverage beyond centralized patterns
        # Pattern 9: Specific numeric/text duplicates like "å¹´é–“1000ä¸‡ãƒ‰ãƒ«ã§å¹´é–“1000ä¸‡ãƒ‰ãƒ«"
        title = re.sub(r'(å¹´é–“\d+[ä¸‡å„„åƒ]?[ãƒ‰ãƒ«å††])(ã§|ã‚’|ãŒ|ã¯)\1', r'\1', title)
        title = re.sub(r'(\d+[ä¸‡å„„åƒ]?[ãƒ‰ãƒ«å††])(ã§|ã‚’|ãŒ|ã¯)\1', r'\1', title)
        # Pattern 10: More specific duplicates with currencies and numbers
        title = re.sub(r'(\d+ä¸‡ãƒ‰ãƒ«)(ã§|ã‚’|ãŒ|ã¯)(\d+ä¸‡ãƒ‰ãƒ«)', r'\1', title)
        title = re.sub(r'(å¹´é–“\d+ä¸‡ãƒ‰ãƒ«)(ã§|ã‚’|ãŒ|ã¯)(å¹´é–“\d+ä¸‡ãƒ‰ãƒ«)', r'\1', title)

        # Debug logging for title cleaning (can be removed in production)
        if 'å¹´é–“' in title and 'ä¸‡ãƒ‰ãƒ«' in title:
            print(f"DEBUG: Currency title after cleaning: '{title}'")

        # Remove verb endings like "ã€œã¨å ±ã˜ã‚‰ã‚Œã¾ã—ãŸ" or "ã€œã¨ç™ºè¡¨ã—ã¾ã—ãŸ"
        # to ensure the title is a proper headline (ä½“è¨€æ­¢ã‚).
        verb_endings_pattern = r"(?:ã¨å ±ã˜ã‚‰ã‚Œã¾ã—ãŸ|ã¨ç™ºè¡¨ã—ã¾ã—ãŸ|ã¨è¿°ã¹ã¾ã—ãŸ|ã¨èªã‚Šã¾ã—ãŸ|ã¨è¡¨æ˜ã—ã¾ã—ãŸ|ã¨æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸ|æ°ã¯|æ°ã¯ã€|ã‚’ç™ºè¡¨|ã‚’å ±å‘Š)$"
        cleaned_title = re.sub(verb_endings_pattern, '', title).strip()

        # Remove incomplete sentence patterns
        incomplete_patterns = r"(?:ã«ã¤ã„ã¦|ã«é–¢ã—ã¦|ã«ãŠã„ã¦|ã«å¯¾ã—ã¦|ã‚’ã‚ãã£ã¦)$"
        cleaned_title = re.sub(incomplete_patterns, '', cleaned_title).strip()

        # Additional cleanup for cases where a comma is left at the end
        if cleaned_title.endswith(('ã€', ',', 'ã€‚', 'ï¼š', ':')):
            cleaned_title = cleaned_title[:-1].strip()

        # Remove trailing particles that make titles incomplete and fix them properly
        if cleaned_title.endswith(('ãŒ', 'ã‚’', 'ã«', 'ã¯', 'ã§', 'ã¨', 'ã§ã®', 'ã®ãŸã‚', 'ã«ã‚ˆã‚Š', 'ã«ã‚ˆã£ã¦', 'ã¨ã—ã¦')):
            # é©åˆ‡ãªçµã³ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’å®Œæˆã•ã›ã‚‹
            if cleaned_title.endswith('ãŒ'):
                cleaned_title = cleaned_title[:-1] + 'ã‚’ç™ºè¡¨'
            elif cleaned_title.endswith('ã‚’'):
                cleaned_title = cleaned_title[:-1] + 'ãŒé€²å±•'
            elif cleaned_title.endswith(('ã«', 'ã§')):
                cleaned_title = cleaned_title[:-1] + 'ãŒåŠ é€Ÿ'
            else:
                cleaned_title = cleaned_title.rstrip('ã¯ã¨ã§ã®ãŸã‚ã«ã‚ˆã‚Šã«ã‚ˆã£ã¦ã¨ã—ã¦').strip()

        return cleaned_title

    def _is_highly_generic_title(self, title: str) -> bool:
        """Check if the title is too generic or low-quality."""
        # Reject generic titles and incomplete patterns
        highly_generic_patterns = [
            r'^AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹$',
            r'^æœ€æ–°AIå‹•å‘$',
            r'^æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹$',
            r'^æ¥­ç•Œå‹•å‘$',
            r'^æœ€æ–°æƒ…å ±$',
            r'^.*ã«ã¤ã„ã¦$',
            r'^.*ã«é–¢ã—ã¦$',
            r'^.*ã‚’ç™ºè¡¨ğŸ†™$',  # Broken titles ending with emoji
            r'^.*ã®$',  # Titles ending with incomplete possessive
            r'.*ã‚’ç™ºè¡¨ã—ãªãŒã‚‰',  # Incomplete "while announcing" patterns
        ]

        return any(re.search(pattern, title, re.IGNORECASE) for pattern in highly_generic_patterns)

    def _generate_specific_title_from_content(self, article: ProcessedArticle) -> str:
        """Generate a more specific title when the original is too generic."""
        try:
            # Extract key information from the article
            raw_article = article.summarized_article.filtered_article.raw_article
            summary_points = article.summarized_article.summary.summary_points

            # Try to extract company names from summary
            major_companies = ['OpenAI', 'Google', 'Microsoft', 'Apple', 'Meta', 'Amazon', 'Anthropic',
                              'DeepMind', 'Hugging Face', 'NVIDIA', 'Intel', 'AMD', 'Tesla', 'Uber']

            companies = []
            for point in summary_points:
                for company in major_companies:
                    if company in point:
                        companies.append(company)

            # Remove duplicates while preserving order
            unique_companies = list(dict.fromkeys(companies))

            # Extract key action verbs
            actions = []
            for point in summary_points:
                for action in ['ç™ºè¡¨', 'ç™ºå£²', 'å…¬é–‹', 'ãƒªãƒªãƒ¼ã‚¹', 'é–‹å§‹', 'æä¾›', 'å°å…¥', 'é–‹ç™º', 'æ”¹è‰¯', 'å‘ä¸Š']:
                    if action in point:
                        actions.append(action)
                        break

            # Build title
            title_parts = []

            # Add company name if found
            if unique_companies:
                title_parts.append(unique_companies[0])
                if len(unique_companies) > 1:
                    title_parts.append(f"ã¨{unique_companies[1]}")

            # Add action if found
            if actions:
                title_parts.append(f"ã€{actions[0]}")

            # Add subject matter
            first_point = summary_points[0] if summary_points else ""
            if 'AI' in first_point:
                title_parts.append("AIæŠ€è¡“")
            elif 'LLM' in first_point:
                title_parts.append("LLM")
            elif 'GPT' in first_point:
                title_parts.append("GPT")
            elif 'API' in first_point:
                title_parts.append("API")

            # Construct final title
            if title_parts:
                generated_title = "".join(title_parts)
                # Ensure it's not too long
                if len(generated_title) > 50:
                    generated_title = generated_title[:47] + "..."
                return generated_title
            else:
                # Ultimate fallback
                return "AIé–¢é€£æŠ€è¡“ã®æœ€æ–°å‹•å‘"

        except Exception as e:
            logger.warning(f"Failed to generate specific title: {e}")
            return "AIé–¢é€£æŠ€è¡“ã®æœ€æ–°å‹•å‘"

    def _generate_specific_lead_paragraphs(self, articles: list[ProcessedArticle], edition: str) -> list[str]:
        """Generate specific lead paragraphs that reflect actual article content."""
        if not articles:
            return []

        try:
            # Extract specific information from top 3 articles
            specific_items = []
            companies_mentioned = set()
            key_developments = []

            for article in articles[:3]:
                try:
                    if (article.summarized_article and
                        article.summarized_article.summary and
                        article.summarized_article.summary.summary_points):

                        summary_points = article.summarized_article.summary.summary_points
                        title = article.summarized_article.filtered_article.raw_article.title

                        # Extract company names
                        for company in ['OpenAI', 'Google', 'Microsoft', 'Apple', 'Meta', 'Amazon', 'Anthropic',
                                       'DeepMind', 'Hugging Face', 'NVIDIA', 'Tesla', 'AgenticaAI', 'Together AI']:
                            if company in " ".join(summary_points) or company in title:
                                companies_mentioned.add(company)

                        # Extract key developments (first point simplified)
                        if summary_points:
                            first_point = summary_points[0]
                            # Simplify and extract key action
                            for action in ['ç™ºè¡¨', 'ãƒªãƒªãƒ¼ã‚¹', 'é–‹ç™º', 'é”æˆ', 'å®Ÿç¾', 'æä¾›', 'å…¬é–‹', 'é–‹å§‹']:
                                if action in first_point:
                                    # Extract the essence of the development
                                    if 'ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°' in first_point and 'è¨˜éŒ²' in first_point:
                                        key_developments.append('AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ€§èƒ½å‘ä¸Š')
                                    elif 'billion' in first_point or 'å„„' in first_point or 'revenue' in first_point:
                                        key_developments.append('AIä¼æ¥­ã®åç›Šæ‹¡å¤§')
                                    elif 'Context Engineering' in first_point or 'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ' in first_point:
                                        key_developments.append('ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŠ€è¡“ã®é€²åŒ–')
                                    elif 'LLM' in first_point and 'å­¦ç¿’' in first_point:
                                        key_developments.append('LLMé–‹ç™ºæ‰‹æ³•ã®ä½“ç³»åŒ–')
                                    elif 'Code Hook' in first_point or 'ãƒ„ãƒ¼ãƒ«' in first_point:
                                        key_developments.append('é–‹ç™ºãƒ„ãƒ¼ãƒ«ã®æ©Ÿèƒ½å¼·åŒ–')
                                    break
                except Exception:
                    continue

            # Build specific paragraphs
            paragraphs = []

            # First paragraph: Specific companies and developments
            if companies_mentioned and key_developments:
                companies_list = list(companies_mentioned)[:2]  # Max 2 companies
                company_str = "ã€".join(companies_list)
                development_str = key_developments[0] if key_developments else "æ–°æŠ€è¡“"

                if len(companies_list) == 1:
                    first_para = f"{company_str}ãŒ{development_str}ã‚’ç™ºè¡¨ã™ã‚‹ãªã©ã€æœ¬æ—¥ã¯å…·ä½“çš„ãªæŠ€è¡“é€²å±•ãŒç›¸æ¬¡ã„ã§å ±å‘Šã•ã‚Œã¾ã—ãŸã€‚"
                else:
                    first_para = f"{company_str}ã‚’ã¯ã˜ã‚ã¨ã™ã‚‹ä¸»è¦ä¼æ¥­ãŒ{development_str}ãªã©é‡è¦ãªç™ºè¡¨ã‚’è¡Œã„ã¾ã—ãŸã€‚"
                paragraphs.append(first_para)
            else:
                paragraphs.append("æœ¬æ—¥ã¯æ³¨ç›®ã™ã¹ãAIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å³é¸ã—ã¦ãŠå±Šã‘ã—ã¾ã™ã€‚")

            # Second paragraph: Additional context
            if len(key_developments) > 1:
                second_dev = key_developments[1]
                second_para = f"ã¾ãŸã€{second_dev}ã«é–¢ã™ã‚‹é€²å±•ã‚‚è¦‹ã‚‰ã‚Œã€AIæŠ€è¡“ã®å¤šé¢çš„ãªç™ºå±•ãŒç¶šã„ã¦ã„ã¾ã™ã€‚"
                paragraphs.append(second_para)
            elif len(companies_mentioned) > 2:
                other_companies = list(companies_mentioned)[2:]
                if other_companies:
                    other_str = "ã€".join(other_companies[:2])
                    second_para = f"{other_str}ã‹ã‚‰ã‚‚é‡è¦ãªæŠ€è¡“ç™ºè¡¨ãŒã‚ã‚Šã€æ¥­ç•Œå…¨ä½“ã§ã®ç«¶äº‰ãŒæ¿€åŒ–ã—ã¦ã„ã¾ã™ã€‚"
                    paragraphs.append(second_para)
                else:
                    paragraphs.append("ã“ã‚Œã‚‰ã®æŠ€è¡“é©æ–°ã«ã‚ˆã‚Šã€AIåˆ†é‡ã®ç«¶äº‰ãŒã•ã‚‰ã«æ´»ç™ºåŒ–ã—ã¦ã„ã¾ã™ã€‚")
            else:
                paragraphs.append("ã“ã‚Œã‚‰ã®æŠ€è¡“é©æ–°ã«ã‚ˆã‚Šã€AIåˆ†é‡ã®ç«¶äº‰ãŒã•ã‚‰ã«æ´»ç™ºåŒ–ã—ã¦ã„ã¾ã™ã€‚")

            # Third paragraph: Closing
            update_count = sum(1 for article in articles if getattr(article, 'is_update', False))
            if update_count > 0:
                third_para = f"ä»Šå›ã¯{update_count}ä»¶ã®ç¶šå ±ã‚‚å«ã‚ã€æ€¥é€Ÿã«å¤‰åŒ–ã™ã‚‹AIæ¥­ç•Œã®æœ€æ–°å‹•å‘ã‚’ãŠä¼ãˆã—ã¾ã™ã€‚"
            else:
                third_para = "ãã‚Œã§ã¯å„ãƒˆãƒ”ãƒƒã‚¯ã®è©³ç´°ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚"
            paragraphs.append(third_para)

            return paragraphs

        except Exception as e:
            logger.warning(f"Failed to generate specific lead paragraphs: {e}")
            return []

    def _shorten_summary_points(self, summary_points: list[str]) -> list[str]:
        """Shorten summary points to fit within character limits."""

        if not summary_points:
            return []

        cleaned_points = []

        for point in summary_points:
            if not isinstance(point, str):
                continue

            # Strip whitespace
            point = point.strip()

            # Skip empty or very short points
            if len(point) < 10:
                continue

            # Remove bullet point markers if present
            point = re.sub(r'^[\-\*\â€¢ãƒ»\u2022]\s*', '', point)

            # Remove redundant prefixes
            point = re.sub(r'^(ã¾ãŸã€|ã•ã‚‰ã«ã€|ãªãŠã€|ä¸€æ–¹ã€)', '', point)

            # Ensure point ends with proper punctuation
            if not point.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                point += 'ã€‚'

            cleaned_points.append(point)

        return cleaned_points

    def _generate_japanese_title(self, article: ProcessedArticle) -> str:
        """
        Generate a Japanese title for an article using template-based rules.

        This is used as a fallback when LLM title generation fails.

        Args:
            article: ProcessedArticle containing the article data

        Returns:
            Generated Japanese title string
        """
        # Ensure regular expressions are available throughout this method
        import re

        try:
            raw_article = article.summarized_article.filtered_article.raw_article
            summary_points = article.summarized_article.summary.summary_points

            # First, try to extract from first summary point
            if summary_points and len(summary_points) > 0:
                first_point = summary_points[0]

                # Improved extraction with better logic
                # Extract company name from summary
                company_patterns = [
                    r'([A-Za-z][A-Za-z0-9]*(?:\\s+[A-Za-z][A-Za-z0-9]*)*?)(?:ç¤¾|ãŒ|ã¯|ã®).{0,20}?(ç™ºè¡¨|é–‹å§‹|æä¾›|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹)',
                    r'(OpenAI|Meta|Google|Microsoft|Anthropic|Apple|Amazon|Tesla|NVIDIA|AMD)(?:ç¤¾|ãŒ|ã¯|ã®)?',
                    r'([A-Za-z]+)(?:ç¤¾|ãŒ|ã¯|ã®).*?(AI|äººå·¥çŸ¥èƒ½|æ©Ÿæ¢°å­¦ç¿’|ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°)'
                ]

                company = None
                for pattern in company_patterns:
                    match = re.search(pattern, first_point)
                    if match:
                        company = match.group(1)
                        break

                # Extract key action/technology
                tech_action_patterns = [
                    r'(AGI|ChatGPT|Claude|Gemini|GPT-\\d+|Llama|LLaMA)(?:ã®|ã‚’|ãŒ)?(.{0,15}?)(?:ç™ºè¡¨|ãƒªãƒªãƒ¼ã‚¹|å…¬é–‹|é–‹ç™º)',
                    r'(?:AI|äººå·¥çŸ¥èƒ½|æ©Ÿæ¢°å­¦ç¿’)(?:ã®|ã‚’|ãŒ)?(.{0,15}?)(?:ç™ºè¡¨|é–‹ç™º|å°å…¥|æ´»ç”¨)',
                    r'(é‡å­|æ³•çš„|åŒ»ç™‚|è‡ªå‹•åŒ–)(?:ã®|ã‚’|ãŒ)?(.{0,10}?)(?:æŠ€è¡“|ã‚·ã‚¹ãƒ†ãƒ |ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ |ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³)',
                    r'(è‘—ä½œæ¨©|è¨´è¨Ÿ|å¥‘ç´„|æŠ•è³‡|è²·å|ææº)(?:ã®|ã‚’|ãŒ)?(.{0,10}?)(?:å•é¡Œ|åˆæ„|ç™ºè¡¨)'
                ]

                tech_action = None
                for pattern in tech_action_patterns:
                    match = re.search(pattern, first_point)
                    if match:
                        tech_action = match.group(1) + (match.group(2) if match.group(2) else "")
                        break

                # Build natural title
                if company and tech_action:
                    title_candidate = f"{company}ã®{tech_action}é–¢é€£å‹•å‘"
                elif company:
                    title_candidate = f"{company}ã®æœ€æ–°å‹•å‘"
                elif tech_action:
                    title_candidate = f"{tech_action}é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                else:
                    # Extract first meaningful phrase (up to 20 chars)
                    clean_text = re.sub(r'^[ã€‚ã€]*', '', first_point)
                    sentences = re.split(r'[ã€‚ã€]', clean_text)
                    if sentences and len(sentences[0]) > 5:
                        first_sentence = sentences[0][:20]
                        title_candidate = first_sentence + ("..." if len(sentences[0]) > 20 else "")
                    else:
                        title_candidate = "AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"

                # Clean up redundant phrases
                title_candidate = re.sub(r'ã—ã¾ã—ãŸã‚’(ç™ºè¡¨|é–‹å§‹|æä¾›)', r'ã‚’\\1', title_candidate)
                title_candidate = re.sub(r'ãŒã—ã¾ã—ãŸ', 'ãŒ', title_candidate)
                title_candidate = re.sub(r'\\s+', ' ', title_candidate).strip()

                # Ensure title doesn't end with incomplete sentences
                title_candidate = re.sub(r'[ã€‚ã€]$', '', title_candidate)
                if not re.search(r'[é–¢ãƒ‹ç™ºè¡¨é–‹å§‹å°å…¥æŠ€è¡“å‹•å‘]', title_candidate):
                    title_candidate += "é–¢é€£"

                return title_candidate

            # Extract from original title as last resort
            title = raw_article.title if raw_article.title else ""

            # Clean up English title and create Japanese equivalent
            if title:
                # Remove common prefixes and suffixes
                cleaned_title = re.sub(r'^[A-Za-z0-9\s\-\.]+:', '', title)  # Remove "Company:" prefixes
                cleaned_title = re.sub(r'https?://\S+', '', cleaned_title)  # Remove URLs
                cleaned_title = cleaned_title.strip()

                # Map common English terms to Japanese
                term_mappings = {
                    "announces": "ãŒç™ºè¡¨",
                    "releases": "ãŒãƒªãƒªãƒ¼ã‚¹",
                    "launches": "ãŒé–‹å§‹",
                    "introduces": "ãŒå°å…¥",
                    "AI": "AI",
                    "OpenAI": "OpenAI",
                    "Google": "Google",
                    "Meta": "Meta",
                    "Microsoft": "Microsoft"
                }

                # Simple mapping for common patterns
                for eng, _jp in term_mappings.items():
                    if eng.lower() in cleaned_title.lower():
                        # Extract company name before the action
                        words = cleaned_title.split()
                        if len(words) > 0:
                            company = words[0]
                            if company in ["OpenAI", "Google", "Meta", "Microsoft", "Apple", "Amazon", "Tesla", "IBM"]:
                                if "AI" in cleaned_title or "model" in cleaned_title.lower():
                                    return f"{company}ã®æ–°AIæŠ€è¡“ç™ºè¡¨"
                                else:
                                    return f"{company}ã®æœ€æ–°å‹•å‘"

                # If no mapping found, use intelligent truncation
                if len(cleaned_title) > 25:
                    return self._intelligent_truncate(cleaned_title, max_chars=25) + "é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"
                else:
                    return cleaned_title + "é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"

            return "AIæŠ€è¡“ã®æœ€æ–°ç™ºè¡¨"

        except Exception as e:
            logger.warning(f"Template title generation failed: {e}")
            return "AIæŠ€è¡“ã®æœ€æ–°ç™ºè¡¨"

    def _extract_key_topic_from_summary(self, summary_point: str) -> str | None:
        """
        Extract key topic/action from a summary point for title generation.

        Args:
            summary_point: A single summary point text

        Returns:
            Key topic string or None if not found
        """

        if not summary_point or not isinstance(summary_point, str):
            return None

        # Pattern to extract key actions/topics from Japanese summary points
        action_patterns = [
            r'(.{5,20}?ã‚’ç™ºè¡¨)',  # "XXXã‚’ç™ºè¡¨"
            r'(.{5,20}?ã‚’ãƒªãƒªãƒ¼ã‚¹)',  # "XXXã‚’ãƒªãƒªãƒ¼ã‚¹"
            r'(.{5,20}?ã‚’é–‹å§‹)',  # "XXXã‚’é–‹å§‹"
            r'(.{5,20}?ã‚’å°å…¥)',  # "XXXã‚’å°å…¥"
            r'(.{5,20}?ãŒå‘ä¸Š)',  # "XXXãŒå‘ä¸Š"
            r'(.{5,20}?ã‚’æ”¹å–„)',  # "XXXã‚’æ”¹å–„"
            r'(.{5,20}?ã«æˆåŠŸ)',  # "XXXã«æˆåŠŸ"
            r'(.{5,20}?ã‚’å®Ÿç¾)',  # "XXXã‚’å®Ÿç¾"
            r'(.{5,20}?ãŒå¯èƒ½)',  # "XXXãŒå¯èƒ½"
        ]

        for pattern in action_patterns:
            match = re.search(pattern, summary_point)
            if match:
                topic = match.group(1).strip()
                # Clean up common prefixes
                topic = re.sub(r'^(æ–°ã—ã„|æœ€æ–°ã®|æ¬¡ä¸–ä»£ã®)', '', topic)
                if len(topic) >= 5:  # Ensure meaningful length
                    return topic

        # Fallback to noun phrase extraction
        noun_patterns = [
            r'([A-Za-z0-9]{3,}[ã®]?(?:API|SDK|ã‚µãƒ¼ãƒ“ã‚¹|æ©Ÿèƒ½|æŠ€è¡“|ã‚·ã‚¹ãƒ†ãƒ |ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ))',
            r'([ã-ã‚“]{2,}[ã®]?(?:æ©Ÿèƒ½|æŠ€è¡“|ã‚µãƒ¼ãƒ“ã‚¹|ã‚·ã‚¹ãƒ†ãƒ |æ€§èƒ½|ç²¾åº¦))',
        ]

        for pattern in noun_patterns:
            match = re.search(pattern, summary_point)
            if match:
                return match.group(1).strip()

        return None

    def _sanitize_heading(self, heading: str) -> str:
        """
        Sanitize heading text for use in table of contents and section headers.

        Args:
            heading: Raw heading text

        Returns:
            Sanitized heading text
        """

        if not heading:
            return ""

        # Remove any markdown syntax
        heading = re.sub(r'[#*_`\[\]]', '', heading)

        # Remove URLs
        heading = re.sub(r'https?://\S+', '', heading)

        # Clean up extra whitespace
        heading = ' '.join(heading.split())

        # Remove trailing punctuation
        heading = re.sub(r'[.,:;!?]+$', '', heading)

        return heading.strip()

    def _normalize_terminology(self, text: str) -> str:
        """
        Normalize AI/tech terminology for consistency.

        Args:
            text: Text to normalize

        Returns:
            Normalized text with consistent terminology
        """

        if not text:
            return ""

        # Common terminology normalizations
        normalizations = {
            # AI model names
            r'\bGPT[\s-]?4[\s-]?o[\s-]?mini\b': 'GPT-4o-mini',
            r'\bGPT[\s-]?4[\s-]?o\b': 'GPT-4o',
            r'\bGPT[\s-]?3\.5\b': 'GPT-3.5',
            r'\bClaude[\s-]?3\.5\b': 'Claude 3.5',
            r'\bGemini[\s-]?Pro\b': 'Gemini Pro',

            # Company names
            r'\bOpenAI\b': 'OpenAI',
            r'\bAnthropic\b': 'Anthropic',
            r'\bGoogle\b': 'Google',
            r'\bMicrosoft\b': 'Microsoft',
            r'\bMeta\b': 'Meta',

            # Technology terms
            r'\bAI\b': 'AI',
            r'\bML\b': 'ML',
            r'\bLLM\b': 'LLM',
            r'\bAPI\b': 'API',
        }

        for pattern, replacement in normalizations.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)

        return text

    def _cleanup_summary_points(self, summary_points: list[str]) -> list[str]:
        """
        Clean up summary points by removing unwanted content.

        Args:
            summary_points: List of summary points to clean

        Returns:
            Cleaned list of summary points
        """

        if not summary_points:
            return []

        cleaned_points = []

        for point in summary_points:
            if not isinstance(point, str):
                continue

            # Strip whitespace
            point = point.strip()

            # Skip empty or very short points
            if len(point) < 10:
                continue

            # Remove bullet point markers if present
            point = re.sub(r'^[\-\*\â€¢ãƒ»\u2022]\s*', '', point)

            # Remove redundant prefixes
            point = re.sub(r'^(ã¾ãŸã€|ã•ã‚‰ã«ã€|ãªãŠã€|ä¸€æ–¹ã€)', '', point)

            # Ensure point ends with proper punctuation
            if not point.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                point += 'ã€‚'

            cleaned_points.append(point)

        return cleaned_points

    def _validate_and_fix_newsletter_content(
        self,
        content: str,
        articles: list[ProcessedArticle]
    ) -> str:
        """
        Validate and fix newsletter content for quality issues.

        Args:
            content: Generated newsletter content
            articles: Source articles for validation

        Returns:
            Fixed newsletter content
        """

        if not content:
            return ""

        # Fix common formatting issues
        content = re.sub(r'\n{3,}', '\n\n', content)  # Limit consecutive newlines
        content = re.sub(r'[ \t]+$', '', content, flags=re.MULTILINE)  # Remove trailing whitespace
        content = content.strip()

        # Ensure proper line endings
        if not content.endswith('\n'):
            content += '\n'

        # Validate that all articles are represented
        missing_articles = []
        for article in articles:
            if article.japanese_title:
                # Check if article title appears in content
                if article.japanese_title not in content:
                    missing_articles.append(article)

        if missing_articles:
            logger.warning(f"Found {len(missing_articles)} articles missing from newsletter content")

        return content

    async def _consolidate_multi_source_articles(self, articles: list[ProcessedArticle]) -> list[ProcessedArticle]:
        """Consolidate temporary multi/related article lists into proper citation data.

        The clustering stage may store helper lists (`_multi_articles_tmp`, `_related_articles_tmp`) on
        individual `ProcessedArticle` instances so that we can later attach richer citation
        information.  This helper walks through the provided articles, generates the corresponding
        `Citation` objects (using ``CitationGenerator`` when available), persists them to the
        article, and finally removes the temporary attributes to avoid unintended side-effects.

        Args:
            articles: List of `ProcessedArticle` objects coming from the workflow.

        Returns:
            The same list with citation information consolidated.
        """

        if not articles:
            return articles

        # If citation generator failed to initialise, just strip helper attributes and return.
        if not getattr(self, "citation_generator", None):
            for art in articles:
                for tmp_attr in ("_multi_articles_tmp", "_related_articles_tmp"):
                    if hasattr(art, tmp_attr):
                        delattr(art, tmp_attr)
            return articles

        # Helper to deduplicate citations by URL while preserving order
        def _dedup_citations(citations_list):
            seen = set()
            unique = []
            for cit in citations_list:
                url = getattr(cit, "url", None)
                if url and url not in seen:
                    seen.add(url)
                    unique.append(cit)
            return unique

        for article in articles:
            try:
                # Handle multi-source representative articles
                if hasattr(article, "_multi_articles_tmp") and not article.is_multi_source_enhanced:
                    cluster_articles = getattr(article, "_multi_articles_tmp", [])

                    # Generate enhanced summary from multiple sources
                    enhanced_summary = await self._generate_multi_source_summary(
                        representative_article=article,
                        cluster_articles=cluster_articles
                    )

                    # Replace the summary with multi-source enhanced version
                    if enhanced_summary:
                        article.summarized_article.summary.summary_points = enhanced_summary

                    # Generate up to 3 citations including the representative itself
                    citations = await self.citation_generator.generate_multi_source_citations(
                        representative_article=article,
                        cluster_articles=cluster_articles,
                        max_citations=3,
                    )

                    # Persist citations and metadata
                    article.citations = _dedup_citations(citations)
                    article.is_multi_source_enhanced = True

                    # Preserve list of source URLs for transparency
                    article.source_urls = [
                        art.summarized_article.filtered_article.raw_article.url for art in cluster_articles
                    ]

                    # Clean up temporary attribute
                    delattr(article, "_multi_articles_tmp")

                # Handle single articles that have a few "related" articles for extra citations
                if hasattr(article, "_related_articles_tmp"):
                    related_articles = getattr(article, "_related_articles_tmp", [])

                    # Merge with any existing citations while obeying the 3-citation limit
                    existing_citations = list(article.citations) if article.citations else []

                    additional_citations = await self.citation_generator.generate_citations(
                        article=article,
                        related_sources=[ra.summarized_article.filtered_article.raw_article for ra in related_articles],
                        max_citations=max(0, 3 - len(existing_citations)),
                    )

                    article.citations = _dedup_citations(existing_citations + additional_citations)

                    delattr(article, "_related_articles_tmp")

            except Exception as e:
                logger.warning(
                    "Failed consolidating multi-source citations",
                    error=str(e),
                )

        return articles

    async def _generate_multi_source_summary(
        self,
        representative_article: ProcessedArticle,
        cluster_articles: list[ProcessedArticle]
    ) -> list[str] | None:
        """Generate comprehensive summary from multiple sources on the same topic."""

        if not self.llm_router or not cluster_articles:
            return None

        try:
            # Collect information from all sources
            source_summaries = []
            rep_raw = representative_article.summarized_article.filtered_article.raw_article

            # Add representative article info
            source_summaries.append({
                "source": rep_raw.source_id,
                "title": rep_raw.title,
                "summary": representative_article.summarized_article.summary.summary_points,
                "content": rep_raw.content[:500] if rep_raw.content else ""
            })

            # Add cluster articles info (limit to top 2 for token management)
            for article in cluster_articles[:2]:
                raw = article.summarized_article.filtered_article.raw_article
                if raw.url != rep_raw.url:  # Avoid duplicates
                    source_summaries.append({
                        "source": raw.source_id,
                        "title": raw.title,
                        "summary": article.summarized_article.summary.summary_points,
                        "content": raw.content[:500] if raw.content else ""
                    })

            # Create comprehensive prompt
            sources_text = ""
            for i, source in enumerate(source_summaries, 1):
                sources_text += f"\nã€ã‚½ãƒ¼ã‚¹{i}: {source['source']}ã€‘\n"
                sources_text += f"ã‚¿ã‚¤ãƒˆãƒ«: {source['title']}\n"
                sources_text += f"è¦ç´„: {' / '.join(source['summary'][:2])}\n"
                if source['content']:
                    sources_text += f"å†…å®¹æŠœç²‹: {source['content'][:200]}...\n"

            prompt = f"""è¤‡æ•°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŒä¸€ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦å ±ã˜ã‚‰ã‚ŒãŸæƒ…å ±ã‚’çµ±åˆã—ã€åŒ…æ‹¬çš„ãªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

æƒ…å ±æº:{sources_text}

è¦æ±‚:
- 4ã¤ã®è¦ç´„ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆ
- å„ãƒã‚¤ãƒ³ãƒˆã¯å…·ä½“çš„ãªæ•°å€¤ãƒ»ä¼æ¥­åãƒ»æŠ€è¡“åã‚’å«ã‚€
- è¤‡æ•°ã‚½ãƒ¼ã‚¹ã®è¦–ç‚¹ã‚’ç·åˆã—ãŸåŒ…æ‹¬çš„ãªå†…å®¹
- æœ€æ–°ã®å‹•å‘ã¨å½±éŸ¿ã‚’æ˜ç¢ºã«èª¬æ˜
- å„ãƒã‚¤ãƒ³ãƒˆ40-60æ–‡å­—ç¨‹åº¦

è¦ç´„ãƒã‚¤ãƒ³ãƒˆ:"""

            response = await self.llm_router.generate_simple_text(
                prompt=prompt,
                max_tokens=400,
                temperature=0.3
            )

            if response:
                # Parse response into bullet points
                lines = response.strip().split('\n')
                summary_points = []

                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('è¦ç´„ãƒã‚¤ãƒ³ãƒˆ'):
                        # Remove bullet markers and numbering
                        line = re.sub(r'^[â€¢\-\*\d\.\)]\s*', '', line)
                        if len(line) > 10:  # Filter out very short lines
                            summary_points.append(line)

                if len(summary_points) >= 3:
                    logger.info(
                        "Generated multi-source summary",
                        representative_id=rep_raw.id,
                        sources_count=len(source_summaries),
                        points_generated=len(summary_points)
                    )
                    return summary_points[:4]  # Return max 4 points

            return None

        except Exception as e:
            logger.error(
                "Failed to generate multi-source summary",
                representative_id=representative_article.summarized_article.filtered_article.raw_article.id,
                error=str(e)
            )
            return None

    def _improve_japanese_quality(self, text: str) -> str:
        """
        Improve Japanese text quality by removing inappropriate expressions
        and enhancing naturalness (addresses quality issues found in newsletters).

        Args:
            text: Original text to improve

        Returns:
            Improved text with better quality and naturalness
        """
        if not text or not text.strip():
            return text

        improved_text = text.strip()

        # 1. Remove redundant/verbose expressions
        redundant_patterns = [
            (r'ã“ã®è¨˜äº‹ã§ã¯[ã€]?', ''),
            (r'è¨˜äº‹ã§ã¯[ã€]?', ''),
            (r'(?:è¨˜äº‹|å†…å®¹)ã«ã‚ˆã‚‹ã¨[ã€]?', ''),
            (r'è¨˜äº‹ã®è‘—è€…ã¯[ã€]?', ''),
            (r'è¨˜äº‹ã®ä¸­ã§[ã€]?', ''),
            (r'ã«ã¤ã„ã¦è¨€åŠ[ã—ã•ã‚Œ](?:ã¦(?:ã„)?ã¾ã™?)?[ã€ã€‚]?', 'ã‚’ç™ºè¡¨'),
            (r'ã«ã¤ã„ã¦è§¦ã‚Œã¦(?:ã„)?ã¾ã™?[ã€ã€‚]?', 'ã‚’èª¬æ˜'),
            (r'ã«(?:ã¤ã„ã¦|é–¢ã—ã¦)è©³ã—ã(?:è¿°ã¹|èª¬æ˜)[ã—ã•ã‚Œ](?:ã¦(?:ã„)?ã¾ã™?)?[ã€ã€‚]?', 'ã‚’è©³è¿°'),
            (r'(?:ä¸Šè¨˜|ä»¥ä¸‹)ã®[ã€]?', ''),
            (r'æä¾›ã•ã‚ŒãŸæƒ…å ±[ã«ã‚ˆã‚‹ã¨]?[ã€]?', ''),
            (r'è¨˜äº‹å†…å®¹[ã¯ã§][ã€]?', ''),
        ]

        for pattern, replacement in redundant_patterns:
            improved_text = re.sub(pattern, replacement, improved_text)

        # 2. Remove negative/uncertain expressions
        negative_patterns = [
            (r'(?:è¨˜äº‹å†…å®¹|æƒ…å ±)ã¯æ–­ç‰‡çš„ã§[ã€]?', ''),
            (r'å…·ä½“çš„ãª(?:èª¬æ˜|å†…å®¹|è©³ç´°)(?:ã¯)?(?:ã‚ã‚Šã¾ã›ã‚“|ä¸æ˜|ãªã—)[ã€ã€‚]?', ''),
            (r'è©³ç´°(?:ã¯)?(?:ä¸æ˜|æ˜ã‚‰ã‹ã§ãªã„)[ã€ã€‚]?', ''),
            (r'æƒ…å ±ãŒ(?:å°‘ãªã„|ä¸è¶³)[ã€ã€‚]?', ''),
            (r'ä¸æ˜ç¢ºãª[ã€]?', ''),
            (r'(?:ã¯ã£ãã‚Šã¨|æ˜ç¢ºã«)(?:ã¯)?(?:è¿°ã¹ã‚‰ã‚Œã¦)?ã„ã¾ã›ã‚“[ã€ã€‚]?', ''),
        ]

        for pattern, replacement in negative_patterns:
            improved_text = re.sub(pattern, replacement, improved_text)

        # 3. Remove personal name references (but keep company/product names)
        personal_name_patterns = [
            (r'kzzzmæ°?(?:ã«ã‚ˆã‚‹|ãŒ|ã¯|ã®)?[ã€]?', ''),
            (r'kzzzmã•ã‚“(?:ã«ã‚ˆã‚‹|ãŒ|ã¯|ã®)?[ã€]?', ''),
            (r'(?:è‘—è€…|ç­†è€…|åŸ·ç­†è€…)(?:ã®)?(?:kzzzm)?(?:æ°|ã•ã‚“)?(?:ã«ã‚ˆã‚‹|ãŒ|ã¯|ã®)?[ã€]?', ''),
            (r'ãƒ‹ãƒƒã‚¯ãƒ»ã‚¿ãƒ¼ãƒªãƒ¼æ°(?:ã«ã‚ˆã‚‹|ãŒ|ã¯|ã®)?[ã€]?', 'OpenAIè²¬ä»»è€…'),
            (r'ã‚¿ãƒ¼ãƒªãƒ¼æ°(?:ã«ã‚ˆã‚‹|ãŒ|ã¯|ã®)?[ã€]?', 'OpenAIå¹¹éƒ¨'),
        ]

        for pattern, replacement in personal_name_patterns:
            improved_text = re.sub(pattern, replacement, improved_text)

        # 4. Fix repetitive "ã€œã—ã¦ã„ã¾ã™" patterns and improve naturalness
        repetitive_patterns = [
            # Fix excessive "ã€œã—ã¦ã„ã¾ã™" usage with varied expressions
            (r'ã—ã¦ã„ã¾ã™([ã€ã€‚]?\s*.*?)ã—ã¦ã„ã¾ã™', r'ã—ã¦ãŠã‚Š\1ã—ã¾ã™'),
            (r'ã¦ã„ã¾ã™([ã€ã€‚]?\s*.*?)ã¦ã„ã¾ã™', r'ã¦ãŠã‚Š\1ã¾ã™'),
            (r'ã•ã‚Œã¦ã„ã¾ã™([ã€ã€‚]?\s*.*?)ã•ã‚Œã¦ã„ã¾ã™', r'ã•ã‚Œ\1ã¾ã—ãŸ'),
            (r'ã¦ãŠã‚Šã¾ã™([ã€ã€‚]?\s*.*?)ã¦ãŠã‚Šã¾ã™', r'ã¦ãŠã‚Š\1ã¾ã™'),

            # Vary verb forms for naturalness - convert stiff "ã—ã¦ã„ã¾ã™" to more varied forms
            (r'ç™ºè¡¨ã—ã¦ã„ã¾ã™', 'ç™ºè¡¨ã—ãŸ'),
            (r'é–‹ç™ºã—ã¦ã„ã¾ã™', 'é–‹ç™ºä¸­'),
            (r'æä¾›ã—ã¦ã„ã¾ã™', 'æä¾›'),
            (r'å®Ÿæ–½ã—ã¦ã„ã¾ã™', 'å®Ÿæ–½'),
            (r'å°å…¥ã—ã¦ã„ã¾ã™', 'å°å…¥'),
            (r'è¨ˆç”»ã—ã¦ã„ã¾ã™', 'è¨ˆç”»'),
            (r'æ¤œè¨ã—ã¦ã„ã¾ã™', 'æ¤œè¨ä¸­'),
            (r'é€²ã‚ã¦ã„ã¾ã™', 'æ¨é€²ä¸­'),
            (r'å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™', 'å–ã‚Šçµ„ã¿ä¸­'),
            (r'å±•é–‹ã—ã¦ã„ã¾ã™', 'å±•é–‹'),
            (r'é‹å–¶ã—ã¦ã„ã¾ã™', 'é‹å–¶'),
            (r'ç¶™ç¶šã—ã¦ã„ã¾ã™', 'ç¶™ç¶š'),

            # Replace formal passive forms with active ones
            (r'è¡Œã‚ã‚Œã¦ã„ã¾ã™', 'å®Ÿæ–½'),
            (r'é€²ã‚ã‚‰ã‚Œã¦ã„ã¾ã™', 'é€²è¡Œä¸­'),
            (r'å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™', 'å®Ÿç¾'),
            (r'å¼·åŒ–ã•ã‚Œã¦ã„ã¾ã™', 'å¼·åŒ–'),
        ]

        for pattern, replacement in repetitive_patterns:
            improved_text = re.sub(pattern, replacement, improved_text)

        # 5. Improve sentence flow and naturalness
        flow_improvements = [
            # Convert verbose reporting to direct statements
            (r'(?:ã¨|ã‚’)ç™ºè¡¨(?:ã—|ã•ã‚Œ)(?:ã¦(?:ã„)?ã¾ã™|ã¾ã—ãŸ)[ã€ã€‚]?', 'ã‚’ç™ºè¡¨'),
            (r'(?:ã¨|ã‚’)æ˜ã‚‰ã‹(?:ã«)?(?:ã—|ã•ã‚Œ)(?:ã¦(?:ã„)?ã¾ã™|ã¾ã—ãŸ)[ã€ã€‚]?', 'ã‚’æ˜ç¢ºåŒ–'),
            (r'(?:ã¨|ã‚’)èª¬æ˜(?:ã—|ã•ã‚Œ)(?:ã¦(?:ã„)?ã¾ã™|ã¾ã—ãŸ)[ã€ã€‚]?', 'ã‚’èª¬æ˜'),
            (r'(?:ã¨|ã‚’)å ±å‘Š(?:ã—|ã•ã‚Œ)(?:ã¦(?:ã„)?ã¾ã™|ã¾ã—ãŸ)[ã€ã€‚]?', 'ã‚’å ±å‘Š'),

            # Remove meta-references
            (r'è¨˜äº‹(?:ã®|ã§ã¯?)[ã€]?', ''),
            (r'(?:ã“ã®ã‚ˆã†ãª|ã“ã†ã—ãŸ)[ã€]?', ''),
            (r'åŒ(?:è¨˜äº‹|å†…å®¹)[ã€]?', ''),

            # Improve conjunctions and reduce monotony
            (r'[ã€ã€‚]\s*ã¾ãŸ[ã€]?', 'ã€'),
            (r'[ã€ã€‚]\s*ã•ã‚‰ã«[ã€]?', 'ã€‚ä¸€æ–¹ã€'),
            (r'[ã€ã€‚]\s*åŠ ãˆã¦[ã€]?', 'ã€‚ã¾ãŸã€'),
            (r'[ã€ã€‚]\s*ãªãŠ[ã€]?', 'ã€‚'),

            # Add variety to sentence patterns to reduce stiffness
            (r'(\w+)ã¯(\w+)ã‚’', r'\1ãŒ\2ã‚’'),  # Vary ã¯/ãŒ usage
            (r'ã«ã‚ˆã‚‹(\w+)', r'ã§ã®\1'),  # Vary ã«ã‚ˆã‚‹/ã§ã®
            (r'ã«ãŠã„ã¦(\w+)', r'ã§\1'),  # Simplify ã«ãŠã„ã¦
            (r'ã«é–¢ã™ã‚‹(\w+)', r'ã®\1'),  # Simplify ã«é–¢ã™ã‚‹
            (r'ã«ã¤ã„ã¦ã®(\w+)', r'ã®\1'),  # Simplify ã«ã¤ã„ã¦ã®
        ]

        for pattern, replacement in flow_improvements:
            improved_text = re.sub(pattern, replacement, improved_text)

        # 5. Clean up spacing and punctuation
        improved_text = re.sub(r'\s+', ' ', improved_text)  # Multiple spaces to single
        improved_text = re.sub(r'[ã€ã€‚]{2,}', 'ã€‚', improved_text)  # Multiple punct to single
        improved_text = re.sub(r'ã€\s*ã€‚', 'ã€‚', improved_text)  # Comma before period

        # 6. Ensure proper sentence ending
        if improved_text and not improved_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼š', 'ï¼‰', 'ã€', 'ã€')):
            if improved_text.endswith('ã§ã™') or improved_text.endswith('ã¾ã™'):
                improved_text += 'ã€‚'

        # 7. Validate minimum meaningful content
        if len(improved_text.strip()) < 10:
            logger.debug(f"Text too short after improvements: '{improved_text.strip()}'")
            return text  # Return original if too short

        logger.debug(f"Japanese quality improvement: '{text[:50]}...' -> '{improved_text[:50]}...'")
        return improved_text.strip()

    # ------------------------------------------------------------------
    # Quality filtering & deduplication helpers (re-added)
    # ------------------------------------------------------------------

    def _filter_articles_by_quality(
        self,
        articles: list[ProcessedArticle],
        quality_threshold: float = 0.35,
    ) -> list[ProcessedArticle]:
        """Filter articles whose AI é–¢é€£åº¦ (relevance score) >= threshold.

        If an article is missing `ai_relevance_score`, it is kept to avoid
        accidental loss of content during early development phases.
        The resulting list is sorted by relevance score (desc).
        """

        if not articles:
            return []

        def _score(art: ProcessedArticle) -> float:
            try:
                base_score = art.summarized_article.filtered_article.ai_relevance_score

                # Add priority boost for official sources and filter low quality
                try:
                    source_priority = getattr(art.summarized_article.filtered_article.raw_article, 'source_priority', 3)
                    if source_priority == 1:  # Official releases
                        base_score += 0.4  # Very strong boost for official sources
                    elif source_priority == 2:  # Newsletters
                        base_score += 0.2  # Strong boost for newsletters
                    elif source_priority == 4:  # Japanese/blog sources
                        # Apply penalty unless score is very high
                        if base_score < 0.7:
                            base_score *= 0.8  # Reduce score for lower-quality sources
                    # Cap at 1.0
                    base_score = min(base_score, 1.0)
                except Exception:
                    pass

                return base_score
            except Exception:
                return 0.5  # neutral default

        filtered = [a for a in articles if _score(a) >= quality_threshold]

        # Always ensure at least one article remains to avoid empty newsletter
        if not filtered:
            filtered = articles[:]

        return sorted(filtered, key=_score, reverse=True)

    def _deduplicate_articles(self, articles: list[ProcessedArticle]) -> list[ProcessedArticle]:
        """Remove articles flagged as duplicates or with identical raw IDs."""

        unique_articles = []
        seen_ids = set()
        duplicate_flagged = 0
        id_duplicates = 0

        for art in articles:
            try:
                raw_id = art.summarized_article.filtered_article.raw_article.id
            except Exception:
                raw_id = None

            # Skip if duplicate checker marked it or ID seen already
            is_dup_flag = False
            try:
                is_dup_flag = getattr(art.duplicate_check, "is_duplicate", False)
            except Exception:
                pass

            if is_dup_flag:
                duplicate_flagged += 1
                if HAS_LOGGER:
                    try:
                        title = art.summarized_article.filtered_article.raw_article.title[:50]
                        logger.debug(f"Article flagged as duplicate: {title}...")
                    except:
                        logger.debug("Article flagged as duplicate (title unavailable)")
                continue

            if raw_id and raw_id in seen_ids:
                id_duplicates += 1
                if HAS_LOGGER:
                    logger.debug(f"Article with duplicate ID: {raw_id}")
                continue

            unique_articles.append(art)
            if raw_id:
                seen_ids.add(raw_id)

        if HAS_LOGGER:
            logger.info(
                "Deduplication summary",
                total_input=len(articles),
                unique_output=len(unique_articles),
                duplicate_flagged=duplicate_flagged,
                id_duplicates=id_duplicates
            )

        return unique_articles

    # ------------------------------------------------------------------
    # Text truncation helper
    # ------------------------------------------------------------------

    def _intelligent_truncate(self, text: str, max_chars: int = 70) -> str:
        """Smartly truncate *text* within *max_chars* keeping sentence integrity."""
        return ensure_sentence_completeness(text, max_chars)

    def save_to_file(self, content: str, output_dir: str = "drafts") -> str:
        """Save newsletter content to markdown file with organized directory structure."""

        timestamp = datetime.now().strftime("%Y-%m-%d_%H%M")

        # Create organized directory structure: drafts/YYYY/MM/
        year = datetime.now().strftime("%Y")
        month = datetime.now().strftime("%m")
        organized_dir = os.path.join(output_dir, year, month)

        # Ensure directory exists
        os.makedirs(organized_dir, exist_ok=True)

        filename = f"{timestamp}_daily_newsletter.md"
        filepath = os.path.join(organized_dir, filename)

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)

            self.logger.info(f"Newsletter saved to: {filepath}")
            return filepath

        except Exception as e:
            self.logger.error(f"Failed to save newsletter: {str(e)}")
            # Fallback to flat structure if organized save fails
            fallback_path = os.path.join(output_dir, filename)
            with open(fallback_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return fallback_path


async def generate_markdown_newsletter(
    articles: list[ProcessedArticle],
    edition: str = "daily",
    processing_summary: dict = None,
    output_dir: str = "drafts",
    templates_dir: str = "src/templates"
) -> NewsletterOutput:
    """
    Convenience function to generate newsletter.

    Args:
        articles: Processed articles
        edition: Newsletter edition
        processing_summary: Processing statistics
        output_dir: Output directory
        templates_dir: Templates directory

    Returns:
        NewsletterOutput
    """

    generator = NewsletterGenerator(templates_dir)
    return await generator.generate_newsletter(
        articles, edition, processing_summary, output_dir
    )

# ---------------------------------------------------------------------------
# Module-level helper functions
# ---------------------------------------------------------------------------

def ensure_sentence_completeness(text: str, max_chars: int = None) -> str:
    """
    æ—¥æœ¬èªã«æœ€é©åŒ–ã•ã‚ŒãŸæ–‡å¢ƒç•Œæ¤œå‡ºã«ã‚ˆã‚‹è‡ªç„¶ãªåˆ‡æ–­å‡¦ç†ã€‚
    ç›®æ¬¡ã®å¯èª­æ€§ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã€é©åˆ‡ãªåˆ‡æ–­ç‚¹ã‚’å„ªå…ˆåº¦é †ã«æ¢ç´¢ã€‚
    """

    # Use TEXT_LIMITS default if max_chars not specified
    if max_chars is None:
        max_chars = TEXT_LIMITS.get('title_short', 120)

    if not text:
        return ""

    if len(text) <= max_chars:
        return text

    snippet = text[:max_chars]

    # å„ªå…ˆåº¦é †ã®åˆ‡æ–­ãƒã‚¤ãƒ³ãƒˆå®šç¾©
    cut_strategies = [
        # 1. å®Œå…¨ãªæ–‡å¢ƒç•Œï¼ˆæœ€å„ªå…ˆï¼‰
        {
            'patterns': [r'[ã€‚ï¼ï¼Ÿ]'],
            'include_match': True,
            'min_length_ratio': 0.3,
            'add_ellipsis': False,
            'description': 'æ–‡æœ«å¥èª­ç‚¹'
        },

        # 2. è‡ªç„¶ãªä¼‘æ­¢ç‚¹
        {
            'patterns': [r'[ã€ï¼Œ]'],
            'include_match': True,
            'min_length_ratio': 0.4,
            'add_ellipsis': False,
            'description': 'èª­ç‚¹'
        },

        # 3. æ‹¬å¼§ã®å¤–å´ï¼ˆæƒ…å ±ã®å®Œçµæ€§ã‚’ä¿æŒï¼‰
        {
            'patterns': [r'[ï¼‰ã€ã€]'],
            'include_match': True,
            'min_length_ratio': 0.3,
            'add_ellipsis': False,
            'description': 'æ‹¬å¼§çµ‚äº†'
        },

        # 4. æ¥ç¶šè©ã®å‰ï¼ˆè«–ç†æ§‹é€ ã‚’ä¿æŒï¼‰
        {
            'patterns': [r'(?=ã¾ãŸ|ã•ã‚‰ã«|ä¸€æ–¹|ãªãŠ|ãŸã ã—|ã—ã‹ã—|ãã—ã¦)'],
            'include_match': False,
            'min_length_ratio': 0.4,
            'add_ellipsis': True,
            'description': 'æ¥ç¶šè©å‰'
        },

        # 5. åŠ©è©ã®å¾Œï¼ˆæœ€å¾Œã®æ‰‹æ®µã€ä½†ã—ä¸å®Œå…¨æ„Ÿã‚’é¿ã‘ã‚‹ï¼‰
        {
            'patterns': [r'(?<=[ã®ã§ã‚‚ã‚„])(?!\s*[ã¯ãŒã‚’ã«])'],  # ã€Œã¯ã€ã€ŒãŒã€ç­‰ã®ç›´å‰ã¯é¿ã‘ã‚‹
            'include_match': False,
            'min_length_ratio': 0.5,
            'add_ellipsis': True,
            'description': 'åŠ©è©å¾Œï¼ˆå®‰å…¨ãªä½ç½®ï¼‰'
        }
    ]

    min_acceptable_length = max(10, int(max_chars * 0.3))

    # å„æˆ¦ç•¥ã‚’è©¦è¡Œ
    for strategy in cut_strategies:
        for pattern in strategy['patterns']:
            matches = list(re.finditer(pattern, snippet))
            if matches:
                # æœ€å¾Œã®ãƒãƒƒãƒã‚’ä½¿ç”¨
                last_match = matches[-1]
                cut_pos = last_match.end() if strategy['include_match'] else last_match.start()

                # æœ€å°é•·è¦ä»¶ãƒã‚§ãƒƒã‚¯
                if cut_pos >= min_acceptable_length:
                    result = snippet[:cut_pos].rstrip()

                    # ä¸å®Œå…¨ãªåŠ©è©ã§çµ‚ã‚ã‚‹å ´åˆã¯ä¿®æ­£
                    if result.endswith(('ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã§', 'ã¨', 'ã‹ã‚‰')):
                        result = result[:-1].rstrip()

                    # æ®‹ã‚Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç¢ºèª
                    remaining = text[cut_pos:].strip()
                    needs_ellipsis = (
                        strategy['add_ellipsis'] and
                        remaining and
                        len(remaining) > 8 and
                        not result.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ'))
                    )

                    final_result = result + ('â€¦' if needs_ellipsis else '')

                    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆç›®æ¬¡ã®å“è³ªç¢ºèªç”¨ï¼‰
                    logger.debug(
                        f"TOC truncation: '{strategy['description']}' at {cut_pos}/{max_chars} chars"
                    )

                    return final_result

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå®‰å…¨ãªä½ç½®ã§ã®åˆ‡æ–­
    # å˜èªå¢ƒç•Œã‚’æ¢ã™ï¼ˆæ—¥æœ¬èªã§ã¯ç©ºç™½ã‚„å¥èª­ç‚¹ï¼‰
    safe_positions = []
    for i in range(len(snippet) - 1, min_acceptable_length, -1):
        char = snippet[i]
        if char in 'ã€€ ã€ï¼Œã€‚ï¼ï¼Ÿï¼‰ã€ã€':
            safe_positions.append(i + (1 if char in 'ã€ï¼Œã€‚ï¼ï¼Ÿï¼‰ã€ã€' else 0))
        elif i > 0 and snippet[i-1] in 'ã®ã§ã‚‚ã‚„' and char not in 'ã¯ãŒã‚’ã«':
            safe_positions.append(i)

    if safe_positions:
        cut_pos = safe_positions[0]  # æœ€ã‚‚å¾Œã‚ã®å®‰å…¨ãªä½ç½®
        result = snippet[:cut_pos].rstrip()

        # æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
        if not result.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', 'ï¼‰', 'ã€', 'ã€')):
            remaining = text[cut_pos:].strip()
            if remaining and len(remaining) > 8:
                result += 'â€¦'

        return result

    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå˜ç´”åˆ‡æ–­
    fallback_pos = max_chars - 3
    while fallback_pos > min_acceptable_length and snippet[fallback_pos] in 'ã¯ãŒã‚’ã«':
        fallback_pos -= 1

    return snippet[:fallback_pos].rstrip() + 'â€¦'

