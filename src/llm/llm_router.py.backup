"""
LLM Router for handling multiple LLM providers with fallback support.
"""
import asyncio
import time
import logging
import re
from typing import Optional, Dict, Any, List
import os

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain_core.language_models import BaseChatModel
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser

from src.models.schemas import SummaryOutput
from src.constants.settings import LLM_CONFIG

# Configure logging
logger = logging.getLogger(__name__)

class LLMRouter:
    """Router for managing multiple LLM providers with fallback support."""
    
    def __init__(self):
        """Initialize the LLM router with client configurations."""
        self.primary_models = ["gemini-2.5-flash"]
        self.fallback_models = ["claude-3-7-sonnet-20250219", "gpt-4o-mini"]
        self.retry_delay = 2.0
        self.timeout = 60
        
        # Initialize clients lazily
        self._clients: Dict[str, BaseChatModel] = {}
        
        logger.info("LLM Router initialized with primary models: %s, fallback models: %s", 
                   self.primary_models, self.fallback_models)
    
    def _create_gemini_client(self) -> ChatGoogleGenerativeAI:
        """Create Gemini client with optimized settings."""
        api_key = os.getenv("GEMINI_API_KEY")
        
        # Debug logging for API key
        if not api_key:
            logger.error("GEMINI_API_KEY environment variable is not set")
            raise ValueError("GEMINI_API_KEY environment variable is required")
        
        # Validate API key format
        if not api_key.startswith("AIzaSy") or len(api_key) < 30:
            logger.error("GEMINI_API_KEY appears to be invalid (wrong format or too short)")
            logger.debug("API key format: starts_with_AIzaSy=%s, length=%d", 
                        api_key.startswith("AIzaSy"), len(api_key))
            raise ValueError("GEMINI_API_KEY appears to be invalid")
        
        logger.info("Creating Gemini client with API key: %s...", api_key[:12])
        
        return ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",  # ã‚³ã‚¹ãƒˆé‡è¦–ã® 2.5 Flash ã«å¤‰æ›´
            google_api_key=api_key,
            temperature=0.1,  # Reduced for consistency
            max_tokens=2048,
            timeout=self.timeout,
            max_retries=2
            # Note: removed response_format as it's not supported by LangChain ChatGoogleGenerativeAI
        )
    
    def _create_claude_client(self) -> ChatAnthropic:
        """Create Claude client."""
        api_key = os.getenv("ANTHROPIC_API_KEY")
        
        if not api_key:
            logger.error("ANTHROPIC_API_KEY environment variable is not set")
            raise ValueError("ANTHROPIC_API_KEY environment variable is required")
        
        # Validate API key format
        if not api_key.startswith("sk-ant-") or len(api_key) < 50:
            logger.error("ANTHROPIC_API_KEY appears to be invalid (wrong format or too short)")
            logger.debug("API key format: starts_with_sk-ant=%s, length=%d", 
                        api_key.startswith("sk-ant-"), len(api_key))
            raise ValueError("ANTHROPIC_API_KEY appears to be invalid")
        
        logger.info("Creating Claude client with API key: %s...", api_key[:20])
        
        return ChatAnthropic(
            model="claude-3-7-sonnet-20250219",
            anthropic_api_key=api_key,
            temperature=0.3,
            max_tokens=2048,
            timeout=self.timeout
        )
    
    def _create_openai_client(self) -> ChatOpenAI:
        """Create OpenAI client."""
        api_key = os.getenv("OPENAI_API_KEY")
        
        if not api_key:
            logger.error("OPENAI_API_KEY environment variable is not set")
            raise ValueError("OPENAI_API_KEY environment variable is required")
        
        # Validate API key format
        if not api_key.startswith("sk-proj-") or len(api_key) < 50:
            logger.error("OPENAI_API_KEY appears to be invalid (wrong format or too short)")
            logger.debug("API key format: starts_with_sk-proj=%s, length=%d", 
                        api_key.startswith("sk-proj-"), len(api_key))
            raise ValueError("OPENAI_API_KEY appears to be invalid")
        
        logger.info("Creating OpenAI client with API key: %s...", api_key[:20])
        
        return ChatOpenAI(
            model="gpt-4o-mini",
            openai_api_key=api_key,
            temperature=0.3,
            max_tokens=2048,
            timeout=self.timeout
        )
    
    def _get_client(self, model_name: str) -> BaseChatModel:
        """Get or create a client for the specified model."""
        if model_name not in self._clients:
            if "gemini" in model_name:
                self._clients[model_name] = self._create_gemini_client()
            elif "claude" in model_name:
                self._clients[model_name] = self._create_claude_client()
            elif "gpt" in model_name:
                self._clients[model_name] = self._create_openai_client()
            else:
                raise ValueError(f"Unsupported model: {model_name}")
        
        return self._clients[model_name]
    
    async def generate_summary(
        self,
        article_title: str,
        article_content: str,
        article_url: str,
        source_name: str,
        max_retries: Optional[int] = None
    ) -> SummaryOutput:
        """
        Generate summary using primary model with fallback support.
        
        Args:
            article_title: Title of the article
            article_content: Content of the article
            article_url: URL of the article
            source_name: Name of the source
            max_retries: Maximum retries for primary model (default: 3)
        
        Returns:
            SummaryOutput: Generated summary with metadata
        """
        if max_retries is None:
            max_retries = 3
        
        # Prepare prompts
        system_prompt = self._get_summary_system_prompt()
        user_prompt = self._get_summary_user_prompt(
            article_title, article_content, article_url, source_name
        )
        
        last_error: Optional[Exception] = None
        primary_model = self.primary_models[0]
        fallback_models = self.fallback_models
        
        logger.info(
            "Attempting summary generation with model: %s, article: %s",
            primary_model,
            article_title[:50] + "..."
        )
        
        # Try primary model with retries
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                
                # Get client
                client = self._get_client(primary_model)
                
                prompt = ChatPromptTemplate.from_messages([
                    ("system", system_prompt),
                    ("human", user_prompt),
                ])
                
                formatted_prompt = prompt.format_messages()
                
                response = await self._ainvoke_json(client, formatted_prompt, primary_model)
                
                # Log raw response for debugging
                logger.debug("RAW_RESP (model=%s): %s", primary_model, response.content[:500] if hasattr(response, "content") else str(response)[:500])
                logger.error("FULL RAW RESPONSE: %s", repr(response.content))
                
                # Parse structured output with robust JSON extraction
                content = response.content.strip()
                
                # Try to extract JSON from various formats
                import json
                json_content = None
                
                # Method 1: Direct JSON parsing
                try:
                    json_content = json.loads(content)
                except:
                    pass
                
                # Method 2: Extract from markdown code blocks
                if json_content is None:
                    json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
                    if json_match:
                        try:
                            json_content = json.loads(json_match.group(1))
                        except:
                            pass
                
                # Method 3: Find JSON object in text (improved for multiline)
                if json_content is None:
                    # Look for complete JSON objects with summary_points, handling newlines
                    json_match = re.search(r'\{[^{}]*"summary_points"[^{}]*\[[^\]]*\][^{}]*\}', content, re.DOTALL)
                    if json_match:
                        try:
                            json_content = json.loads(json_match.group(0))
                        except:
                            pass
                
                # Method 4: Clean and try again
                if json_content is None:
                    cleaned_content = content
                    for prefix in ["```json", "```"]:
                        if cleaned_content.startswith(prefix):
                            cleaned_content = cleaned_content[len(prefix):]
                    for suffix in ["```"]:
                        if cleaned_content.endswith(suffix):
                            cleaned_content = cleaned_content[:-len(suffix)]
                    cleaned_content = cleaned_content.strip()
                    try:
                        json_content = json.loads(cleaned_content)
                    except:
                        pass
                
                # Method 5 â€“ Extract bullet list and convert to JSON
                if json_content is None:
                    bullet_lines = re.findall(r'^[\-*â€¢ãƒ»\u2022]\s*(.+)', content, re.MULTILINE)
                    if len(bullet_lines) >= 3:
                        json_content = {
                            "summary_points": bullet_lines[:4]
                        }
                
                # NEW: Method 6 â€“ Extract from pure text and convert to bullet points
                if json_content is None:
                    logger.debug("Attempting Method 6: Pure text to bullet conversion")
                    
                    # Split text into sentences
                    sentences = re.split(r'[ã€‚.!?]', content)
                    # Filter out short sentences and clean them
                    meaningful_sentences = []
                    for sentence in sentences:
                        sentence = sentence.strip()
                        # Skip very short sentences, meta comments, or instruction text
                        if (len(sentence) >= 30 and 
                            not any(word in sentence.lower() for word in [
                                "ç”³ã—è¨³", "ã™ã¿ã¾ã›ã‚“", "sorry", "i apologize", "i cannot",
                                "ä»¥ä¸‹ã«", "è¦ç´„ã—ã¾ã™", "ã¾ã¨ã‚ã‚‹ã¨", "ã«ã¤ã„ã¦èª¬æ˜", "json", "format"
                            ])):
                            meaningful_sentences.append(sentence)
                    
                    # Take first 3-4 meaningful sentences as bullet points
                    if len(meaningful_sentences) >= 3:
                        json_content = {
                            "summary_points": meaningful_sentences[:4]
                        }
                        logger.debug("Method 6 successful: Created %d bullet points from text", 
                                   len(json_content["summary_points"]))
                
                # If we have JSON content, validate it with Pydantic
                if json_content:
                    try:
                        summary = SummaryOutput(**json_content)
                        # Early success return - skip fallbacks (Lawrence's requirement)
                        summary.model_used = primary_model
                        processing_time = time.time() - start_time
                        estimated_cost = self._estimate_cost(primary_model, user_prompt, response.content)
                        
                        logger.info(
                            "Summary generation successful (early return) - model: %s, attempt: %d, time: %.2fs, cost: $%.4f",
                            primary_model, attempt + 1, processing_time, estimated_cost
                        )
                        return summary
                        
                    except Exception as e:
                        logger.warning(f"Pydantic validation failed: {e}, using direct text parsing")
                        logger.debug(f"JSON content was: {json_content}")
                        # Direct fallback to text parsing instead of problematic parser.parse()
                        summary = self._parse_text_to_summary(content, model_name=primary_model)
                else:
                    # Direct text parsing instead of parser.parse() which causes KeyError
                    logger.debug("No JSON found, parsing as text directly")
                    logger.debug(f"Raw content from LLM: {content[:200]}...")
                    summary = self._parse_text_to_summary(content, model_name=primary_model)
                    
                # Validate summary
                self._validate_summary(summary)
                
                # Add metadata
                summary.model_used = primary_model
                
                # Calculate processing time
                processing_time = time.time() - start_time
                
                # Estimate cost
                estimated_cost = self._estimate_cost(
                primary_model, user_prompt, response.content
                )
                
                logger.info(
                "Summary generation successful - model: %s, attempt: %d, time: %.2fs, cost: $%.4f, confidence: %.2f",
                primary_model,
                attempt + 1,
                processing_time,
                estimated_cost,
                summary.confidence_score
                )
                
                return summary
                
            except Exception as e:
                last_error = e
                processing_time = time.time() - start_time
                
                # Log detailed error information
                logger.warning(
                    "Summary generation attempt failed - model: %s, attempt: %d, error: %s (%s), time: %.2fs",
                    primary_model,
                    attempt + 1,
                    str(e),
                    type(e).__name__,
                    processing_time
                )
                logger.error("Full exception details:", exc_info=True)
                
                # Log response content for debugging if available
                if 'response' in locals() and hasattr(response, 'content'):
                    logger.debug(
                        "Response content: %s",
                        response.content[:500] if response.content else "None"
                    )
                    
                    # Wait before retry
                    if attempt < max_retries - 1:
                        await asyncio.sleep(self.retry_delay * (attempt + 1))
        
        # Primary model failed, try fallback models (1 attempt each)
        for fallback_model in fallback_models:
            logger.info(
                "Attempting fallback summary generation - model: %s, article: %s",
                fallback_model,
                article_title[:50] + "..."
            )
            
            try:
                start_time = time.time()
                
                # Get client
                client = self._get_client(fallback_model)
                
                prompt = ChatPromptTemplate.from_messages([
                    ("system", self._get_summary_system_prompt()),
                    ("human", self._get_summary_user_prompt(
                        article_title, article_content, article_url, source_name
                    )),
                ])
                
                formatted_prompt = prompt.format_messages()
                
                response = await self._ainvoke_json(client, formatted_prompt, fallback_model)
                
                # Log raw response for debugging
                logger.debug("RAW_RESP (model=%s): %s", fallback_model, response.content[:500] if hasattr(response, "content") else str(response)[:500])
                
                # Parse structured output with enhanced JSON extraction
                content = response.content.strip()
                
                # Try to extract JSON from various formats (same as primary model)
                import json
                json_content = None
                
                # Method 1: Direct JSON parsing
                try:
                    json_content = json.loads(content)
                except:
                    pass
                
                # Method 2: Extract from markdown code blocks
                if json_content is None:
                    json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
                    if json_match:
                        try:
                            json_content = json.loads(json_match.group(1))
                        except:
                            pass
                
                # Method 3: Find JSON object in text (improved for multiline)
                if json_content is None:
                    # Look for complete JSON objects with summary_points, handling newlines
                    json_match = re.search(r'\{[^{}]*"summary_points"[^{}]*\[[^\]]*\][^{}]*\}', content, re.DOTALL)
                    if json_match:
                        try:
                            json_content = json.loads(json_match.group(0))
                        except:
                            pass
                
                # Method 4: Clean and try again
                if json_content is None:
                    cleaned_content = content
                    for prefix in ["```json", "```"]:
                        if cleaned_content.startswith(prefix):
                            cleaned_content = cleaned_content[len(prefix):]
                    for suffix in ["```"]:
                        if cleaned_content.endswith(suffix):
                            cleaned_content = cleaned_content[:-len(suffix)]
                    cleaned_content = cleaned_content.strip()
                    try:
                        json_content = json.loads(cleaned_content)
                    except:
                        pass
                
                # Method 5: Extract bullet list and convert to JSON
                if json_content is None:
                    bullet_lines = re.findall(r'^[\-*â€¢ãƒ»\u2022]\s*(.+)', content, re.MULTILINE)
                    if len(bullet_lines) >= 3:
                        json_content = {
                            "summary_points": bullet_lines[:4]
                        }
                
                # Method 6: Extract from pure text and convert to bullet points
                if json_content is None:
                    logger.debug("Fallback Method 6: Pure text to bullet conversion")
                    sentences = re.split(r'[ã€‚.!?]', content)
                    meaningful_sentences = []
                    for sentence in sentences:
                        sentence = sentence.strip()
                        if (len(sentence) >= 30 and 
                            not any(word in sentence.lower() for word in [
                                "ç”³ã—è¨³", "ã™ã¿ã¾ã›ã‚“", "sorry", "i apologize", "i cannot",
                                "ä»¥ä¸‹ã«", "è¦ç´„ã—ã¾ã™", "ã¾ã¨ã‚ã‚‹ã¨", "ã«ã¤ã„ã¦èª¬æ˜", "json", "format"
                            ])):
                            meaningful_sentences.append(sentence)
                    
                    if len(meaningful_sentences) >= 3:
                        json_content = {
                            "summary_points": meaningful_sentences[:4]
                        }
                        logger.debug("Fallback Method 6 successful: Created %d bullet points from text", 
                                   len(json_content["summary_points"]))
                
                # If we have JSON content, validate it with Pydantic
                if json_content:
                    try:
                        summary = SummaryOutput(**json_content)
                    except Exception as e:
                        logger.warning(f"Fallback Pydantic validation failed: {e}, using direct text parsing")
                        # Direct fallback to text parsing instead of problematic parser.parse()
                        summary = self._parse_text_to_summary(content)
                else:
                    # Direct text parsing instead of parser.parse() which causes KeyError
                    logger.debug("Fallback: No JSON found, parsing as text directly")
                    summary = self._parse_text_to_summary(content)
                
                # Validate summary
                self._validate_summary(summary)
                
                # Add metadata
                summary.model_used = fallback_model
                
                # Calculate processing time
                processing_time = time.time() - start_time
                
                # Estimate cost
                estimated_cost = self._estimate_cost(
                    fallback_model, self._get_summary_user_prompt(
                        article_title, article_content, article_url, source_name
                    ), response.content
                )
                
                logger.info(
                    "Fallback summary generation successful - model: %s, time: %.2fs, cost: $%.4f, confidence: %.2f",
                    fallback_model,
                    processing_time,
                    estimated_cost,
                    summary.confidence_score
                )
                
                return summary
                
            except Exception as e:
                last_error = e
                processing_time = time.time() - start_time
                
                logger.warning(
                    "Fallback summary generation failed - model: %s, error: %s, time: %.2fs",
                    fallback_model,
                    str(e),
                    processing_time
                )
        
        # All models failed, create fallback summary
        logger.error(
            "All LLM models failed, creating fallback summary - last_error: %s, article: %s",
            str(last_error),
            article_title
        )
        
        return self._create_fallback_summary(
            article_title, article_content, str(last_error)
        )
    
    def _get_summary_system_prompt(self) -> str:
        """Get system prompt for summary generation."""
        
        return """ã‚ãªãŸã¯é«˜å“è³ªãªæ—¥æœ¬èªAIãƒ‹ãƒ¥ãƒ¼ã‚¹å°‚é–€è¦ç´„è€…ã§ã™ã€‚AIé–¢é€£è¨˜äº‹ã‚’è©³ç´°ã§æƒ…å ±è±Šå¯Œãªè¦ç´„ã¨ã—ã¦ä½œæˆã—ã¦ãã ã•ã„ã€‚
# Assistant must respond in valid JSON according to the specified schema.

ã€å¿…é ˆè¦ä»¶ã€‘
1. å¿…ãš3-4å€‹ã®ç®‡æ¡æ›¸ããƒã‚¤ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§ç”Ÿæˆ
2. å„ãƒã‚¤ãƒ³ãƒˆã¯60-150æ–‡å­—ï¼ˆå¾“æ¥ã‚ˆã‚Šè©³ç´°ã§æƒ…å ±é‡è±Šå¯Œã«ï¼‰
3. å…·ä½“çš„ãªä¼æ¥­åã€è£½å“åã€æ•°å€¤ã€èƒŒæ™¯æƒ…å ±ã‚’å«ã‚ã‚‹
4. æ˜ç¢ºã§å°‚é–€çš„ãªæ—¥æœ¬èªï¼ˆæ•¬ä½“ï¼šã§ã™ãƒ»ã¾ã™èª¿ã§çµ±ä¸€ï¼‰
5. ãƒ¡ã‚¿ã‚³ãƒ¡ãƒ³ãƒˆã‚„æŒ‡ç¤ºèªã¯çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢
6. å®¢è¦³çš„ã§äº‹å®Ÿã«åŸºã¥ããªãŒã‚‰ã‚‚èª­ã¿ã‚„ã™ã
7. æŠ€è¡“çš„è©³ç´°ã¨æ„ç¾©ã®ä¸¡æ–¹ã‚’å«ã‚ã‚‹

ã€ã‚¹ã‚¿ã‚¤ãƒ«ä¾‹ã€‘ï¼ˆé«˜å“è³ªãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼å‚è€ƒï¼‰
- ã€ŒOpenAIã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«o3-proãŒChatGPTã®Pro/Teamãƒ¦ãƒ¼ã‚¶ãƒ¼ãŠã‚ˆã³APIã§åˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã¯æ—¢å ±ã®æ­£å¼ãƒªãƒªãƒ¼ã‚¹ã«ç¶šãã‚‚ã®ã§ã™ã€‚ã€
- ã€Œæ—¢å­˜ã®o3ãƒ¢ãƒ‡ãƒ«ã¯APIä¾¡æ ¼ãŒ80%å¤§å¹…å€¤ä¸‹ã’ã•ã‚ŒãŸã“ã¨ã«åŠ ãˆã€ChatGPT Plusãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆ©ç”¨æ ã‚‚é€±100ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰200ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¸ã¨å€å¢—ã•ã‚Œã¾ã—ãŸã€‚ã€

ã€é‡è¦ã€‘å¿…ãšä»¥ä¸‹ã®æ­£ç¢ºãªJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®å½¢å¼ã‚„è¿½åŠ ã®èª¬æ˜ã¯ä¸€åˆ‡ä¸è¦ã§ã™ï¼š

ã€å…·ä½“çš„ãªå‡ºåŠ›ä¾‹ã€‘
{{
  "summary_points": [
    "OpenAIç¤¾ãŒGPT-5ã‚’æ­£å¼ç™ºè¡¨ã—ã€æ¨è«–èƒ½åŠ›ãŒå‰ä¸–ä»£æ¯”50%å‘ä¸Šã—ãŸã¨ç™ºè¡¨ã—ã¾ã—ãŸ",
    "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ©Ÿèƒ½ãŒå¼·åŒ–ã•ã‚Œã€å‹•ç”»ç†è§£ãƒ»ç”ŸæˆãŒå¯èƒ½ã«ãªã‚Šã¾ã™", 
    "ä¼æ¥­å‘ã‘APIã¯2025å¹´Q3ã‹ã‚‰æ®µéšçš„æä¾›é–‹å§‹äºˆå®šã§ã™",
    "ç ”ç©¶æ©Ÿé–¢ã¨ã®å”åŠ›ã«ã‚ˆã‚ŠåŒ»ç™‚ãƒ»ç§‘å­¦åˆ†é‡ã§ã®æ´»ç”¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™"
  ],
  "confidence_score": 0.9,
  "source_reliability": "high"
}}

ä¸Šè¨˜ã¨åŒã˜å½¢å¼ã§ã€è¨˜äº‹å†…å®¹ã«åŸºã¥ã„ãŸå…·ä½“çš„ãªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""
    
    def _get_summary_user_prompt(
        self,
        title: str,
        content: str,
        url: str,
        source: str
    ) -> str:
        """Generate user prompt for summary."""
        return f"""è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {title}
        
è¨˜äº‹å†…å®¹:
{content[:2000]}

ã‚½ãƒ¼ã‚¹: {source}
URL: {url}

ä¸Šè¨˜ã®è¨˜äº‹ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚"""

    # ------------------------------------------------------------------
    # Internal helper: async invoke that returns raw response
    # ------------------------------------------------------------------
    async def _ainvoke_json(self, client: BaseChatModel, messages, model_name: str):
        """Wrapper around `client.ainvoke` to standardise error handling.

        Some LangChain chat model clients return a ChatMessage; others return
        a wrapper object. We keep the result as-is so that callers can access
        `.content` safely. Any networking or API error will be propagated to
        the caller and handled by the retry logic upstream.
        """
        try:
            response = await client.ainvoke(messages)
            return response
        except Exception as e:  # noqa: BLE001
            logger.warning("ainvoke failed (model=%s): %s", model_name, e)
            raise

    # ------------------------------------------------------------------
    # Fallback summary generator (all LLM calls exhausted)
    # ------------------------------------------------------------------
    def _create_fallback_summary(self, article_title: str, article_content: str, last_error: str) -> SummaryOutput:
        """Create a minimal but valid SummaryOutput when all LLMs fail.

        Parameters
        ----------
        article_title : str
            Title of the article (used as first bullet point)
        article_content : str
            Raw article text â€“ first 100 characters will be truncated.
        """
        truncated = (article_content[:100] + "...") if article_content else "å†…å®¹å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
        bullets = [
            f"ã€é€Ÿå ±ã€‘{article_title}",
            truncated,
            f"â€»è‡ªå‹•è¦ç´„ã«å¤±æ•—ã—ãŸãŸã‚ã€å…ƒè¨˜äº‹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼: {last_error}"
        ]
        return SummaryOutput(
            summary_points=bullets,
            confidence_score=0.0,
            source_reliability="low"
        )
    
    def _validate_summary(self, summary: SummaryOutput) -> None:
        """Lightweight sanity check for SummaryOutput produced via text parsing.

        Raises
        ------
        ValueError
            If the summary does not satisfy basic quality gates.
        """
        if not summary or not summary.summary_points:
            raise ValueError("Summary has no points")
        if not (1 <= len(summary.summary_points) <= 6):
            raise ValueError("Unexpected number of summary points")

        # Ensure each summary point is substantial (>=30 æ–‡å­—ï¼characters)
        for pt in summary.summary_points:
            if len(pt.strip()) < 30:
                raise ValueError("Summary point too short (<30 chars)")

    def _estimate_cost(self, model_name: str, prompt: str, response_content: str) -> float:
        """Very coarse cost estimation based on token count heuristics.

        Note: This is *not* an accurate billing estimator â€“ it is only used for
        loggingï¼monitoring. We approximate tokens â‰ˆ characters / 4 and apply
        publicly-known $/1Kâ€token rates as of 2025-06 for major models. If the
        model is unknown, we return 0.0 to avoid crashing.
        """
        # Fallback safe-guard
        if not prompt or not response_content:
            return 0.0

        # Rough token count (characters / 4)
        prompt_tokens = max(1, len(prompt) // 4)
        response_tokens = max(1, len(response_content) // 4)
        total_tokens = prompt_tokens + response_tokens

        # USD per 1K tokens (approx.)
        rate_lookup = {
            "gemini-2.5-flash": 0.0005,   # hypothetical
            "claude-3-7-sonnet-20250219": 0.0008,
            "gpt-4o-mini": 0.0006,
        }
        rate = rate_lookup.get(model_name, 0.0)
        return round(total_tokens / 1000 * rate, 4)

    def _parse_text_to_summary(self, content: str, model_name: str = "unknown") -> SummaryOutput:
        """Parse LLM text response to SummaryOutput when JSON parsing fails.
        
        This method extracts summary points from free-form text responses.
        """
        if not content or not content.strip():
            raise ValueError("Empty content provided for text parsing")
        
        lines = [line.strip() for line in content.split('\n') if line.strip()]
        summary_points = []
        
        # Extract bullet points or numbered items
        for line in lines:
            # Skip common headers/footers
            if any(skip in line.lower() for skip in ['è¦ç´„', 'summary', 'ä»¥ä¸‹', 'ä¸Šè¨˜']):
                continue
                
            # Look for bullet points or numbered items
            if re.match(r'^[-*â€¢]\s*', line) or re.match(r'^\d+\.\s*', line):
                # Remove bullet/number prefix
                clean_line = re.sub(r'^[-*â€¢]\s*', '', line)
                clean_line = re.sub(r'^\d+\.\s*', '', clean_line)
                if len(clean_line) > 15:  # Substantial content
                    summary_points.append(clean_line)
            elif len(line) > 30 and 'ã€‚' in line:
                # Treat longer sentences as summary points
                summary_points.append(line)
        
        # If no structured points found, split by sentences
        if not summary_points:
            sentences = re.split(r'[ã€‚.!?]', content)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 20:
                    summary_points.append(sentence + 'ã€‚')
        
        # Ensure we have at least some content
        if not summary_points:
            summary_points = [f"è¨˜äº‹ã®è¦ç´„ï¼š{content[:100]}..." if len(content) > 100 else content]
        
        # Limit to reasonable number of points
        summary_points = summary_points[:5]
        
        return SummaryOutput(
            summary_points=summary_points,
            confidence_score=0.3,  # Lower confidence for text parsing
            source_reliability="medium",
            model_used=model_name
        )

    async def generate_simple_text(
        self,
        prompt: str,
        max_tokens: int = 250,  # Reduced from 300 for faster processing (Lawrence's requirement)
        temperature: float = 0.2,
        max_retries: Optional[int] = None
    ) -> Optional[str]:
        """
        Generate simple text using the primary model with fallback support.
        
        Args:
            prompt: The text prompt
            max_tokens: Maximum tokens in response
            temperature: Sampling temperature
            max_retries: Maximum retries for primary model (default: 2)
        
        Returns:
            Generated text or None if all models fail
        """
        if max_retries is None:
            max_retries = 2
        
        last_error: Optional[Exception] = None
        primary_model = self.primary_models[0]
        fallback_models = self.fallback_models
        
        logger.info(
            "Attempting simple text generation with model: %s",
            primary_model
        )
        
        # Try primary model with retries
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                
                # Get client
                client = self._get_client(primary_model)
                
                prompt_template = ChatPromptTemplate.from_messages([
                    ("human", prompt),
                ])
                
                formatted_prompt = prompt_template.format_messages()
                
                response = await self._ainvoke_json(client, formatted_prompt, primary_model)
                
                processing_time = time.time() - start_time
                
                logger.info(
                    "Simple text generation successful - model: %s, attempt: %d, time: %.2fs",
                    primary_model,
                    attempt + 1,
                    processing_time
                )
                
                return response.content.strip() if hasattr(response, 'content') else str(response).strip()
                
            except Exception as e:
                last_error = e
                processing_time = time.time() - start_time
                
                logger.warning(
                    "Simple text generation attempt failed - model: %s, attempt: %d, error: %s, time: %.2fs",
                    primary_model,
                    attempt + 1,
                    str(e),
                    processing_time
                )
                
                if attempt < max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (attempt + 1))
        
        # Try fallback models (1 attempt each)
        for fallback_model in fallback_models:
            logger.info(
                "Attempting fallback simple text generation - model: %s",
                fallback_model
            )
            
            try:
                start_time = time.time()
                
                client = self._get_client(fallback_model)
                
                prompt_template = ChatPromptTemplate.from_messages([
                    ("human", prompt),
                ])
                
                formatted_prompt = prompt_template.format_messages()
                
                response = await self._ainvoke_json(client, formatted_prompt, fallback_model)
                
                processing_time = time.time() - start_time
                
                logger.info(
                    "Fallback simple text generation successful - model: %s, time: %.2fs",
                    fallback_model,
                    processing_time
                )
                
                return response.content.strip() if hasattr(response, 'content') else str(response).strip()
                
            except Exception as e:
                last_error = e
                processing_time = time.time() - start_time
                
                logger.warning(
                    "Fallback simple text generation failed - model: %s, error: %s, time: %.2fs",
                    fallback_model,
                    str(e),
                    processing_time
                )
        
        # All models failed
        logger.error(
            "All LLM models failed for simple text generation - last_error: %s",
            str(last_error)
        )
        
        return None
    
    async def generate_japanese_title(
        self,
        article: Any,
        max_tokens: int = 100,
        temperature: float = 0.3
    ) -> Optional[str]:
        """
        Generate a Japanese title for an article using LLM.
    
    Args:
            article: ProcessedArticle object
            max_tokens: Maximum tokens for response
            temperature: Temperature for generation
    
    Returns:
            Japanese title string or None if generation fails
        """
        try:
            # Extract content from article
            summary_points = []
            if (hasattr(article, 'summarized_article') and 
                article.summarized_article and
                hasattr(article.summarized_article, 'summary') and 
                article.summarized_article.summary and
                hasattr(article.summarized_article.summary, 'summary_points')):
                summary_points = article.summarized_article.summary.summary_points
            
            original_title = ""
            if (hasattr(article, 'summarized_article') and
                article.summarized_article and
                hasattr(article.summarized_article, 'filtered_article') and
                article.summarized_article.filtered_article and
                hasattr(article.summarized_article.filtered_article, 'raw_article') and
                article.summarized_article.filtered_article.raw_article):
                original_title = article.summarized_article.filtered_article.raw_article.title
            
            if not summary_points or not original_title:
                return None
            
            # Create prompt for Japanese title generation
            prompt = f"""
ã‚ãªãŸã¯æ—¥æœ¬ã®é«˜ç´šAIãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼ã®ãƒ—ãƒ­ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®è¨˜äº‹è¦ç´„ã‹ã‚‰ã€èª­è€…ã‚’å¼•ãã¤ã‘ã‚‹é­…åŠ›çš„ãªæ—¥æœ¬èªã®è¦‹å‡ºã—ã‚’**å‰µä½œ**ã—ã¦ãã ã•ã„ã€‚

**ğŸ¯ é‡è¦ãªç›®æ¨™**
1608ç‰ˆã®æˆåŠŸä¾‹ã®ã‚ˆã†ãªé«˜å“è³ªãªè¦‹å‡ºã—ã‚’ä½œæˆã™ã‚‹ã€‚å…·ä½“çš„ã§é­…åŠ›çš„ã€ã‹ã¤å®Œçµã—ãŸæ–‡ç« ã«ã™ã‚‹ã€‚

**ğŸ“ å¿…é ˆè¦ä»¶**
- **æ–‡å­—æ•°**: 20ï½55æ–‡å­—ï¼ˆæœ€é©ãªæƒ…å ±é‡ï¼‰
- **å®Œçµæ€§**: è¦‹å‡ºã—å˜ä½“ã§å†…å®¹ãŒç†è§£ã§ãã€æ–‡é€”åˆ‡ã‚Œã¯çµ¶å¯¾ç¦æ­¢
- **å…·ä½“æ€§**: ä¼æ¥­åãƒ»è£½å“åãƒ»æ•°å€¤ãƒ»ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å«ã‚ã‚‹
- **æ–°é®®ã•**: ã‚ã‚ŠãŒã¡ãªè¡¨ç¾ã‚’é¿ã‘ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¾¡å€¤ã‚’å¼·èª¿

**ğŸ“Š å…¥åŠ›æƒ…å ±**
å…ƒã‚¿ã‚¤ãƒˆãƒ«: {original_title}
è¦ç´„:
{'\\n'.join(f'- {point}' for point in summary_points[:3])}

**âœ… ç›®æ¨™ãƒ¬ãƒ™ãƒ«**
- Googleã®AIã€ŒGemini CLIã€ãŒç‰©è­°ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ã®ç›´æ¥ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡ŒãŒã‚‚ãŸã‚‰ã™åˆ©ä¾¿æ€§ã¨æ·±åˆ»ãªã‚»ã‚­ãƒ¥ãƒªãƒ†
- AIã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—Cluelyã€æ‹¡æ•£æˆ¦ç•¥ã¨ã€ŒåŠé€æ˜ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ã€UIã§10é€±é–“ã§1500ä¸‡ãƒ‰ãƒ«ã‚’èª¿é”
- Metaã€OpenAIã®ç²¾é‹­ç ”ç©¶è€…3åã‚’é›»æ’ƒç²å¾—ã€ã‚¶ãƒƒã‚«ãƒ¼ãƒãƒ¼ã‚°è‚ã„ã‚Šã®AGIé–‹ç™ºã‚’åŠ é€Ÿ

**âŒ çµ¶å¯¾é¿ã‘ã‚‹ã¹ãæ‚ªã„ä¾‹**
- Googleã€AIåˆ†é‡ã§ç™ºè¡¨ã‚’ç™ºè¡¨ (é‡è¤‡ãƒ»ä¸è‡ªç„¶)
- Microsoftã€AIåˆ†é‡ã§ç™ºè¡¨ã‚’ç™ºè¡¨ (é‡è¤‡ãƒ»ä¸è‡ªç„¶)
- AIæŠ€è¡“ã®æœ€æ–°ç™ºè¡¨ (æŠ½è±¡çš„ãƒ»ã¤ã¾ã‚‰ãªã„)
- ã€œæ°ãŒ (é€”ä¸­ã§æ–‡åˆ‡ã‚Œ)
- ã€œã«ã¤ã„ã¦ (æŠ½è±¡çš„)

**ğŸ¯ å‡ºåŠ›**
æ—¥æœ¬èªè¦‹å‡ºã—ï¼ˆ20ï½55æ–‡å­—ã€1è¡Œã§ï¼‰:"""
            
            # Generate using simple text generation
            result = await self.generate_simple_text(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                max_retries=2
            )
            
            if result:
                # Clean up the result with improved multi-line handling (Lawrence's requirement)
                lines = [line.strip() for line in result.splitlines() if line.strip()]
                if lines:
                    # Take the last substantial line (YouTube title fix)
                    title = lines[-1]
                else:
                    title = result.strip()
                
                # Remove quotes if present
                title = re.sub(r'^["\'ã€Œ]|["\'ã€]$', '', title)
                # Remove any remaining newlines
                title = title.replace('\n', ' ').strip()
                
                # Validate length and quality
                if 25 <= len(title) <= 60:
                    # Additional quality checks
                    if not title.endswith(('ã¨ç™ºè¡¨', 'ã¨å ±å‘Š', 'ã¨è¿°ã¹', 'ã¨èªã£')):
                        return title
                    # If it ends with incomplete patterns, try to clean
                    cleaned_title = title.rstrip('ã¨ç™ºè¡¨ã¨å ±å‘Šã¨è¿°ã¹ã¨èªã£')
                    if 25 <= len(cleaned_title) <= 60:
                        return cleaned_title
                elif len(title) > 60:
                    # Smart truncation at sentence boundaries
                    truncated = title[:60]
                    # Find last complete word boundary
                    last_space = truncated.rfind(' ')
                    last_comma = truncated.rfind('ã€')
                    last_period = truncated.rfind('ã€‚')
                    
                    best_cut = max(last_space, last_comma, last_period)
                    if best_cut > 40:  # Only if we don't cut too much
                        return truncated[:best_cut]
                    else:
                        return truncated[:57] + "..."
                
                return None
            
            return None
            
        except Exception as e:
            logger.warning(f"Failed to generate Japanese title: {e}")
            return None

# Example usage (for testing)
async def main():
    """Test the LLM router."""